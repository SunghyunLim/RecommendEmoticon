{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tweet의 감정상태에 따른 이모티콘 추천\n",
    "\n",
    "이 워크샵은 크게 세가지 과정으로 되어 있습니다.\n",
    "\n",
    "- **기본 과정 (ML 파이프라인)**\n",
    "    - \"데이타 준비\" --> \"데이타 전처리\" --> \"모델 빌드\" --> \"모델 학습\" --> \"모델 배포 및 추론\"\n",
    "- OPTION 1\n",
    "    - API GATEWAY 통해서 추론하기\n",
    "- OPTION 2\n",
    "    - Kubernetics/Kubeflow --> SageMaker Traing Job\n",
    "\n",
    "\n",
    "- Git의 많은 부분의 소스 코드는 아래 Git에서 가져왔습니다. 참고 하세요.\n",
    "    - Chris Fregly, Antje Barth, Book, Data Science on AWS, \n",
    "    - [Source: Data Science on Amazon Web Services](https://github.com/data-science-on-aws/workshop)\n",
    "\n",
    "## 솔류션 아키텍쳐\n",
    "![solutions-architecture](Tweet-BERT/img/solutions-architecture.png)\n",
    "\n",
    "## 사용된 기술들 (Techniques)\n",
    "![techniques-used](Tweet-BERT/img/techniques-used.png)\n",
    "\n",
    "## 결과\n",
    "\n",
    "![result-recommend-emoticon](Tweet-BERT/img/result-recommend-emoticon.png)\n",
    "\n",
    "## 기본 과정 (ML 파이프라인)\n",
    "\n",
    "### Prepare Tweet Data\n",
    "- 1.1.Prepare-Tweet-Data.ipynb (1분 소요)<br>\n",
    "    - Tweet 입력 데이터 클린징 및 S3에 업로드\n",
    "    - [Module 1.0]\n",
    "        - Option: [Module 1.5]- BERT Input 변환 과정 확인\n",
    "        \n",
    "### Preprocess Tweet Data\n",
    "- OPTION\n",
    "    - 2.0.Option-Scratch-Convert-Input-TFRecord.ipynb (소요시간 약 1분)\n",
    "        - 이 노트북은 Text 입력 데이타가 최종적으로 사용할 TF Record 형태로 변환하는 과정을 보여줌\n",
    "        - 만일 이 과정이 익숙하지 않다면 실행을 권고 함\n",
    "\n",
    "\n",
    "\n",
    "- 2.1.Convert-Input-TFRecord.ipynb (6분 소요) <br>\n",
    "    - Input Text --> BERT Feature Vector 로 변환 --> TF Record로 변환\n",
    "    - SageMaker Processor Job으로 2개의 인스턴스를 사용하여 위 스크립트를 실행\n",
    "    - Train, Validation, Test의 각각 2개의 TF Records 파일이 S3에 저장됨.\n",
    "\n",
    "### Build a Train Script\n",
    "- 3.1.Write-Train-Script.ipynb (1분 소요)\n",
    "    - 학습에 사용할 스크립트를 작성 합니다.\n",
    "- OPTION\n",
    "    - 3.2.Train-LocalMode.ipynb\n",
    "        - 작성한 학습 스크립트를 Local Mode를 사용하여 로직 테스트 합니다.\n",
    "\n",
    "### Train a Model\n",
    "학습은 아래 두가지 옵션이 있습니다. \n",
    "- BYOS (Bring Your Own Script)로서 보통 Script Mode 로 합니다. SageMaker의 Built-in Container (예: Tensorflow)에 학습 스크립트를 제공하고, 이후에 데이타를 입력(S3) 으로 제공하고, 출력으로서 모델 아티펙트를 S3에 저장 됩니다.\n",
    "- BYOC (Bring Your Own Container)로서 실행환경의 Docker Image (학습 스크립트 + 환경:텐서플로우) 를 가져와서, 데이타를 입력(S3) 으로 제공하고, 출력으로서 모델 아티펙트를 S3에 저장 됩니다.\n",
    "\n",
    "**위의 두 방식 중에 어느 한가지를 사용하여 학습을 하세요.**\n",
    "- 선택 1: BYOS\n",
    "    - 3.3.Train-BYOS-ScriptMode.ipynb     \n",
    "        - 스크립트 모드로 학습 합니다.\n",
    "- 선택 2: BYOC\n",
    "    - 3.4.1.Make-Train-Image-ECR.ipynb\n",
    "        - 로컬 노트북에서 Docker Image를 만들고, 이를 ECR(Elastic Container Registry)에 등록 합니다.\n",
    "    - 3.4.2-Train-BYOC.ipynb    \n",
    "        - ECR에 등록된 Docker Image를 가져와서 학습 합니다.\n",
    "    \n",
    "### Depoly a Model\n",
    "배포를 하기 위해서는 기본적으로 세자기 항목이 필요 합니다.\n",
    "- (1) 모델 아티펙트\n",
    "    - 이 부분은 위의 학습의 산출물로서 S3에 저장이 되어 있습니다.\n",
    "- (2) 추론 (Inference) image \n",
    "    - 위의 모델 아티펙트가 동작하기 위한 실행 환경을 제공 합니다. 예로 Tensorflow Serving 같은 것이 있습니다.\n",
    "- (3) 추론 스크립트 (예: inference.py)\n",
    "    - 모델 아티펙트에 \"입력\" 을 넣고, 이후에 모델 아티펙트를 통해서 \"출력\" 이 나오는데, \"입력\" 에 대한 사전처리와 \"출력\" 에 대한 사후 처리에 대한 스크립트 입니다.\n",
    "    \n",
    "이 과정에서는 두가지의 Inference Image 및 추론 스크립트 제공 방식을 사용 합니다.\n",
    "- 방식 (1) \n",
    "    - Built-in Inference Image - Tensorflow Serving 2.0, \n",
    "    - 추론 스크립트 : Endpoint 기반에 tensorflow serving predictor를 생성하여 사용\n",
    "- 방식 (2)\n",
    "    - Custom Inference Image 생성\n",
    "    - 추론 스크립트 : SageMaker Model를 생성시에 추론 스크립트 제공 (inference.py)\n",
    "\n",
    "**위의 두 방식 중에 어느 한가지를 사용하여 배포를 하세요.**\n",
    "- 선택 1: Built-in Inference Image\n",
    "    - 4.1.Deploy-Built-In-TFS-Image.ipynb     \n",
    "        - Tensorflow Serving 2.0 이미지를 가지고 배포 합니다.\n",
    "- 선택 2: Custom Inference Image\n",
    "    - 4.2.1.Make-Custom-Inference-Image-ECR.ipynb\n",
    "        - 로컬 노트북에서 Docker Image를 만들고, 이를 ECR(Elastic Container Registry)에 등록 합니다.\n",
    "    - 4.2.2.Deploy-Custom-Inference-Image.ipynb\n",
    "        - ECR에 등록된 Docker Image를 가져와서 학습 합니다.\n",
    "\n",
    "## OPTION 1: API GATEWAY 통해서 추론하기\n",
    "기존에는 추론을 SageMaker Notebook에서 했습니다. 이번에는 API GATEWAY를 이용하여 외부 웹 페이지에서 HTTP POST Call 을 하여 추론 결과를 얻습니다. 아래 그림과 같이 구현 합니다.\n",
    "\n",
    "![Fig.4.3.APIGATEWAYflow](Tweet-BERT/img/Fig.4.3.APIGATEWAY_flow.png)\n",
    "아래 노트북의 가이드를 따라 하시면 됩니다.\n",
    "- 4.3.API_GATEWAY.ipynb\n",
    "\n",
    "## OPTION 2: Kubernetics/Kubeflow --> SageMaker Traing Job\n",
    "\n",
    "Kubernetics/Kubeflow 에서 SageMaker Cloud Cluster로 학습을 요청하여, 학습의 결과인 모델 아티펙트를 가지고 SageMaker Cloud Cluster에서 추론을 하는 과정 입니다.\n",
    "자세한 가이드는 \n",
    "**[관련 가이드](Tweet-BERT/install_EKS_Kubeflow/README.md)** 참조 하세요.\n",
    "\n",
    "![Fig.6.1.Kubenetics-Busting-Sagemaker](Tweet-BERT/img/Fig.6.1.Kubenetics-Busting-Sagemaker.png)\n",
    "\n",
    "\n",
    "---\n",
    "## Reference\n",
    "- Chris Fregly, Antje Barth, Book, Data Science on AWS, https://www.oreilly.com/library/view/data-science-on/9781492079385/\n",
    "    - Source: Data Science on Amazon Web Services\n",
    "        - https://github.com/data-science-on-aws/workshop\n",
    "\n",
    "\n",
    "- Original Data Source\n",
    "    - Douwe Osinga, Deep Learning Cookbook. Ch7, Suggesting Emojis\n",
    "https://www.amazon.com/Deep-Learning-Cookbook-Practical-Recipes/dp/149199584X\n",
    "\n",
    "\n",
    "- Huggingface Transformers <br>\n",
    "https://github.com/huggingface/transformers\n",
    "\n",
    "- Kubernetes and Amazon SageMaker for machine learning — best of both worlds\n",
    "https://towardsdatascience.com/kubernetes-and-amazon-sagemaker-for-machine-learning-best-of-both-worlds-part-1-37580689a92f\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
