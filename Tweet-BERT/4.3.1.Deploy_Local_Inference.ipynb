{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local inference on Docker Image\n",
    "\n",
    "- Tensorflow Serving Model:\n",
    "    - https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-serving-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias training_job_name\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert2tweet-2020-07-08-07-58-27-895\n"
     ]
    }
   ],
   "source": [
    "training_job_name = 'bert2tweet-2020-07-08-07-58-27-895'\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image = '057716757052.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-serving:2.0.0-cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "model = Model(model_data='s3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name),\n",
    "              role=role,\n",
    "              framework_version='2.0.0',\n",
    "              entry_point='inference.py',\n",
    "              image=container_image\n",
    "             ) # Elastic Inference does not yet support TF 2.1.0 as of sagemaker==1.56.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When an error occurs, run \"docker container ls\" on terminal and if exists and stop with \"docker container rm <Container ID>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmp159pek3h_algo-1-4b38r_1\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:nginx config: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m load_module modules/ngx_http_js_module.so;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m worker_processes auto;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m daemon off;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m error_log  /dev/stderr error;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m events {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m http {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   default_type application/json;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   js_include tensorflow-serving.js;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   upstream tfs_upstream {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     server localhost:8501;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   upstream gunicorn_upstream {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     server unix:/tmp/gunicorn.sock fail_timeout=1;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   server {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     client_body_buffer_size 100m;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     subrequest_output_buffer_size 100m;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     set $tfs_version 2.0;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     set $default_tfs_model None;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     location /tfs {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         rewrite ^/tfs/(.*) /$1  break;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         proxy_redirect off;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         proxy_pass_request_headers off;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         proxy_set_header Content-Type 'application/json';\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         proxy_set_header Accept 'application/json';\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         proxy_pass http://tfs_upstream;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     location /ping {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         proxy_pass http://gunicorn_upstream/ping;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     location /invocations {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         proxy_pass http://gunicorn_upstream/invocations;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     location /models {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         proxy_pass http://gunicorn_upstream/models;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     location / {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m         return 404 '{\"error\": \"Not Found\"}';\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:tfs_utils:using default model name: saved_model\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:tfs_utils:tensorflow serving model config: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m model_config_list: {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   config: {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     name: \"saved_model\",\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     base_path: \"/opt/ml/model/tensorflow/saved_model\",\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:using default model name: saved_model\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:tensorflow serving model config: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m model_config_list: {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   config: {\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     name: \"saved_model\",\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     base_path: \"/opt/ml/model/tensorflow/saved_model\",\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:started tensorflow serving (pid: 8)\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:gunicorn command: gunicorn -b unix:/tmp/gunicorn.sock -k gevent --chdir /sagemaker --pythonpath /opt/ml/model/code -e TFS_GRPC_PORT=9000 -e SAGEMAKER_MULTI_MODEL=False -e SAGEMAKER_SAFE_PORT_RANGE=None python_service:app\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.134814: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.134844: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: saved_model\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.235092: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: saved_model version: 0} exhausted max_num_retries: 0\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.235122: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.235133: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.235143: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.235161: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/tensorflow/saved_model/0\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:gunicorn version info:\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m gunicorn (version 20.0.4)\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:started gunicorn (pid: 44)\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.282599: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.353594: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.361736: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m [2020-07-09 07:16:01 +0000] [44] [INFO] Starting gunicorn 20.0.4\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:gunicorn server is ready!\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m [2020-07-09 07:16:01 +0000] [44] [INFO] Listening at: unix:/tmp/gunicorn.sock (44)\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m [2020-07-09 07:16:01 +0000] [44] [INFO] Using worker: gevent\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m [2020-07-09 07:16:01 +0000] [58] [INFO] Booting worker with pid: 58\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:nginx version info:\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m nginx version: nginx/1.18.0\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m built by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m built with OpenSSL 1.1.1  11 Sep 2018\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m TLS SNI support enabled\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.18.0/debian/debuild-base/nginx-1.18.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:__main__:started nginx (pid: 59)\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:01.534954: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:02.227487: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:$LD_LIBRARY_PATH\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:02.227606: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:$LD_LIBRARY_PATH\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:02.227618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:transformers.file_utils:TensorFlow version 2.1.0 available.\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:02.893118: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/tensorflow/saved_model/0\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:03.117703: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 1882539 microseconds.\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:03.132692: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/tensorflow/saved_model/0/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:03.149552: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: saved_model version: 0} exhausted max_num_retries: 0\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:03.149584: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:03.151781: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 2020-07-09 07:16:03.152987: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:filelock:Lock 140022005410224 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpph8qe1ja\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 1.09MB/s]\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:filelock:Lock 140022005410224 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "CPU times: user 1min 11s, sys: 12.5 s, total: 1min 23s\u001b[36malgo-1-4b38r_1  |\u001b[0m 172.18.0.1 - - [09/Jul/2020:07:16:03 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\n",
      "\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "instance_type='local'\n",
    "local_endpoint = model.deploy(initial_instance_count = 1,\n",
    "                             instance_type = instance_type,\n",
    "                             wait=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint without custom Docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-tensorflow-serving-2020-07-09-04-50-37-884'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_endpoint.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:python_service:http://gunicorn_upstream/invocations\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m INFO:tfs_utils:sagemaker tfs attributes: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m {}\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m instances: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m  ['This is great', 'The weather is wonderful']\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m instance:  This is great\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m transformed_instance: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m  {'input_ids': [101, 2023, 2003, 2307, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m instance:  The weather is wonderful\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m transformed_instance: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m  {'input_ids': [101, 1996, 4633, 2003, 6919, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m transformed_data: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m  {'instances': [{'input_ids': [101, 2023, 2003, 2307, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 1996, 4633, 2003, 6919, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]}\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m ###############################\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m log_probabilities: \n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m  [[-1.37874043, -1.19526446, -1.26716495, -1.1868751, 7.1464591, -1.53433263, -2.15244746, -1.74911451, -2.27858257, -1.96475625], [-1.3166939, -1.18195844, -1.41097331, -1.0972333, 7.164114, -1.53150845, -2.15987682, -1.74124718, -2.26538515, -1.96443963]]\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m ###############################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-1.37874043,\n",
       "  -1.19526446,\n",
       "  -1.26716495,\n",
       "  -1.1868751,\n",
       "  7.1464591,\n",
       "  -1.53433263,\n",
       "  -2.15244746,\n",
       "  -1.74911451,\n",
       "  -2.27858257,\n",
       "  -1.96475625],\n",
       " [-1.3166939,\n",
       "  -1.18195844,\n",
       "  -1.41097331,\n",
       "  -1.0972333,\n",
       "  7.164114,\n",
       "  -1.53150845,\n",
       "  -2.15987682,\n",
       "  -1.74124718,\n",
       "  -2.26538515,\n",
       "  -1.96443963]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 172.18.0.1 - - [09/Jul/2020:07:22:27 +0000] \"POST /invocations HTTP/1.1\" 200 256 \"-\" \"-\"\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 172.18.0.1 - - [09/Jul/2020:07:25:02 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\n",
      "\u001b[36malgo-1-4b38r_1  |\u001b[0m 172.18.0.1 - - [09/Jul/2020:07:27:41 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\n"
     ]
    }
   ],
   "source": [
    "reviews = [\"This is great\", \"The weather is wonderful\"]\n",
    "predicted_classes = local_endpoint.predict(reviews)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
