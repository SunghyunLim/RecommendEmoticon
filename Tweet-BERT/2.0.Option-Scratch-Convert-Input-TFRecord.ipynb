{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTION - [Module 2.0] BERT Input 변환 과정 확인\n",
    "이 노트북은 Text 입력 데이타가 최종적으로 사용할 TF Record 형태로 변환하는 과정을 보여줌. 이를 위해 [huggingface의 Transformers](https://github.com/huggingface/transformers) 를 사용 함.\n",
    "\n",
    "- S3에서 Tweet 데이터 로컬에 다운로드\n",
    "- Input Text --> BERT Feature Vector 로 변환 --> TF Record로 변환\n",
    "```python\n",
    "    # BERT Feature Vector\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id)\n",
    "```\n",
    "- 변환된 예시 Tweet 보여줌\n",
    "---\n",
    "Reference\n",
    "- Chris Fregly, Antje Barth, Book, Data Science on AWS, https://www.oreilly.com/library/view/data-science-on/9781492079385/\n",
    "    - Source: Data Science on Amazon Web Services\n",
    "        - https://github.com/data-science-on-aws/workshop\n",
    "- Transformers (https://github.com/huggingface/transformers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import boto3\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Download tweet data from S3 </h2>\n",
    "이전 노트북에서 업로드한 S3의 데이타 파일 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-16 13:34:55     838266 tweet_file_01.csv.gz\n",
      "2020-08-16 13:34:55     838236 tweet_file_02.csv.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {s3_destination_path_csv}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name_01:  s3://sagemaker-ap-northeast-2-343441690612/tweet_emoticon/csv/tweet_file_01.csv.gz\n",
      "file_name_02:  s3://sagemaker-ap-northeast-2-343441690612/tweet_emoticon/csv/tweet_file_02.csv.gz\n"
     ]
    }
   ],
   "source": [
    "file_name_01 = os.path.join(s3_destination_path_csv, \"tweet_file_01.csv.gz\")\n",
    "file_name_02 = os.path.join(s3_destination_path_csv, \"tweet_file_02.csv.gz\")\n",
    "print(\"file_name_01: \", file_name_01)\n",
    "print(\"file_name_02: \", file_name_02)\n",
    "tweet_file_01_df = pd.read_csv(file_name_01, compression='gzip')\n",
    "tweet_file_02_df = pd.read_csv(file_name_02, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process BERT Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.2.2)\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "astroid 2.3.3 requires wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q wrapt --upgrade --ignore-installed\n",
    "!pip install -q transformers==2.8.0\n",
    "!pip install -q tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# We set sequence to be at most 128 tokens long\n",
    "MAX_SEQ_LENGTH = 32\n",
    "LABEL_VALUES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "    \n",
    "class InputFeatures(object):\n",
    "    \"\"\"\n",
    "    BERT feature vectors\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 label_id\n",
    "                ):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "class Input(object):\n",
    "    \"\"\"\n",
    "    A single training/test input for sequence classifications\n",
    "    \"\"\"\n",
    "    def __init__(self, text, label=None):\n",
    "        \"\"\"Constructs an Input.\n",
    "        Args:\n",
    "          text: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        \n",
    "def convert_input(text_input):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # \n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "\n",
    "    tokens = tokenizer.tokenize(text_input.text)\n",
    "    print('**tokens**\\n{}\\n'.format(tokens))\n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "    input_ids = encode_plus_tokens['input_ids']\n",
    "    input_mask = encode_plus_tokens['attention_mask']\n",
    "    segment_ids = [0] * MAX_SEQ_LENGTH\n",
    "\n",
    "    label_id = label_map[text_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id)\n",
    "\n",
    "    print('**input_ids**\\n{}\\n'.format(features.input_ids))\n",
    "    print('**input_mask**\\n{}\\n'.format(features.input_mask))\n",
    "    print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\n",
    "    print('**label_id**\\n{}\\n'.format(features.label_id))\n",
    "\n",
    "    return features\n",
    "    \n",
    "# We'll need to transform our data into a format that BERT understands\n",
    "# - `text` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "# - `label` is the star_rating label (0, 1, 2, 3, 4, 5, ..,9) for our training input data\n",
    "\n",
    "def transform_inputs_to_tfrecord(inputs):\n",
    "    \n",
    "    tf_records = list()\n",
    "    for (input_idx, text_input) in enumerate(inputs):            \n",
    "        if input_idx % 10000 == 0:\n",
    "            print('Writing input {} of {}\\n'.format(input_idx, len(inputs)))\n",
    "\n",
    "        features = convert_input(text_input)\n",
    "        \n",
    "        all_features = collections.OrderedDict()\n",
    "        all_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
    "        all_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
    "        all_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
    "        all_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
    "\n",
    "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
    "\n",
    "        tf_records.append(tf_record.SerializeToString())\n",
    "\n",
    "    return tf_records\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show BERT feature vectors\n",
    "\n",
    "아래는 \"Tweet\" 의 **자연어** 가 Transfomer의 입력 형태인 **token, input_id, input_mask, segment_id, label_id** 로 변환 되는 것을 보여 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01_sample = tweet_file_01_df[0:1]\n",
    "df02_sample = tweet_file_02_df[0:1]\n",
    "DATA_COLUMN = 'TWEET'\n",
    "LABEL_COLUMN = 'LABEL'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: \n",
      "  street hood world 2016 by on soundcloud prod by yoshimi tcprt bzbrt 4 hiphop music\n",
      "Writing input 0 of 1\n",
      "\n",
      "**tokens**\n",
      "['street', 'hood', 'world', '2016', 'by', 'on', 'sound', '##cl', '##oud', 'pro', '##d', 'by', 'yo', '##shi', '##mi', 'tc', '##pr', '##t', 'b', '##z', '##br', '##t', '4', 'hip', '##hop', 'music']\n",
      "\n",
      "**input_ids**\n",
      "[101, 2395, 7415, 2088, 2355, 2011, 2006, 2614, 20464, 19224, 4013, 2094, 2011, 10930, 6182, 4328, 22975, 18098, 2102, 1038, 2480, 19892, 2102, 1018, 5099, 18471, 2189, 102, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweet: \\n {}\".format(df01_sample[DATA_COLUMN][0]))\n",
    "inputs = df01_sample.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                   label = x[LABEL_COLUMN]), axis=1)\n",
    "tf_records = transform_inputs_to_tfrecord(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: \n",
      "  r g will never let you down yesterday was amazing\n",
      "Writing input 0 of 1\n",
      "\n",
      "**tokens**\n",
      "['r', 'g', 'will', 'never', 'let', 'you', 'down', 'yesterday', 'was', 'amazing']\n",
      "\n",
      "**input_ids**\n",
      "[101, 1054, 1043, 2097, 2196, 2292, 2017, 2091, 7483, 2001, 6429, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**input_mask**\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**segment_ids**\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "**label_id**\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweet: \\n {}\".format(df02_sample[DATA_COLUMN][0]))\n",
    "inputs = df02_sample.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                   label = x[LABEL_COLUMN]), axis=1)\n",
    "tf_records = transform_inputs_to_tfrecord(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\xba\\x01\\n3\\n\\x0bsegment_ids\\x12$\\x1a\"\\n \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n;\\n\\tinput_ids\\x12.\\x1a,\\n*e\\x9e\\x08\\x93\\x08\\xb1\\x10\\x94\\x11\\xf4\\x11\\xe1\\x0f\\xab\\x10\\xbb:\\xd1\\x0f\\x9d2f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n2\\n\\ninput_mask\\x12$\\x1a\"\\n \\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n\\x12\\n\\tlabel_ids\\x12\\x05\\x1a\\x03\\n\\x01\\x02'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
