{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACK- [Module 4.1] Deploy from Scratch\n",
    "\n",
    "\n",
    "여기서는 다음과 같은 작업을 합니다.\n",
    "\n",
    "- 모델 아티펙트 (model.tar.gz) 파일을 S3에서 로컬에 다운로드\n",
    "- TF Saved_Model 의 정의를 확인\n",
    "- SageMaker Model 생성\n",
    "- Endpoint 생성\n",
    "- Inference의 Request Serializer and Deserializer 생성\n",
    "- 프리딕터 생성\n",
    "- 셈플 데이타로 추론\n",
    "\n",
    "---\n",
    "이 노트북은 약 10분 정도 소요 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 프로그램 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q --upgrade pip\n",
    "# !pip install -q wrapt --upgrade --ignore-installed\n",
    "# !pip install -q tensorflow==2.1.0\n",
    "# !pip install -q transformers==2.8.0\n",
    "# !pip install -q sagemaker==1.56.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Model to the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-training-2020-08-02-12-18-28-858\n"
     ]
    }
   ],
   "source": [
    "training_job_name = 'tensorflow-training-2020-08-02-12-18-28-858'\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_download = 'model'\n",
    "# os.makedirs(model_download, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz {model_download}/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvzf   {model_download}/model.tar.gz\n",
    "# !saved_model_cli show --all --dir ./tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "model = Model(model_data='s3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name),\n",
    "              role=role,\n",
    "              framework_version='2.0.0') # Elastic Inference does not yet support TF 2.1.0 as of sagemaker==1.56.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmpwyb5y0cc_algo-1-nnley_1\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:using default model name: saved_model\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:tensorflow serving model config: \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m model_config_list: {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   config: {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     name: \"saved_model\",\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     base_path: \"/opt/ml/model/tensorflow/saved_model\",\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:nginx config: \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m load_module modules/ngx_http_js_module.so;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m worker_processes auto;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m daemon off;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m error_log  /dev/stderr error;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m events {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m http {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   default_type application/json;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   js_include tensorflow-serving.js;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   upstream tfs_upstream {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     server localhost:8501;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   upstream gunicorn_upstream {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     server unix:/tmp/gunicorn.sock fail_timeout=1;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   server {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     client_body_buffer_size 100m;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     subrequest_output_buffer_size 100m;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     set $tfs_version 2.0;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     set $default_tfs_model saved_model;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     location /tfs {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         rewrite ^/tfs/(.*) /$1  break;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         proxy_redirect off;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         proxy_pass_request_headers off;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         proxy_set_header Content-Type 'application/json';\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         proxy_set_header Accept 'application/json';\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         proxy_pass http://tfs_upstream;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     location /ping {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         js_content ping_without_model;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     location /invocations {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         js_content invocations;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     location ~ ^/models/(.*)/invoke {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         js_content invocations;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     location /models {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         proxy_pass http://gunicorn_upstream/models;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     location / {\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m         return 404 '{\"error\": \"Not Found\"}';\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:started tensorflow serving (pid: 10)\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:nginx version info:\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m nginx version: nginx/1.16.1\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m built by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m built with OpenSSL 1.1.1  11 Sep 2018\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m TLS SNI support enabled\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.16.1/debian/debuild-base/nginx-1.16.1=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m INFO:__main__:started nginx (pid: 12)\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.003309: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.003346: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: saved_model\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.105204: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: saved_model version: 0} exhausted max_num_retries: 0\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.105235: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.105244: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.105253: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.105276: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/tensorflow/saved_model/0\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.153211: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.225591: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.228137: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:23.409157: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:24.715062: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/tensorflow/saved_model/0\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:24.937849: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 1832572 microseconds.\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:24.953044: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/tensorflow/saved_model/0/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:24.971230: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: saved_model version: 0} exhausted max_num_retries: 0\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:24.971263: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:24.980024: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 2020-08-02 12:57:24.987624: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36malgo-1-nnley_1  |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "!\u001b[36malgo-1-nnley_1  |\u001b[0m 172.18.0.1 - - [02/Aug/2020:12:57:25 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\n"
     ]
    }
   ],
   "source": [
    "# instance_type='ml.m4.xlarge'\n",
    "instance_type = 'local'\n",
    "\n",
    "deployed_model = model.deploy(initial_instance_count = 1,\n",
    "                             instance_type = instance_type,\n",
    "                             wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name:  tensorflow-inference-2020-08-02-12-56-53-567\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = deployed_model.endpoint\n",
    "print('Endpoint name:  {}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def input_handler(instances, tokenizer, max_seq_length):\n",
    "    transformed_instances = []\n",
    "\n",
    "    for instance in instances:\n",
    "        encode_plus_tokens = tokenizer.encode_plus(instance,\n",
    "                                                   pad_to_max_length=True,\n",
    "                                                   max_length= max_seq_length)\n",
    "\n",
    "        input_ids = encode_plus_tokens['input_ids']\n",
    "        input_mask = encode_plus_tokens['attention_mask']\n",
    "        segment_ids = [0] * max_seq_length\n",
    "\n",
    "        transformed_instance = {\"input_ids\": input_ids, \n",
    "                                \"input_mask\": input_mask, \n",
    "                                \"segment_ids\": segment_ids}\n",
    "\n",
    "        transformed_instances.append(transformed_instance)\n",
    "\n",
    "    transformed_data = {\"instances\": transformed_instances}\n",
    "\n",
    "#    return json.dumps(transformed_data)\n",
    "    return transformed_data\n",
    "\n",
    "def output_handler(log_probabilities, classes):\n",
    "    import tensorflow as tf\n",
    "\n",
    "#     response_body = response.read().decode('utf-8')\n",
    "\n",
    "#     response_json = json.loads(response_body)\n",
    "\n",
    "#     log_probabilities = response_json[\"predictions\"]\n",
    "    print(log_probabilities)\n",
    "\n",
    "    predicted_classes = []\n",
    "\n",
    "    # Convert log_probabilities => softmax (all probabilities add up to 1) => argmax (final prediction)\n",
    "    for log_probability in log_probabilities:\n",
    "        softmax = tf.nn.softmax(log_probability)    \n",
    "        predicted_class_idx = tf.argmax(softmax, axis=-1, output_type=tf.int32)\n",
    "        predicted_class = classes[predicted_class_idx]\n",
    "        predicted_classes.append(predicted_class)\n",
    "\n",
    "    return predicted_classes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [{'input_ids': [101, 1031, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 1005, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 1048, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 1037, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 1055, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 1056, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 1005, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [101, 1033, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "transformer_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_seq_length = 32\n",
    "# reviews = [\"This is\"]\n",
    "reviews = \"['last']\"\n",
    "\n",
    "transformed_input = input_handler(reviews,transformer_tokenizer,max_seq_length )\n",
    "\n",
    "print(transformed_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probabilities: \u001b[36malgo-1-nnley_1  |\u001b[0m 172.18.0.1 - - [02/Aug/2020:13:05:53 +0000] \"POST /invocations HTTP/1.1\" 200 1060 \"-\" \"-\"\n",
      " [[-2.78938413, -2.8333919, -2.30564737, -2.80323434, -0.967011631, -0.674893916, 0.7996611, 6.15671396, -1.28165615, 0.20762749], [-1.46880817, -2.51432586, -3.01440501, -2.2301929, 4.03530264, -2.53345418, 1.70059085, -0.686957419, 0.242017537, -0.176542819], [-2.21749187, -1.99028897, -1.41359711, -2.16797948, -1.1606046, -0.664495, 5.31617498, 2.21805549, -1.97009885, -2.38786364], [-2.16882825, -2.7535584, -2.49117899, -3.0971, -0.506615102, -1.00784576, -0.254950404, 5.20294476, -1.79389775, 2.36805415], [-2.83191109, -2.29273534, -1.39180243, -2.80927444, 2.32096767, -1.76754916, 0.555659115, 4.83452177, -2.2591188, -1.16866124], [-1.41685355, -1.54134703, -1.70415258, -1.31951106, 0.2614429, -1.77130902, 6.72061825, -1.53032529, -0.900431573, -2.29838204], [-1.46880817, -2.51432586, -3.01440501, -2.2301929, 4.03530264, -2.53345418, 1.70059085, -0.686957419, 0.242017537, -0.176542819], [-1.2193507, -1.36530411, -2.70937538, -2.10475016, 3.37097836, -1.93698585, 4.4490509, -1.54371607, -1.12004566, -1.86128473]]\n",
      "[[-2.78938413, -2.8333919, -2.30564737, -2.80323434, -0.967011631, -0.674893916, 0.7996611, 6.15671396, -1.28165615, 0.20762749], [-1.46880817, -2.51432586, -3.01440501, -2.2301929, 4.03530264, -2.53345418, 1.70059085, -0.686957419, 0.242017537, -0.176542819], [-2.21749187, -1.99028897, -1.41359711, -2.16797948, -1.1606046, -0.664495, 5.31617498, 2.21805549, -1.97009885, -2.38786364], [-2.16882825, -2.7535584, -2.49117899, -3.0971, -0.506615102, -1.00784576, -0.254950404, 5.20294476, -1.79389775, 2.36805415], [-2.83191109, -2.29273534, -1.39180243, -2.80927444, 2.32096767, -1.76754916, 0.555659115, 4.83452177, -2.2591188, -1.16866124], [-1.41685355, -1.54134703, -1.70415258, -1.31951106, 0.2614429, -1.77130902, 6.72061825, -1.53032529, -0.900431573, -2.29838204], [-1.46880817, -2.51432586, -3.01440501, -2.2301929, 4.03530264, -2.53345418, 1.70059085, -0.686957419, 0.242017537, -0.176542819], [-1.2193507, -1.36530411, -2.70937538, -2.10475016, 3.37097836, -1.93698585, 4.4490509, -1.54371607, -1.12004566, -1.86128473]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7, 4, 6, 7, 7, 6, 4, 6]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-nnley_1  |\u001b[0m 172.18.0.1 - - [02/Aug/2020:14:24:21 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\n",
      "\u001b[36mtmpwyb5y0cc_algo-1-nnley_1 exited with code 137\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\", line 618, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\", line 677, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 137\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/local/image.py\", line 623, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmpwyb5y0cc/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_json = deployed_model.predict(transformed_input)\n",
    "log_probabilities = response_json[\"predictions\"]\n",
    "print(\"log_probabilities: \", log_probabilities)\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "output_handler(log_probabilities, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = \"['last']\"\n",
    "deployed_model.predict(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
