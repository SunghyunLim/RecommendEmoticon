{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 3.1] Commandline 에서 Train Script 실행\n",
    "이 노트북은 Docker에서 실행을 하지 않고, 환경 변수를 설정 한 후에 Commandline에서 실행을 보여 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.1.1)\n",
      "\u001b[31mERROR: astroid 2.3.3 has requirement wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q wrapt --upgrade --ignore-installed\n",
    "!pip install -q tensorflow==2.1.0\n",
    "!pip install -q transformers==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_script_bert_tweet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_script_bert_tweet.py\n",
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import pprint\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers.configuration_distilbert import DistilBertConfig\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "####################\n",
    "# 기존 소스에 변경 사항\n",
    "# Define 10 labels\n",
    "####################\n",
    "CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "###################\n",
    "# TF 에 입력될 Data Set 생성 함수\n",
    "# BERT-related Function\n",
    "####################\n",
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "\n",
    "    y = record['label_ids']\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "# BERT-related Function\n",
    "def file_based_input_dataset_builder(channel,\n",
    "                                     input_filenames,\n",
    "                                     pipe_mode,\n",
    "                                     is_training,\n",
    "                                     drop_remainder,\n",
    "                                     batch_size,\n",
    "                                     epochs,\n",
    "                                     steps_per_epoch,\n",
    "                                     max_seq_length):\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "\n",
    "    if pipe_mode:\n",
    "        print('***** Using pipe_mode with channel {}'.format(channel))\n",
    "        from sagemaker_tensorflow import PipeModeDataset\n",
    "        dataset = PipeModeDataset(channel=channel,\n",
    "                                  record_format='TFRecord')\n",
    "    else:\n",
    "        print('***** Using input_filenames {}'.format(input_filenames))\n",
    "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
    "\n",
    "    dataset = dataset.repeat(epochs * steps_per_epoch * 100)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    name_to_features = {\n",
    "      \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        record = tf.io.parse_single_example(record, name_to_features)\n",
    "        # TODO:  wip/bert/bert_attention_head_view/train.py\n",
    "        # Convert input_ids into input_tokens with DistilBert vocabulary \n",
    "        #  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\n",
    "        #    hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\n",
    "        return record\n",
    "    \n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "          lambda record: _decode_record(record, name_to_features),\n",
    "          batch_size=batch_size,\n",
    "          drop_remainder=drop_remainder,\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    dataset.cache()\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(seed=42,\n",
    "                                  buffer_size=1000,\n",
    "                                  reshuffle_each_iteration=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_checkpoint_model(checkpoint_path):\n",
    "    '''\n",
    "    Load the last checkpoint file\n",
    "    '''\n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    glob_pattern = os.path.join(checkpoint_path, '*.h5')\n",
    "    print('glob pattern {}'.format(glob_pattern))\n",
    "\n",
    "    list_of_checkpoint_files = glob.glob(glob_pattern)\n",
    "    print('List of checkpoint files {}'.format(list_of_checkpoint_files))\n",
    "    \n",
    "    latest_checkpoint_file = max(list_of_checkpoint_files)\n",
    "    print('Latest checkpoint file {}'.format(latest_checkpoint_file))\n",
    "\n",
    "    initial_epoch_number_str = latest_checkpoint_file.rsplit('_', 1)[-1].split('.h5')[0]\n",
    "    initial_epoch_number = int(initial_epoch_number_str)\n",
    "\n",
    "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                                               latest_checkpoint_file,\n",
    "                                               config=config)\n",
    "\n",
    "    print('loaded_model {}'.format(loaded_model))\n",
    "    print('initial_epoch_number {}'.format(initial_epoch_number))\n",
    "    \n",
    "    return loaded_model, initial_epoch_number\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    '''\n",
    "    커멘드 라인 변수를 읽음. 상당수는 환경 변수의 값을 디폴트로 설정 함\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--train_data', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--validation_data', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    parser.add_argument('--test_data',\n",
    "                        type=str,\n",
    "                        default=os.environ['SM_CHANNEL_TEST'])\n",
    "    parser.add_argument('--output_dir',\n",
    "                        type=str,\n",
    "                        default=os.environ['SM_OUTPUT_DIR'])\n",
    "    parser.add_argument('--hosts', \n",
    "                        type=list, \n",
    "                        default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current_host', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CURRENT_HOST'])    \n",
    "    parser.add_argument('--num_gpus', \n",
    "                        type=int, \n",
    "                        default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--checkpoint_base_path', \n",
    "                        type=str, \n",
    "                        default='/opt/ml/checkpoints')\n",
    "    parser.add_argument('--use_xla',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--use_amp',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--max_seq_length',\n",
    "                        type=int,\n",
    "                        default=128)\n",
    "    parser.add_argument('--train_batch_size',\n",
    "                        type=int,\n",
    "                        default=128)\n",
    "    parser.add_argument('--validation_batch_size',\n",
    "                        type=int,\n",
    "                        default=256)\n",
    "    parser.add_argument('--test_batch_size',\n",
    "                        type=int,\n",
    "                        default=256)\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=1)\n",
    "    parser.add_argument('--learning_rate',\n",
    "                        type=float,\n",
    "                        default=0.00003)\n",
    "    parser.add_argument('--epsilon',\n",
    "                        type=float,\n",
    "                        default=0.00000001)\n",
    "    parser.add_argument('--train_steps_per_epoch',\n",
    "                        type=int,\n",
    "                        default=1)\n",
    "    parser.add_argument('--validation_steps',\n",
    "                        type=int,\n",
    "                        default=1)\n",
    "    parser.add_argument('--test_steps',\n",
    "                        type=int,\n",
    "                        default=1)\n",
    "    parser.add_argument('--freeze_bert_layer',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--run_validation',\n",
    "                        type=eval,\n",
    "                        default=False)    \n",
    "    parser.add_argument('--run_test',\n",
    "                        type=eval,\n",
    "                        default=False)    \n",
    "    parser.add_argument('--run_sample_predictions',\n",
    "                        type=eval,\n",
    "                        default=False)\n",
    "    parser.add_argument('--enable_checkpointing',\n",
    "                        type=eval,\n",
    "                        default=False)    \n",
    "    parser.add_argument('--model_dir', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_MODEL_DIR'])\n",
    "    \n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # This points to the S3 location - this should not be used by our code\n",
    "    # We should use /opt/ml/model/ instead\n",
    "    # parser.add_argument('--model_dir', \n",
    "    #                     type=str, \n",
    "    #                     default=os.environ['SM_MODEL_DIR'])\n",
    "    \n",
    "    args = parse_args()\n",
    "\n",
    "    print(\"################ Args: #######################\") \n",
    "    print(args)\n",
    "    \n",
    "    env_var = os.environ \n",
    "    print(\"################ Environment Variables: ################\") \n",
    "    pprint.pprint(dict(env_var), width = 1) \n",
    "\n",
    "#     print('SM_TRAINING_ENV {}'.format(env_var['SM_TRAINING_ENV']))\n",
    "#     sm_training_env_json = json.loads(env_var['SM_TRAINING_ENV'])\n",
    "#     is_master = sm_training_env_json['is_master']\n",
    "\n",
    "    print(\"################ Extract varaibles from Command Args #######################\") \n",
    "    is_master = True\n",
    "    print('is_master {}'.format(is_master))\n",
    "        \n",
    "    train_data = args.train_data\n",
    "    print('train_data {}'.format(train_data))\n",
    "    validation_data = args.validation_data\n",
    "    print('validation_data {}'.format(validation_data))\n",
    "    test_data = args.test_data\n",
    "    print('test_data {}'.format(test_data))    \n",
    "    \n",
    "    # local_model_dir = args.model_dir\n",
    "    local_model_dir = os.environ['SM_MODEL_DIR']\n",
    "    print('local_model_dir {}'.format(local_model_dir))        \n",
    "    \n",
    "    output_dir = args.output_dir\n",
    "    print('output_dir {}'.format(output_dir))    \n",
    "    hosts = args.hosts\n",
    "    print('hosts {}'.format(hosts))    \n",
    "    current_host = args.current_host\n",
    "    print('current_host {}'.format(current_host))    \n",
    "    num_gpus = args.num_gpus\n",
    "    print('num_gpus {}'.format(num_gpus))\n",
    "    job_name = os.environ['SAGEMAKER_JOB_NAME']\n",
    "    print('job_name {}'.format(job_name))    \n",
    "    use_xla = args.use_xla\n",
    "    print('use_xla {}'.format(use_xla))    \n",
    "    use_amp = args.use_amp\n",
    "    print('use_amp {}'.format(use_amp))    \n",
    "    max_seq_length = args.max_seq_length\n",
    "    print('max_seq_length {}'.format(max_seq_length))    \n",
    "    train_batch_size = args.train_batch_size\n",
    "    print('train_batch_size {}'.format(train_batch_size))    \n",
    "    validation_batch_size = args.validation_batch_size\n",
    "    print('validation_batch_size {}'.format(validation_batch_size))    \n",
    "    test_batch_size = args.test_batch_size\n",
    "    print('test_batch_size {}'.format(test_batch_size))    \n",
    "    epochs = args.epochs\n",
    "    print('epochs {}'.format(epochs))    \n",
    "    learning_rate = args.learning_rate\n",
    "    print('learning_rate {}'.format(learning_rate))    \n",
    "    epsilon = args.epsilon\n",
    "    print('epsilon {}'.format(epsilon))    \n",
    "    train_steps_per_epoch = args.train_steps_per_epoch\n",
    "    print('train_steps_per_epoch {}'.format(train_steps_per_epoch))    \n",
    "    validation_steps = args.validation_steps\n",
    "    print('validation_steps {}'.format(validation_steps))    \n",
    "    test_steps = args.test_steps\n",
    "    print('test_steps {}'.format(test_steps))    \n",
    "    freeze_bert_layer = args.freeze_bert_layer\n",
    "    print('freeze_bert_layer {}'.format(freeze_bert_layer))    \n",
    "        \n",
    "    run_validation = args.run_validation\n",
    "    print('run_validation {}'.format(run_validation))    \n",
    "    run_test = args.run_test\n",
    "    print('run_test {}'.format(run_test))    \n",
    "    run_sample_predictions = args.run_sample_predictions\n",
    "    print('run_sample_predictions {}'.format(run_sample_predictions))\n",
    "    \n",
    "    \n",
    "    enable_checkpointing = args.enable_checkpointing\n",
    "    print('enable_checkpointing {}'.format(enable_checkpointing))    \n",
    "\n",
    "    checkpoint_base_path = args.checkpoint_base_path\n",
    "    print('checkpoint_base_path {}'.format(checkpoint_base_path))\n",
    "\n",
    "    print(\" ################ Set Checkpoint path: ################\")\n",
    "    if is_master:\n",
    "        checkpoint_path = checkpoint_base_path\n",
    "    else:\n",
    "        checkpoint_path = '/tmp/checkpoints'        \n",
    "    print('checkpoint_path {}'.format(checkpoint_path))\n",
    "    \n",
    "    # Determine if PipeMode is enabled \n",
    "    pipe_mode_str = os.environ.get('SM_INPUT_DATA_CONFIG', '')\n",
    "    pipe_mode = (pipe_mode_str.find('Pipe') >= 0)\n",
    "    print('Using pipe_mode: {}'.format(pipe_mode))\n",
    " \n",
    "    # Model Output \n",
    "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, 'transformers/fine-tuned/')\n",
    "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=True)\n",
    "\n",
    "    # SavedModel Output\n",
    "    tensorflow_saved_model_path = os.path.join(local_model_dir, 'tensorflow/saved_model/0')\n",
    "    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\n",
    "\n",
    "    print (\"################ Mirrored distributed_strategy ################\")\n",
    "    \n",
    "    #distributed_strategy = tf.distribute.MirroredStrategy()\n",
    "    # Comment out when using smdebug as smdebug does not support MultiWorkerMirroredStrategy() as of smdebug 0.8.0\n",
    "    distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "    with distributed_strategy.scope():\n",
    "        tf.config.optimizer.set_jit(use_xla)\n",
    "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": use_amp})\n",
    "\n",
    "        train_data_filenames = glob(os.path.join(train_data, '*.tfrecord'))\n",
    "        print('train_data_filenames {}'.format(train_data_filenames))\n",
    "        train_dataset = file_based_input_dataset_builder(\n",
    "            channel='train',\n",
    "            input_filenames=train_data_filenames,\n",
    "            pipe_mode=pipe_mode,\n",
    "            is_training=True,\n",
    "            drop_remainder=False,\n",
    "            batch_size=train_batch_size,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=train_steps_per_epoch,\n",
    "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
    "\n",
    "        tokenizer = None\n",
    "        config = None\n",
    "        model = None\n",
    "\n",
    "        # This is required when launching many instances at once...  the urllib request seems to get denied periodically\n",
    "        successful_download = False\n",
    "        retries = 0\n",
    "        while (retries < 5 and not successful_download):\n",
    "            try:\n",
    "                tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "                config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\n",
    "                                                          num_labels=len(CLASSES))\n",
    "                model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                                              config=config)\n",
    "                successful_download = True\n",
    "                print('Sucessfully downloaded after {} retries.'.format(retries))\n",
    "            except:\n",
    "                retries = retries + 1\n",
    "                random_sleep = random.randint(1, 30)\n",
    "                print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\n",
    "                time.sleep(random_sleep)\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        initial_epoch_number = 0 \n",
    "\n",
    "        if enable_checkpointing:\n",
    "            print('***** Checkpoint enabled *****')\n",
    "            \n",
    "            os.makedirs(checkpoint_path, exist_ok=True)        \n",
    "            if os.listdir(checkpoint_path):\n",
    "                print('***** Found checkpoint *****')\n",
    "                print(checkpoint_path)\n",
    "                model, initial_epoch_number = load_checkpoint_model(checkpoint_path)\n",
    "                print('***** Using checkpoint model {} *****'.format(model))\n",
    "                \n",
    "            checkpoint_callback = ModelCheckpoint(\n",
    "                    filepath=os.path.join(checkpoint_path, 'tf_model_{epoch:05d}.h5'),\n",
    "                    save_weights_only=False,\n",
    "                    verbose=1,\n",
    "                    monitor='val_accuracy')\n",
    "            print('*** CHECKPOINT CALLBACK {} ***'.format(checkpoint_callback))\n",
    "            callbacks.append(checkpoint_callback)\n",
    "\n",
    "        if not tokenizer or not model or not config:\n",
    "            print('Not properly initialized...')\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "        print('** use_amp {}'.format(use_amp))        \n",
    "        if use_amp:\n",
    "            # loss scaling is currently required when using mixed precision\n",
    "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\n",
    "  \n",
    "        print('*** OPTIMIZER {} ***'.format(optimizer))\n",
    "        \n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "        print('Compiled model {}'.format(model))          \n",
    "        model.layers[0].trainable = not freeze_bert_layer\n",
    "        print(model.summary())\n",
    "\n",
    "        if run_validation:\n",
    "            validation_data_filenames = glob(os.path.join(validation_data, '*.tfrecord'))\n",
    "            print('validation_data_filenames {}'.format(validation_data_filenames))\n",
    "            validation_dataset = file_based_input_dataset_builder(\n",
    "                channel='validation',\n",
    "                input_filenames=validation_data_filenames,\n",
    "                pipe_mode=pipe_mode,\n",
    "                is_training=False,\n",
    "                drop_remainder=False,\n",
    "                batch_size=validation_batch_size,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=validation_steps,\n",
    "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
    "            \n",
    "            print('Starting Training and Validation...')\n",
    "            validation_dataset = validation_dataset.take(validation_steps)\n",
    "            train_and_validation_history = model.fit(train_dataset,\n",
    "                                                     shuffle=True,\n",
    "                                                     epochs=epochs,\n",
    "                                                     initial_epoch=initial_epoch_number,\n",
    "                                                     steps_per_epoch=train_steps_per_epoch,\n",
    "                                                     validation_data=validation_dataset,\n",
    "                                                     validation_steps=validation_steps,\n",
    "                                                     callbacks=callbacks)                                \n",
    "            print(train_and_validation_history)\n",
    "        else: # Not running validation\n",
    "            print('Starting Training (Without Validation)...')\n",
    "            train_history = model.fit(train_dataset,\n",
    "                                      shuffle=True,\n",
    "                                      epochs=epochs,\n",
    "                                      initial_epoch=initial_epoch_number,\n",
    "                                      steps_per_epoch=train_steps_per_epoch,\n",
    "                                      callbacks=callbacks)                \n",
    "            print(train_history)\n",
    "\n",
    "        if run_test:\n",
    "            test_data_filenames = glob(os.path.join(test_data, '*.tfrecord'))\n",
    "            print('test_data_filenames {}'.format(test_data_filenames))\n",
    "            test_dataset = file_based_input_dataset_builder(\n",
    "                channel='test',\n",
    "                input_filenames=test_data_filenames,\n",
    "                pipe_mode=pipe_mode,\n",
    "                is_training=False,\n",
    "                drop_remainder=False,\n",
    "                batch_size=test_batch_size,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=test_steps,\n",
    "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\n",
    "\n",
    "            print('Starting test...')\n",
    "            test_history = model.evaluate(test_dataset,\n",
    "                                          steps=test_steps,\n",
    "                                          callbacks=callbacks)\n",
    "                                 \n",
    "            print('Test history {}'.format(test_history))\n",
    "            \n",
    "        # Save the Fine-Yuned Transformers Model as a New \"Pre-Trained\" Model\n",
    "        print(\"########  Save the Fine-Yuned Transformers Model as a New Pre-Trained Model ##########\")\n",
    "        print('transformer_fine_tuned_model_path {}'.format(transformer_fine_tuned_model_path))   \n",
    "        model.save_pretrained(transformer_fine_tuned_model_path)\n",
    "\n",
    "        # Save the TensorFlow SavedModel for Serving Predictions\n",
    "        print(\"########  Save the TensorFlow SavedModel for Serving Predictions Model ##########\")        \n",
    "        print('tensorflow_saved_model_path {}'.format(tensorflow_saved_model_path))   \n",
    "        model.save(tensorflow_saved_model_path, save_format='tf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment Variables for a Script Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM_CHANNEL_TRAIN = 'data/output/bert/train'\n",
    "SM_CHANNEL_VALIDATION = 'data/output/bert/validation'\n",
    "SM_CHANNEL_TEST = 'data/output/bert/test'\n",
    "SM_OUTPUT_DIR = 'output'\n",
    "SM_HOSTS={}\n",
    "SM_CURRENT_HOST = 'LOCAL_NOTEBOOK'\n",
    "SM_NUM_GPUS = 0\n",
    "# SM_TRAINING_ENV = {}\n",
    "SAGEMAKER_JOB_NAME = \"LOCAL_JOB\"\n",
    "SM_MODEL_DIR = 'output/model'\n",
    "\n",
    "os.makedirs(SM_CHANNEL_TRAIN, exist_ok=True)\n",
    "os.makedirs(SM_CHANNEL_VALIDATION, exist_ok=True)\n",
    "os.makedirs(SM_CHANNEL_TEST, exist_ok=True)\n",
    "os.makedirs(SM_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(SM_MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_CHANNEL_TRAIN=data/output/bert/train\n",
      "env: SM_CHANNEL_VALIDATION=data/output/bert/validation\n",
      "env: SM_CHANNEL_TEST=data/output/bert/test\n",
      "env: SM_OUTPUT_DIR=output\n",
      "env: SM_HOSTS={}\n",
      "env: SM_CURRENT_HOST=LOCAL_NOTEBOOK\n",
      "env: SM_NUM_GPUS=0\n",
      "env: SAGEMAKER_JOB_NAME=LOCAL_JOB\n",
      "env: SM_MODEL_DIR=output/model\n"
     ]
    }
   ],
   "source": [
    "%env SM_CHANNEL_TRAIN={SM_CHANNEL_TRAIN}\n",
    "%env SM_CHANNEL_VALIDATION={SM_CHANNEL_VALIDATION}\n",
    "%env SM_CHANNEL_TEST={SM_CHANNEL_TEST}\n",
    "%env SM_OUTPUT_DIR={SM_OUTPUT_DIR}\n",
    "%env SM_HOSTS={SM_HOSTS}\n",
    "%env SM_CURRENT_HOST={SM_CURRENT_HOST}\n",
    "%env SM_NUM_GPUS={SM_NUM_GPUS}\n",
    "# %env SM_TRAINING_ENV = {SM_TRAINING_ENV}\n",
    "%env SAGEMAKER_JOB_NAME={SAGEMAKER_JOB_NAME}\n",
    "%env SM_MODEL_DIR={SM_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Default Command line Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts = 'LOCAL_NOTEBOOK'\n",
    "checkpoint_base_path = 'output/checkpoint'\n",
    "os.makedirs(checkpoint_base_path, exist_ok=True)\n",
    "max_seq_length = 128\n",
    "train_batch_size = 128\n",
    "validation_batch_size = 256\n",
    "test_batch_size = 256\n",
    "epochs = 1\n",
    "learning_rate = 0.00003\n",
    "freeze_bert_layer = False\n",
    "run_validation = True\n",
    "run_test = True\n",
    "run_sample_predictions = False\n",
    "enable_checkpointing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-22 01:07:47.799413: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-07-22 01:07:47.799510: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-07-22 01:07:47.799522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Requirement already satisfied: transformers==2.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.1.91)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2.23.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.18.1)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.14.16)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2020.6.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (4.44.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.7)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (1.17.16)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3->transformers==2.8.0) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3->transformers==2.8.0) (0.15.2)\n",
      "Requirement already satisfied: sagemaker-tensorflow==2.1.0.1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.1.0.1.0.0)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (1.4.1)\n",
      "################ Args: #######################\n",
      "Namespace(checkpoint_base_path='output/checkpoint', current_host='LOCAL_NOTEBOOK', enable_checkpointing=False, epochs=1, epsilon=1e-08, freeze_bert_layer=False, hosts=['L', 'O', 'C', 'A', 'L', '_', 'N', 'O', 'T', 'E', 'B', 'O', 'O', 'K'], learning_rate=3e-05, max_seq_length=128, model_dir='output/model', num_gpus=0, output_dir='output', run_sample_predictions=False, run_test=True, run_validation=False, test_batch_size=256, test_data='data/output/bert/test', test_steps=1, train_batch_size=128, train_data='data/output/bert/train', train_steps_per_epoch=1, use_amp=False, use_xla=False, validation_batch_size=256, validation_data='data/output/bert/validation', validation_steps=1)\n",
      "################ Environment Variables: ################\n",
      "{'AWS_AUTO_SCALING_HOME': '/opt/aws/apitools/as',\n",
      " 'AWS_CLOUDWATCH_HOME': '/opt/aws/apitools/mon',\n",
      " 'AWS_ELB_HOME': '/opt/aws/apitools/elb',\n",
      " 'AWS_PATH': '/opt/aws',\n",
      " 'BASH_FUNC_module()': '() '\n",
      "                       '{  '\n",
      "                       'eval '\n",
      "                       '`/usr/bin/modulecmd '\n",
      "                       'bash '\n",
      "                       '$*`\\n'\n",
      "                       '}',\n",
      " 'CLICOLOR': '1',\n",
      " 'CONDA_BACKUP_JAVA_HOME': '/usr/lib/jvm/java',\n",
      " 'CONDA_BACKUP_JAVA_LD_LIBRARY_PATH': '',\n",
      " 'CONDA_DEFAULT_ENV': 'python3',\n",
      " 'CONDA_EXE': '/home/ec2-user/anaconda3/bin/conda',\n",
      " 'CONDA_PREFIX': '/home/ec2-user/anaconda3/envs/python3',\n",
      " 'CONDA_PREFIX_1': '/home/ec2-user/anaconda3/envs/JupyterSystemEnv',\n",
      " 'CONDA_PREFIX_2': '/home/ec2-user/anaconda3',\n",
      " 'CONDA_PROMPT_MODIFIER': '(python3) ',\n",
      " 'CONDA_PYTHON_EXE': '/home/ec2-user/anaconda3/bin/python',\n",
      " 'CONDA_SHLVL': '3',\n",
      " 'CUDA_PATH': '/usr/local/cuda-10.0',\n",
      " 'CVS_RSH': 'ssh',\n",
      " 'EC2_AMITOOL_HOME': '/opt/aws/amitools/ec2',\n",
      " 'EC2_HOME': '/opt/aws/apitools/ec2',\n",
      " 'ENV_NAME': 'python3',\n",
      " 'GIT_PAGER': 'cat',\n",
      " 'GIT_PYTHON_REFRESH': 'quiet',\n",
      " 'HISTCONTROL': 'ignoredups',\n",
      " 'HISTSIZE': '1000',\n",
      " 'HOME': '/home/ec2-user',\n",
      " 'HOSTNAME': 'ip-172-16-42-162',\n",
      " 'JAVA_HOME': '/usr/lib/jvm/java',\n",
      " 'JAVA_LD_LIBRARY_PATH': '/home/ec2-user/anaconda3/envs/JupyterSystemEnv/jre/lib/amd64/server',\n",
      " 'JPY_PARENT_PID': '4800',\n",
      " 'KMP_DUPLICATE_LIB_OK': 'True',\n",
      " 'KMP_INIT_AT_FORK': 'FALSE',\n",
      " 'LANG': 'en_US.UTF-8',\n",
      " 'LD_LIBRARY_PATH': '/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:',\n",
      " 'LD_LIBRARY_PATH_WITHOUT_CUDA': '/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:',\n",
      " 'LD_LIBRARY_PATH_WITH_DEFAULT_CUDA': '/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:',\n",
      " 'LESSOPEN': '||/usr/bin/lesspipe.sh '\n",
      "             '%s',\n",
      " 'LESS_TERMCAP_mb': '\\x1b[01;31m',\n",
      " 'LESS_TERMCAP_md': '\\x1b[01;38;5;208m',\n",
      " 'LESS_TERMCAP_me': '\\x1b[0m',\n",
      " 'LESS_TERMCAP_se': '\\x1b[0m',\n",
      " 'LESS_TERMCAP_ue': '\\x1b[0m',\n",
      " 'LESS_TERMCAP_us': '\\x1b[04;38;5;111m',\n",
      " 'LOADEDMODULES': '',\n",
      " 'LOGNAME': 'ec2-user',\n",
      " 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:',\n",
      " 'MAIL': '/var/spool/mail/ec2-user',\n",
      " 'MANPATH': '/opt/aws/neuron/share/man:',\n",
      " 'MODULEPATH': '/usr/share/Modules/modulefiles:/etc/modulefiles',\n",
      " 'MODULESHOME': '/usr/share/Modules',\n",
      " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
      " 'PAGER': 'cat',\n",
      " 'PATH': '/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ec2-user/anaconda3/envs/python3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ec2-user/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/home/ec2-user/anaconda3/bin:/home/ec2-user/anaconda3/condabin:/home/ec2-user/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/opt/aws/bin',\n",
      " 'PKG_CONFIG_PATH': '/usr/local/lib/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/lib/pkgconfig:',\n",
      " 'PWD': '/home/ec2-user/SageMaker/RecommendEmoticon/Tweet-BERT',\n",
      " 'PYTHON_INSTALL_LAYOUT': 'amzn',\n",
      " 'PYTHON_VERSION': '3.6',\n",
      " 'SAGEMAKER_JOB_NAME': 'LOCAL_JOB',\n",
      " 'SHELL': '/bin/sh',\n",
      " 'SHLVL': '2',\n",
      " 'SM_CHANNEL_TEST': 'data/output/bert/test',\n",
      " 'SM_CHANNEL_TRAIN': 'data/output/bert/train',\n",
      " 'SM_CHANNEL_VALIDATION': 'data/output/bert/validation',\n",
      " 'SM_CURRENT_HOST': 'LOCAL_NOTEBOOK',\n",
      " 'SM_HOSTS': '{}',\n",
      " 'SM_MODEL_DIR': 'output/model',\n",
      " 'SM_NUM_GPUS': '0',\n",
      " 'SM_OUTPUT_DIR': 'output',\n",
      " 'TERM': 'xterm-color',\n",
      " 'USER': 'ec2-user',\n",
      " '_': '/home/ec2-user/anaconda3/envs/python3/bin/python',\n",
      " '_CE_CONDA': '',\n",
      " '_CE_M': ''}\n",
      "################ Extract varaibles from Command Args #######################\n",
      "is_master True\n",
      "train_data data/output/bert/train\n",
      "validation_data data/output/bert/validation\n",
      "test_data data/output/bert/test\n",
      "local_model_dir output/model\n",
      "output_dir output\n",
      "hosts ['L', 'O', 'C', 'A', 'L', '_', 'N', 'O', 'T', 'E', 'B', 'O', 'O', 'K']\n",
      "current_host LOCAL_NOTEBOOK\n",
      "num_gpus 0\n",
      "job_name LOCAL_JOB\n",
      "use_xla False\n",
      "use_amp False\n",
      "max_seq_length 128\n",
      "train_batch_size 128\n",
      "validation_batch_size 256\n",
      "test_batch_size 256\n",
      "epochs 1\n",
      "learning_rate 3e-05\n",
      "epsilon 1e-08\n",
      "train_steps_per_epoch 1\n",
      "validation_steps 1\n",
      "test_steps 1\n",
      "freeze_bert_layer False\n",
      "run_validation False\n",
      "run_test True\n",
      "run_sample_predictions False\n",
      "enable_checkpointing False\n",
      "checkpoint_base_path output/checkpoint\n",
      " ################ Set Checkpoint path: ################\n",
      "checkpoint_path output/checkpoint\n",
      "Using pipe_mode: False\n",
      "################ Mirrored distributed_strategy ################\n",
      "2020-07-22 01:07:54.822626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-07-22 01:07:54.835515: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2020-07-22 01:07:54.835568: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-16-42-162): /proc/driver/nvidia/version does not exist\n",
      "2020-07-22 01:07:54.836826: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2020-07-22 01:07:54.877802: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\n",
      "2020-07-22 01:07:54.879014: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cb3adbfbc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-22 01:07:54.879042: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "train_data_filenames ['data/output/bert/train/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/train/part-unknown-tweet_file_02.tfrecord']\n",
      "***** Using input_filenames ['data/output/bert/train/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/train/part-unknown-tweet_file_02.tfrecord']\n",
      "WARNING:tensorflow:From tf_script_bert_tweet.py:94: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "2020-07-22 01:07:56.040669: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Sucessfully downloaded after 0 retries.\n",
      "** use_amp False\n",
      "*** OPTIMIZER <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fbde00e6470> ***\n",
      "Compiled model <transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification object at 0x7fbde04212b0>\n",
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  7690      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,961,162\n",
      "Trainable params: 66,961,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Starting Training (Without Validation)...\n",
      "Train for 1 steps\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "! python tf_script_bert_tweet.py \\\n",
    "    --hosts {hosts} \\\n",
    "    --checkpoint_base_path {checkpoint_base_path} \\\n",
    "    --epochs {epochs} \\\n",
    "    --run_test {run_test}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-22 01:08:18.282055: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-07-22 01:08:18.282455: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-07-22 01:08:18.282468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Requirement already satisfied: transformers==2.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.1.91)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.7)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.18.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2020.6.8)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (4.44.1)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.14.16)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (3.0.12)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2020.4.5.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (1.17.16)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3->transformers==2.8.0) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3->transformers==2.8.0) (2.8.1)\n",
      "Requirement already satisfied: sagemaker-tensorflow==2.1.0.1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.1.0.1.0.0)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.23.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn==0.23.1) (0.14.1)\n",
      "################ Args: #######################\n",
      "Namespace(checkpoint_base_path='output/checkpoint', current_host='LOCAL_NOTEBOOK', enable_checkpointing=False, epochs=1, epsilon=1e-08, freeze_bert_layer=False, hosts=['L', 'O', 'C', 'A', 'L', '_', 'N', 'O', 'T', 'E', 'B', 'O', 'O', 'K'], learning_rate=3e-05, max_seq_length=128, model_dir='output/model', num_gpus=0, output_dir='output', run_sample_predictions=False, run_test=False, run_validation=True, test_batch_size=256, test_data='data/output/bert/test', test_steps=1, train_batch_size=128, train_data='data/output/bert/train', train_steps_per_epoch=1, use_amp=False, use_xla=False, validation_batch_size=256, validation_data='data/output/bert/validation', validation_steps=1)\n",
      "################ Environment Variables: ################\n",
      "{'AWS_AUTO_SCALING_HOME': '/opt/aws/apitools/as',\n",
      " 'AWS_CLOUDWATCH_HOME': '/opt/aws/apitools/mon',\n",
      " 'AWS_ELB_HOME': '/opt/aws/apitools/elb',\n",
      " 'AWS_PATH': '/opt/aws',\n",
      " 'BASH_FUNC_module()': '() '\n",
      "                       '{  '\n",
      "                       'eval '\n",
      "                       '`/usr/bin/modulecmd '\n",
      "                       'bash '\n",
      "                       '$*`\\n'\n",
      "                       '}',\n",
      " 'CLICOLOR': '1',\n",
      " 'CONDA_BACKUP_JAVA_HOME': '/usr/lib/jvm/java',\n",
      " 'CONDA_BACKUP_JAVA_LD_LIBRARY_PATH': '',\n",
      " 'CONDA_DEFAULT_ENV': 'python3',\n",
      " 'CONDA_EXE': '/home/ec2-user/anaconda3/bin/conda',\n",
      " 'CONDA_PREFIX': '/home/ec2-user/anaconda3/envs/python3',\n",
      " 'CONDA_PREFIX_1': '/home/ec2-user/anaconda3/envs/JupyterSystemEnv',\n",
      " 'CONDA_PREFIX_2': '/home/ec2-user/anaconda3',\n",
      " 'CONDA_PROMPT_MODIFIER': '(python3) ',\n",
      " 'CONDA_PYTHON_EXE': '/home/ec2-user/anaconda3/bin/python',\n",
      " 'CONDA_SHLVL': '3',\n",
      " 'CUDA_PATH': '/usr/local/cuda-10.0',\n",
      " 'CVS_RSH': 'ssh',\n",
      " 'EC2_AMITOOL_HOME': '/opt/aws/amitools/ec2',\n",
      " 'EC2_HOME': '/opt/aws/apitools/ec2',\n",
      " 'ENV_NAME': 'python3',\n",
      " 'GIT_PAGER': 'cat',\n",
      " 'GIT_PYTHON_REFRESH': 'quiet',\n",
      " 'HISTCONTROL': 'ignoredups',\n",
      " 'HISTSIZE': '1000',\n",
      " 'HOME': '/home/ec2-user',\n",
      " 'HOSTNAME': 'ip-172-16-42-162',\n",
      " 'JAVA_HOME': '/usr/lib/jvm/java',\n",
      " 'JAVA_LD_LIBRARY_PATH': '/home/ec2-user/anaconda3/envs/JupyterSystemEnv/jre/lib/amd64/server',\n",
      " 'JPY_PARENT_PID': '4800',\n",
      " 'KMP_DUPLICATE_LIB_OK': 'True',\n",
      " 'KMP_INIT_AT_FORK': 'FALSE',\n",
      " 'LANG': 'en_US.UTF-8',\n",
      " 'LD_LIBRARY_PATH': '/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:',\n",
      " 'LD_LIBRARY_PATH_WITHOUT_CUDA': '/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:',\n",
      " 'LD_LIBRARY_PATH_WITH_DEFAULT_CUDA': '/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:',\n",
      " 'LESSOPEN': '||/usr/bin/lesspipe.sh '\n",
      "             '%s',\n",
      " 'LESS_TERMCAP_mb': '\\x1b[01;31m',\n",
      " 'LESS_TERMCAP_md': '\\x1b[01;38;5;208m',\n",
      " 'LESS_TERMCAP_me': '\\x1b[0m',\n",
      " 'LESS_TERMCAP_se': '\\x1b[0m',\n",
      " 'LESS_TERMCAP_ue': '\\x1b[0m',\n",
      " 'LESS_TERMCAP_us': '\\x1b[04;38;5;111m',\n",
      " 'LOADEDMODULES': '',\n",
      " 'LOGNAME': 'ec2-user',\n",
      " 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:',\n",
      " 'MAIL': '/var/spool/mail/ec2-user',\n",
      " 'MANPATH': '/opt/aws/neuron/share/man:',\n",
      " 'MODULEPATH': '/usr/share/Modules/modulefiles:/etc/modulefiles',\n",
      " 'MODULESHOME': '/usr/share/Modules',\n",
      " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
      " 'PAGER': 'cat',\n",
      " 'PATH': '/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/home/ec2-user/anaconda3/envs/python3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ec2-user/.dl_binaries/bin:/usr/local/mpi/bin:/opt/aws/neuron/bin:/home/ec2-user/anaconda3/bin:/home/ec2-user/anaconda3/condabin:/home/ec2-user/anaconda3/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/libexec/gcc/x86_64-amazon-linux/4.8.5:/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/opt/aws/bin',\n",
      " 'PKG_CONFIG_PATH': '/usr/local/lib/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/lib/pkgconfig:',\n",
      " 'PWD': '/home/ec2-user/SageMaker/RecommendEmoticon/Tweet-BERT',\n",
      " 'PYTHON_INSTALL_LAYOUT': 'amzn',\n",
      " 'PYTHON_VERSION': '3.6',\n",
      " 'SAGEMAKER_JOB_NAME': 'LOCAL_JOB',\n",
      " 'SHELL': '/bin/sh',\n",
      " 'SHLVL': '2',\n",
      " 'SM_CHANNEL_TEST': 'data/output/bert/test',\n",
      " 'SM_CHANNEL_TRAIN': 'data/output/bert/train',\n",
      " 'SM_CHANNEL_VALIDATION': 'data/output/bert/validation',\n",
      " 'SM_CURRENT_HOST': 'LOCAL_NOTEBOOK',\n",
      " 'SM_HOSTS': '{}',\n",
      " 'SM_MODEL_DIR': 'output/model',\n",
      " 'SM_NUM_GPUS': '0',\n",
      " 'SM_OUTPUT_DIR': 'output',\n",
      " 'TERM': 'xterm-color',\n",
      " 'USER': 'ec2-user',\n",
      " '_': '/home/ec2-user/anaconda3/envs/python3/bin/python',\n",
      " '_CE_CONDA': '',\n",
      " '_CE_M': ''}\n",
      "################ Extract varaibles from Command Args #######################\n",
      "is_master True\n",
      "train_data data/output/bert/train\n",
      "validation_data data/output/bert/validation\n",
      "test_data data/output/bert/test\n",
      "local_model_dir output/model\n",
      "output_dir output\n",
      "hosts ['L', 'O', 'C', 'A', 'L', '_', 'N', 'O', 'T', 'E', 'B', 'O', 'O', 'K']\n",
      "current_host LOCAL_NOTEBOOK\n",
      "num_gpus 0\n",
      "job_name LOCAL_JOB\n",
      "use_xla False\n",
      "use_amp False\n",
      "max_seq_length 128\n",
      "train_batch_size 128\n",
      "validation_batch_size 256\n",
      "test_batch_size 256\n",
      "epochs 1\n",
      "learning_rate 3e-05\n",
      "epsilon 1e-08\n",
      "train_steps_per_epoch 1\n",
      "validation_steps 1\n",
      "test_steps 1\n",
      "freeze_bert_layer False\n",
      "run_validation True\n",
      "run_test False\n",
      "run_sample_predictions False\n",
      "enable_checkpointing False\n",
      "checkpoint_base_path output/checkpoint\n",
      " ################ Set Checkpoint path: ################\n",
      "checkpoint_path output/checkpoint\n",
      "Using pipe_mode: False\n",
      "################ Mirrored distributed_strategy ################\n",
      "2020-07-22 01:08:25.348585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-07-22 01:08:25.377128: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2020-07-22 01:08:25.377183: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-16-42-162): /proc/driver/nvidia/version does not exist\n",
      "2020-07-22 01:08:25.378000: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2020-07-22 01:08:25.409807: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\n",
      "2020-07-22 01:08:25.410624: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f0f0fdab00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-22 01:08:25.410649: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "train_data_filenames ['data/output/bert/train/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/train/part-unknown-tweet_file_02.tfrecord']\n",
      "***** Using input_filenames ['data/output/bert/train/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/train/part-unknown-tweet_file_02.tfrecord']\n",
      "WARNING:tensorflow:From tf_script_bert_tweet.py:94: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "2020-07-22 01:08:26.627394: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Sucessfully downloaded after 0 retries.\n",
      "** use_amp False\n",
      "*** OPTIMIZER <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f1cec2f4a20> ***\n",
      "Compiled model <transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification object at 0x7f1cfc355b70>\n",
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  7690      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,961,162\n",
      "Trainable params: 66,961,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "validation_data_filenames ['data/output/bert/validation/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/validation/part-unknown-tweet_file_02.tfrecord']\n",
      "***** Using input_filenames ['data/output/bert/validation/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/validation/part-unknown-tweet_file_02.tfrecord']\n",
      "Starting Training and Validation...\n",
      "Train for 1 steps, validate for 1 steps\n"
     ]
    }
   ],
   "source": [
    "# Validation \n",
    "! python tf_script_bert_tweet.py \\\n",
    "    --hosts {hosts} \\\n",
    "    --checkpoint_base_path {checkpoint_base_path} \\\n",
    "    --epochs {epochs} \\\n",
    "    --run_validation {run_validation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
