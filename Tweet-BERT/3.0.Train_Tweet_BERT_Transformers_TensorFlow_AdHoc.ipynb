{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 3.0] Fine-Tuning a BERT Model and Create a Text Classifier\n",
    "\n",
    "이 노트북은 레이블의 수(5개 --> 10개)를 제외하고 아래 참조의 노트북을 재실행을 함. \n",
    "아래와 같은 작업을 진행 함\n",
    "\n",
    "- TF에 입력할 Data Builder 정의\n",
    "- Train, Validation, Test Dataset 생성\n",
    "- Pre-trained BERT with classification layer on top 를 로딩시에 에 10개의 레이블 정보를 제공\n",
    "- Train 데이타를 가지고 학습 및 모델 저장\n",
    "- 저장된 모엘을 Reload를 해서 Test 데이타를 추론 함\n",
    "- Accuracy 및 Confusion Matrix를 생성\n",
    "\n",
    "---\n",
    "이 노트북은 약 3분 정도 소요 됩니다.\n",
    "\n",
    "---\n",
    "Reference\n",
    "- Fine-Tuning a BERT Model and Create a Text Classifier\n",
    "- https://github.com/data-science-on-aws/workshop/blob/master/07_train/01_Train_Reviews_BERT_Transformers_TensorFlow_AdHoc.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.2)\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "astroid 2.3.3 requires wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q wrapt --upgrade --ignore-installed\n",
    "!pip install -q tensorflow==2.1.0\n",
    "!pip install -q transformers==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers.configuration_distilbert import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 30\n",
    "steps = 1000\n",
    "STEPS_PER_EPOCH= steps\n",
    "VALIDATION_STEPS= steps\n",
    "TEST_STEPS= steps\n",
    "\n",
    "# VALIDATION_STEPS= int(steps / 2)\n",
    "# TEST_STEPS= int(steps / 2)\n",
    "\n",
    "learning_rate = 4e-5\n",
    "BATCH_SIZE= 128\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH= 32\n",
    "CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "NUM_GPUS=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 셋 생성위한 함수 (TF 위한 Data Builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record['label_ids']\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_dataset_builder(channel,\n",
    "                                     input_filenames,\n",
    "                                     pipe_mode,\n",
    "                                     is_training,\n",
    "                                     drop_remainder):\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "\n",
    "    if pipe_mode:\n",
    "        print('***** Using pipe_mode with channel {}'.format(channel))\n",
    "        from sagemaker_tensorflow import PipeModeDataset\n",
    "        dataset = PipeModeDataset(channel=channel,\n",
    "                                  record_format='TFRecord')\n",
    "    else:\n",
    "        print('***** Using input_filenames {}'.format(input_filenames))\n",
    "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
    "\n",
    "    dataset = dataset.repeat(EPOCHS * STEPS_PER_EPOCH)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    name_to_features = {\n",
    "      \"input_ids\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "      \"input_mask\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "      \"segment_ids\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        return tf.io.parse_single_example(record, name_to_features)\n",
    "        \n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "          lambda record: _decode_record(record, name_to_features),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          drop_remainder=drop_remainder,\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    dataset.cache()\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(seed=42,\n",
    "                                  buffer_size=10,\n",
    "                                  reshuffle_each_iteration=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'data/output/bert/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Dataset 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 개의 TF Record 파일을 가지고 Train Dataset을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_filenames ['data/output/bert/train/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/train/part-unknown-tweet_file_02.tfrecord']\n",
      "***** Using input_filenames ['data/output/bert/train/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/train/part-unknown-tweet_file_02.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "train_data_filenames = glob('{}/*.tfrecord'.format(train_data))\n",
    "print('train_data_filenames {}'.format(train_data_filenames))\n",
    "train_dataset = file_based_input_dataset_builder(\n",
    "    channel = 'train',\n",
    "    input_filenames = train_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=True,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ({input_ids: (None, 32), input_mask: (None, 32), segment_ids: (None, 32)}, (None,)), types: ({input_ids: tf.int64, input_mask: tf.int64, segment_ids: tf.int64}, tf.int64)>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = 'data/output/bert/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_data_filenames ['data/output/bert/validation/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/validation/part-unknown-tweet_file_02.tfrecord']\n",
      "***** Using input_filenames ['data/output/bert/validation/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/validation/part-unknown-tweet_file_02.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "validation_data_filenames = glob('{}/*.tfrecord'.format(validation_data))\n",
    "\n",
    "print('validation_data_filenames {}'.format(validation_data_filenames))\n",
    "validation_dataset = file_based_input_dataset_builder(\n",
    "    channel='validation',\n",
    "    input_filenames=validation_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=False,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = 'data/output/bert/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/output/bert/test/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/test/part-unknown-tweet_file_02.tfrecord']\n",
      "***** Using input_filenames ['data/output/bert/test/part-unknown-tweet_file_01.tfrecord', 'data/output/bert/test/part-unknown-tweet_file_02.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "test_data_filenames = glob('{}/*.tfrecord'.format(test_data))\n",
    "\n",
    "print(test_data_filenames)\n",
    "\n",
    "test_dataset = file_based_input_dataset_builder(\n",
    "    channel='test',\n",
    "    input_filenames=test_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=False,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained BERT Model 에 10개의 레이블 제공\n",
    "- TFDistilBertForSequenceClassification 는 BERT Layer에 Classification Layer를 가지고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조: TFDistilBertForSequenceClassification\n",
    "- https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertforsequenceclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\n",
    "                                          num_labels = len(CLASSES)\n",
    "                                         )\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Custom Classifier Model \n",
    "Pre-trained 모델 정의를 변경 함 \n",
    "\n",
    "- TFDistilBertForSequenceClassification는 tf.keras.Model의 sub-class 임\n",
    "- BERT Layer만을 Freezing을 시키고 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  7690      \n",
      "_________________________________________________________________\n",
      "dropout_259 (Dropout)        multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,961,162\n",
      "Trainable params: 598,282\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate= learning_rate, epsilon=1e-08)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.layers[0].trainable = False # BERT Layer를 Freezing\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "log_dir = './tensorboard/'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks.append(tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1000 steps, validate for 1000 steps\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 108s 108ms/step - loss: 2.1456 - accuracy: 0.2455 - val_loss: 2.2089 - val_accuracy: 0.1364\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.9032 - accuracy: 0.3504 - val_loss: 2.1471 - val_accuracy: 0.2045\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.7509 - accuracy: 0.4066 - val_loss: 2.1230 - val_accuracy: 0.1818\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.6386 - accuracy: 0.4467 - val_loss: 2.1293 - val_accuracy: 0.1818\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.5451 - accuracy: 0.4775 - val_loss: 2.1705 - val_accuracy: 0.1818\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.4712 - accuracy: 0.5035 - val_loss: 2.2151 - val_accuracy: 0.1591\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.4043 - accuracy: 0.5279 - val_loss: 2.2710 - val_accuracy: 0.1591\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.3499 - accuracy: 0.5458 - val_loss: 2.2833 - val_accuracy: 0.1818\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.2949 - accuracy: 0.5657 - val_loss: 2.3481 - val_accuracy: 0.1591\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.2483 - accuracy: 0.5789 - val_loss: 2.3718 - val_accuracy: 0.1591\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.2043 - accuracy: 0.5952 - val_loss: 2.4246 - val_accuracy: 0.1818\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.1632 - accuracy: 0.6091 - val_loss: 2.4363 - val_accuracy: 0.1818\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.1280 - accuracy: 0.6234 - val_loss: 2.4778 - val_accuracy: 0.1818\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.0914 - accuracy: 0.6348 - val_loss: 2.5249 - val_accuracy: 0.1591\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 100s 100ms/step - loss: 1.0548 - accuracy: 0.6463 - val_loss: 2.5306 - val_accuracy: 0.2045\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 1.0187 - accuracy: 0.6592 - val_loss: 2.5872 - val_accuracy: 0.1818\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 100s 100ms/step - loss: 0.9883 - accuracy: 0.6712 - val_loss: 2.6012 - val_accuracy: 0.1818\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 100s 100ms/step - loss: 0.9543 - accuracy: 0.6832 - val_loss: 2.6601 - val_accuracy: 0.2045\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 100s 100ms/step - loss: 0.9231 - accuracy: 0.6949 - val_loss: 2.6652 - val_accuracy: 0.2045\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 100s 100ms/step - loss: 0.8958 - accuracy: 0.7047 - val_loss: 2.6947 - val_accuracy: 0.2045\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 100s 100ms/step - loss: 0.8639 - accuracy: 0.7152 - val_loss: 2.7225 - val_accuracy: 0.2045\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 100s 100ms/step - loss: 0.8362 - accuracy: 0.7251 - val_loss: 2.7687 - val_accuracy: 0.2045\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 0.8087 - accuracy: 0.7352 - val_loss: 2.7854 - val_accuracy: 0.2045\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 100s 100ms/step - loss: 0.7767 - accuracy: 0.7472 - val_loss: 2.8177 - val_accuracy: 0.2045\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 0.7517 - accuracy: 0.7557 - val_loss: 2.8492 - val_accuracy: 0.1591\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 101s 101ms/step - loss: 0.7268 - accuracy: 0.7655 - val_loss: 2.8709 - val_accuracy: 0.2045\n",
      "Epoch 27/30\n",
      " 348/1000 [=========>....................] - ETA: 33s - loss: 0.7122 - accuracy: 0.7706"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    shuffle=True,\n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=VALIDATION_STEPS,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history.keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trained model {}'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Holdout Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history = model.evaluate(test_dataset, steps=TEST_STEPS, callbacks = callbacks)\n",
    "print(test_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './fine-tuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $model_dir\n",
    "model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat $model_dir/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습된 모델을 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(model_dir,\n",
    "                                                                     id2label={\n",
    "                                                                       0: 0,\n",
    "                                                                       1: 1,\n",
    "                                                                       2: 2,\n",
    "                                                                       3: 3,\n",
    "                                                                       4: 4,\n",
    "                                                                       5: 5,\n",
    "                                                                       6: 6,\n",
    "                                                                       7: 7,\n",
    "                                                                       8: 8,\n",
    "                                                                       9: 9                                                                         \n",
    "                                                                     },\n",
    "                                                                     label2id={\n",
    "                                                                       0: 0,\n",
    "                                                                       1: 1,\n",
    "                                                                       2: 2,\n",
    "                                                                       3: 3,\n",
    "                                                                       4: 4,\n",
    "                                                                       5: 5,\n",
    "                                                                       6: 6,\n",
    "                                                                       7: 7,\n",
    "                                                                       8: 8,\n",
    "                                                                       9: 9                                                                         \n",
    "                                                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "if NUM_GPUS >= 1:\n",
    "    inference_device = 0 # GPU 0\n",
    "else:\n",
    "    inference_device = -1 # CPU\n",
    "print('inference_device {}'.format(inference_device))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "inference_pipeline = TextClassificationPipeline(model = loaded_model,\n",
    "                                                tokenizer = tokenizer,\n",
    "                                                framework = 'tf',\n",
    "                                                device = inference_device) # -1 is CPU, 0 is GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\n",
    "print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gz_file = 'data/split/tweet_file_01.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "df_sample_reviews = pd.read_csv(train_gz_file, \n",
    "                                compression='gzip')[['TWEET', 'LABEL']].sample(n=100)\n",
    "print(df_sample_reviews.shape)\n",
    "print(df_sample_reviews.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict(review_body):\n",
    "    prediction_map = inference_pipeline(review_body)\n",
    "    return prediction_map[0]['label']\n",
    "\n",
    "y_pred = df_sample_reviews['TWEET'].map(predict)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_sample_reviews['LABEL']\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_true, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_pred=y_pred, y_true=y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "def plot_conf_mat(cm, classes, title, cmap = plt.cm.Greens):\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plot_conf_mat(cm, \n",
    "              classes=['0','1', '2', '3', '4', '5','6','7','8','9'], \n",
    "              title='Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
