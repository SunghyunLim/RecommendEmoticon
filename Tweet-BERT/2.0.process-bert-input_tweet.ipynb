{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Raw Text into BERT Embeddings with Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Preprocess Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess-scikit-text-to-bert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess-scikit-text-to-bert.py\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import functools\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "DATA_COLUMN = 'TWEET'\n",
    "LABEL_COLUMN = 'LABEL'\n",
    "LABEL_VALUES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"BERT feature vectors.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    \n",
    "    \n",
    "class Input(object):\n",
    "  \"\"\"A single training/test input for sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, text, label=None):\n",
    "    \"\"\"Constructs an Input.\n",
    "    Args:\n",
    "      text: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.label = label\n",
    "\n",
    "def convert_input(text_input, max_seq_length):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    #\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # \n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "    #\n",
    "    tokens = tokenizer.tokenize(text_input.text)    \n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=max_seq_length)\n",
    "\n",
    "    # Convert the text-based tokens to ids from the pre-trained BERT vocabulary\n",
    "    input_ids = encode_plus_tokens['input_ids']\n",
    "    # Specifies which tokens BERT should pay attention to (0 or 1)\n",
    "    input_mask = encode_plus_tokens['attention_mask']\n",
    "    # Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # Label for our training data (star_rating 1 through 5)\n",
    "    label_id = label_map[text_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id)\n",
    "\n",
    "    return features\n",
    "\n",
    "def convert_features_to_tfrecord(inputs,\n",
    "                                 output_file,\n",
    "                                 max_seq_length):\n",
    "    \"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, text_input) in enumerate(inputs):\n",
    "        if input_idx % 10000 == 0:\n",
    "            print(\"Writing example %d of %d\" % (input_idx, len(inputs)))\n",
    "\n",
    "            bert_features = convert_input(text_input, max_seq_length)\n",
    "        \n",
    "            tfrecord_features = collections.OrderedDict()\n",
    "            \n",
    "            tfrecord_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_ids))\n",
    "            tfrecord_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_mask))\n",
    "            tfrecord_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.segment_ids))\n",
    "            tfrecord_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[bert_features.label_id]))\n",
    "\n",
    "            tfrecord = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\n",
    "            \n",
    "            tfrecord_writer.write(tfrecord.SerializeToString())\n",
    "\n",
    "    tfrecord_writer.close()\n",
    "\n",
    "    \n",
    "    \n",
    "def list_arg(raw_value):\n",
    "    \"\"\"argparse type for a list of strings\"\"\"\n",
    "    return str(raw_value).split(',')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def _transform_tsv_to_tfrecord(file, \n",
    "                               max_seq_length, \n",
    "                               balance_dataset):\n",
    "    print('file {}'.format(file))\n",
    "    print('max_seq_length {}'.format(max_seq_length))\n",
    "    print('balance_dataset {}'.format(balance_dataset))\n",
    "\n",
    "    filename_without_extension = Path(Path(file).stem).stem\n",
    "\n",
    "    df = pd.read_csv(file, \n",
    "                     compression='gzip')\n",
    "\n",
    "    df.isna().values.any()\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of dataframe {}'.format(df.shape))\n",
    "\n",
    "        \n",
    "    print('Shape of dataframe before splitting {}'.format(df.shape))\n",
    "    \n",
    "    print('train split percentage {}'.format(args.train_split_percentage))\n",
    "    print('validation split percentage {}'.format(args.validation_split_percentage))\n",
    "    print('test split percentage {}'.format(args.test_split_percentage))    \n",
    "    \n",
    "    holdout_percentage = 1.00 - args.train_split_percentage\n",
    "    print('holdout percentage {}'.format(holdout_percentage))\n",
    "    df_train, df_holdout = train_test_split(df, \n",
    "                                            test_size=holdout_percentage, \n",
    "                                            stratify=df[LABEL_COLUMN])\n",
    "\n",
    "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
    "    print('test holdout percentage {}'.format(test_holdout_percentage))\n",
    "    df_validation, df_test = train_test_split(df_holdout, \n",
    "                                              test_size=test_holdout_percentage,\n",
    "                                              stratify=df_holdout[LABEL_COLUMN])\n",
    "    \n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_validation = df_validation.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of train dataframe {}'.format(df_train.shape))\n",
    "    print('Shape of validation dataframe {}'.format(df_validation.shape))\n",
    "    print('Shape of test dataframe {}'.format(df_test.shape))\n",
    "\n",
    "    train_inputs = df_train.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                         label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_inputs = df_validation.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                            label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    test_inputs = df_test.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    # Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "    # \n",
    "    # \n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # 4. Map our words to indexes using a vocab file that BERT provides\n",
    "    # 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "    # 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "    # \n",
    "    # We don't have to worry about these details.  The Transformers tokenizer does this for us.\n",
    "    # \n",
    "    train_data = '{}/bert/train'.format(args.output_data)\n",
    "    validation_data = '{}/bert/validation'.format(args.output_data)\n",
    "    test_data = '{}/bert/test'.format(args.output_data)\n",
    "\n",
    "    # Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\n",
    "    df_train_embeddings = convert_features_to_tfrecord(train_inputs, \n",
    "                                                       '{}/part-{}-{}.tfrecord'.format(train_data, args.current_host, filename_without_extension), \n",
    "                                                       max_seq_length)\n",
    "\n",
    "    df_validation_embeddings = convert_features_to_tfrecord(validation_inputs, '{}/part-{}-{}.tfrecord'.format(validation_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "\n",
    "    df_test_embeddings = convert_features_to_tfrecord(test_inputs, '{}/part-{}-{}.tfrecord'.format(test_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "        \n",
    "def parse_args():\n",
    "    # Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\n",
    "    resconfig = {}\n",
    "    try:\n",
    "        with open('/opt/ml/config/resourceconfig.json', 'r') as cfgfile:\n",
    "            resconfig = json.load(cfgfile)\n",
    "    except FileNotFoundError:\n",
    "        print('/opt/ml/config/resourceconfig.json not found.  current_host is unknown.')\n",
    "        pass # Ignore\n",
    "\n",
    "    # Local testing with CLI args\n",
    "    parser = argparse.ArgumentParser(description='Process')\n",
    "\n",
    "    parser.add_argument('--hosts', type=list_arg,\n",
    "        default=resconfig.get('hosts', ['unknown']),\n",
    "        help='Comma-separated list of host names running the job'\n",
    "    )\n",
    "    parser.add_argument('--current-host', type=str,\n",
    "        default=resconfig.get('current_host', 'unknown'),\n",
    "        help='Name of this host running the job'\n",
    "    )\n",
    "    parser.add_argument('--input-data', type=str,\n",
    "        default='/opt/ml/processing/input/data',\n",
    "    )\n",
    "    parser.add_argument('--output-data', type=str,\n",
    "        default='/opt/ml/processing/output',\n",
    "    )\n",
    "    parser.add_argument('--train-split-percentage', type=float,\n",
    "        default=0.90,\n",
    "    )\n",
    "    parser.add_argument('--validation-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )    \n",
    "    parser.add_argument('--test-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )\n",
    "    parser.add_argument('--balance-dataset', type=eval,\n",
    "        default=False\n",
    "    )\n",
    "    parser.add_argument('--max-seq-length', type=int,\n",
    "        default=128,\n",
    "    )  \n",
    "    \n",
    "    return parser.parse_args()\n",
    "        \n",
    "    \n",
    "def process(args):\n",
    "    print('Current host: {}'.format(args.current_host))\n",
    "    \n",
    "    train_data = None\n",
    "    validation_data = None\n",
    "    test_data = None\n",
    "\n",
    "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \n",
    "                                                 max_seq_length=args.max_seq_length,\n",
    "                                                 balance_dataset=args.balance_dataset\n",
    "\n",
    "    )\n",
    "    input_files = glob.glob('{}/*'.format(args.input_data))\n",
    "    print(\"********** input files ***************\")    \n",
    "    print(\"args.input_data: \", args.input_data)\n",
    "    print(\"input_files: \", input_files)\n",
    "\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print('num_cpus {}'.format(num_cpus))\n",
    "\n",
    "    p = multiprocessing.Pool(num_cpus)\n",
    "    p.map(transform_tsv_to_tfrecord, input_files)\n",
    "\n",
    "    print(\"********** Listing tf-record files ***************\")        \n",
    "    print('Listing contents of {}'.format(args.output_data))\n",
    "    dirs_output = os.listdir(args.output_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(train_data))\n",
    "    dirs_output = os.listdir(train_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(validation_data))\n",
    "    dirs_output = os.listdir(validation_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(test_data))\n",
    "    dirs_output = os.listdir(test_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    print('################ START #######################')    \n",
    "    print('Loaded arguments:')\n",
    "    print(args)\n",
    "    \n",
    "    print('Environment variables:')\n",
    "#     print(os.environ)\n",
    "\n",
    "    process(args)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Local Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_split_data_dir = 'data/split'\n",
    "output_data_dir = 'data/output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/output/bert/train\n",
    "! mkdir -p data/output/bert/validation\n",
    "! mkdir -p data/output/bert/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.1.0\n",
      "  Downloading tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "\u001b[K     |██████████████████████▎         | 293.3 MB 118.1 MB/s eta 0:00:02    |██████▏                         | 81.7 MB 117.6 MB/s eta 0:00:03     |████████████▎                   | 161.6 MB 117.6 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 421.8 MB 20 kB/s \n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 110.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 112.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.18.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 14.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 90.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 91.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.12.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (46.1.3.post20200330)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 20.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 17.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 117.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.2)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 107.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.2.0)\n",
      "Building wheels for collected packages: absl-py, gast, termcolor\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=ac8693447d584d787320f8b6644c06bf876bd771ccf1795c393e92e34ba05093\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=6641eb1ec48af1033fe6fa7c54d6824bdc5526bd69ce04dab66ef03e9e083cb4\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=21f6c4c0863ead6c3a987a75286b3a9557bc07972a838eb9b3c2a2ce89e91ed7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built absl-py gast termcolor\n",
      "Installing collected packages: grpcio, keras-preprocessing, absl-py, opt-einsum, keras-applications, gast, google-pasta, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, tensorflow-estimator, termcolor, astor, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.1.0 gast-0.2.2 google-auth-1.18.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.2.1 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "2020-06-27 06:00:56.262509: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-06-27 06:00:56.262612: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-06-27 06:00:56.262625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2.1.0\n",
      "Collecting transformers==2.8.0\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 23.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.6.8-cp36-cp36m-manylinux2010_x86_64.whl (660 kB)\n",
      "\u001b[K     |████████████████████████████████| 660 kB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 39.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 57.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.18.1)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.14.8)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (3.0.12)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (4.44.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (1.17.8)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.8->boto3->transformers==2.8.0) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.8->boto3->transformers==2.8.0) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=62cf04980f41328e787bcd61b7699f5f373541803792a740c102c17aaf91c177\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, regex, sacremoses, tokenizers, dataclasses, transformers\n",
      "Successfully installed dataclasses-0.7 regex-2020.6.8 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.5.2 transformers-2.8.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 6.88MB/s]\n",
      "/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\n",
      "################ START #######################\n",
      "Loaded arguments:\n",
      "Namespace(balance_dataset=False, current_host='unknown', hosts=['unknown'], input_data='data/split', max_seq_length=128, output_data='data/output', test_split_percentage=0.05, train_split_percentage=0.9, validation_split_percentage=0.05)\n",
      "Environment variables:\n",
      "Current host: unknown\n",
      "********** input files ***************\n",
      "args.input_data:  data/split\n",
      "input_files:  ['data/split/tweet_file_02.csv.gz', 'data/split/tweet_file_01.csv.gz']\n",
      "num_cpus 16\n",
      "file data/split/tweet_file_02.csv.gz\n",
      "max_seq_length 128\n",
      "balance_dataset False\n",
      "file data/split/tweet_file_01.csv.gz\n",
      "max_seq_length 128\n",
      "balance_dataset False\n",
      "Shape of dataframe (116039, 2)\n",
      "Shape of dataframe before splitting (116039, 2)\n",
      "train split percentage 0.9\n",
      "validation split percentage 0.05\n",
      "test split percentage 0.05\n",
      "holdout percentage 0.09999999999999998\n",
      "Shape of dataframe (116038, 2)\n",
      "Shape of dataframe before splitting (116038, 2)\n",
      "train split percentage 0.9\n",
      "validation split percentage 0.05\n",
      "test split percentage 0.05\n",
      "holdout percentage 0.09999999999999998\n",
      "test holdout percentage 0.5000000000000001\n",
      "test holdout percentage 0.5000000000000001\n",
      "Shape of train dataframe (104434, 2)\n",
      "Shape of validation dataframe (5801, 2)\n",
      "Shape of test dataframe (5803, 2)\n",
      "Shape of train dataframe (104435, 2)\n",
      "Shape of validation dataframe (5801, 2)\n",
      "Shape of test dataframe (5803, 2)\n",
      "Writing example 0 of 104434\n",
      "Writing example 0 of 104435\n",
      "Writing example 10000 of 104435\n",
      "Writing example 10000 of 104434\n",
      "Writing example 20000 of 104434\n",
      "Writing example 20000 of 104435\n",
      "Writing example 30000 of 104435\n",
      "Writing example 30000 of 104434\n",
      "Writing example 40000 of 104435\n",
      "Writing example 40000 of 104434\n",
      "Writing example 50000 of 104435\n",
      "Writing example 50000 of 104434\n",
      "Writing example 60000 of 104434\n",
      "Writing example 60000 of 104435\n",
      "Writing example 70000 of 104435\n",
      "Writing example 70000 of 104434\n",
      "Writing example 80000 of 104435\n",
      "Writing example 80000 of 104434\n",
      "Writing example 90000 of 104434\n",
      "Writing example 90000 of 104435\n",
      "Writing example 100000 of 104434\n",
      "Writing example 100000 of 104435\n",
      "Writing example 0 of 5801\n",
      "Writing example 0 of 5801\n",
      "Writing example 0 of 5803\n",
      "Writing example 0 of 5803\n",
      "********** Listing tf-record files ***************\n",
      "Listing contents of data/output\n",
      "bert\n",
      "Listing contents of None\n",
      "3.5.certify_train_rnn.ipynb\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      ".ipynb_checkpoints\n",
      "train_container\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "data\n",
      "tf_script_BERT_tweet.py\n",
      "preprocess-scikit-text-to-bert.py\n",
      "temp\n",
      "3.4.Train_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "1.0.bert-input_tweet.ipynb\n",
      "3.1..Train_Notebook_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "daemon.json.1\n",
      "tf_script_bert_tweet.py\n",
      "daemon.json.2\n",
      "fine-tuned\n",
      "daemon.json\n",
      "tensorboard\n",
      "local_mode_setup.sh.1\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "TweetInput.py\n",
      "output\n",
      "__pycache__\n",
      "temp.ipynb\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "local_mode_setup.sh\n",
      "Listing contents of None\n",
      "3.5.certify_train_rnn.ipynb\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      ".ipynb_checkpoints\n",
      "train_container\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "data\n",
      "tf_script_BERT_tweet.py\n",
      "preprocess-scikit-text-to-bert.py\n",
      "temp\n",
      "3.4.Train_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "1.0.bert-input_tweet.ipynb\n",
      "3.1..Train_Notebook_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "daemon.json.1\n",
      "tf_script_bert_tweet.py\n",
      "daemon.json.2\n",
      "fine-tuned\n",
      "daemon.json\n",
      "tensorboard\n",
      "local_mode_setup.sh.1\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "TweetInput.py\n",
      "output\n",
      "__pycache__\n",
      "temp.ipynb\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "local_mode_setup.sh\n",
      "Listing contents of None\n",
      "3.5.certify_train_rnn.ipynb\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      ".ipynb_checkpoints\n",
      "train_container\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "data\n",
      "tf_script_BERT_tweet.py\n",
      "preprocess-scikit-text-to-bert.py\n",
      "temp\n",
      "3.4.Train_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "1.0.bert-input_tweet.ipynb\n",
      "3.1..Train_Notebook_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "daemon.json.1\n",
      "tf_script_bert_tweet.py\n",
      "daemon.json.2\n",
      "fine-tuned\n",
      "daemon.json\n",
      "tensorboard\n",
      "local_mode_setup.sh.1\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "TweetInput.py\n",
      "output\n",
      "__pycache__\n",
      "temp.ipynb\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "local_mode_setup.sh\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "!python preprocess-scikit-text-to-bert.py \\\n",
    "    --input-data {save_split_data_dir} \\\n",
    "    --output-data {output_data_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Processing Job using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = SKLearnProcessor(framework_version = '0.20.0',\n",
    "                             role = role,\n",
    "                             instance_type = 'ml.c5.2xlarge',\n",
    "                             instance_count = 2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_input_data = s3_destination_path_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage = 0.90\n",
    "validation_split_percentage = 0.05\n",
    "test_split_percentage = 0.05\n",
    "max_seq_length = 128\n",
    "balance_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-06-27-06-07-25-298\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processor.run(code='preprocess-scikit-text-to-bert.py',\n",
    "              inputs=[ProcessingInput(source=s3_raw_input_data,\n",
    "                                      destination='/opt/ml/processing/input/data/',\n",
    "                                      s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--max-seq-length', str(max_seq_length),\n",
    "                         '--balance-dataset', str(balance_dataset)\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-06-27-06-07-25-298\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to CloudWatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2020-06-27-06-07-25-298;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, scikit_processing_job_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/?region=us-east-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, scikit_processing_job_name, region)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at processor jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingJobSummaries': [{'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-27-06-07-25-298',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-27-06-07-25-298',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 27, 6, 7, 25, 696000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 27, 6, 7, 25, 899000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'InProgress'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-23-05-20-02-678',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-23-05-20-02-678',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 23, 5, 20, 3, 128000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 23, 5, 24, 15, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 23, 5, 24, 15, 523000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-23-04-38-09-156',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-23-04-38-09-156',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 23, 4, 38, 9, 485000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 23, 4, 42, 25, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 23, 4, 42, 25, 38000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-23-04-29-44-965',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-23-04-29-44-965',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 23, 4, 29, 45, 279000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 23, 4, 32, 53, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 23, 4, 32, 53, 423000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Failed',\n",
       "   'FailureReason': 'AlgorithmError: See job logs for more information'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-23-02-15-46-280',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-23-02-15-46-280',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 23, 2, 15, 46, 660000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 23, 2, 19, 7, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 23, 2, 19, 7, 131000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Failed',\n",
       "   'FailureReason': 'AlgorithmError: See job logs for more information'},\n",
       "  {'ProcessingJobName': '1592351727-imgsegmentation-ExplodingTensor-0a34961b',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/1592351727-imgsegmentation-explodingtensor-0a34961b',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 16, 23, 55, 32, 590000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 17, 0, 5, 19, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 17, 0, 5, 19, 636000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Stopped'},\n",
       "  {'ProcessingJobName': '1592351727-imgsegmentation-LossNotDecreasing-0a669722',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/1592351727-imgsegmentation-lossnotdecreasing-0a669722',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 16, 23, 55, 31, 482000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 17, 0, 3, 44, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 17, 0, 3, 44, 334000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Stopped'},\n",
       "  {'ProcessingJobName': '1592351727-imgsegmentation-VanishingGradient-16e7a96b',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/1592351727-imgsegmentation-vanishinggradient-16e7a96b',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 16, 23, 55, 30, 172000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 17, 0, 1, 31, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 17, 0, 1, 31, 555000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Stopped'},\n",
       "  {'ProcessingJobName': 'pr-1-e83359821f9c4192a0947de05f65c9f5b6d5c5f8f8654b45afffc01b08',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/pr-1-e83359821f9c4192a0947de05f65c9f5b6d5c5f8f8654b45afffc01b08',\n",
       "   'CreationTime': datetime.datetime(2020, 5, 27, 6, 52, 24, 268000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 5, 27, 6, 55, 39, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 5, 27, 6, 55, 39, 524000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'db-1-eed9c5e539fe40c69f0c199495de6443850c7443593a4d92bf87fadc51',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/db-1-eed9c5e539fe40c69f0c199495de6443850c7443593a4d92bf87fadc51',\n",
       "   'CreationTime': datetime.datetime(2020, 5, 27, 6, 49, 45, 668000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 5, 27, 6, 52, 20, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 5, 27, 6, 52, 20, 730000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'}],\n",
       " 'NextToken': 'cIws2QhTXUIa8bi8XY6Q7JUM+leXzY+WUYv213SjXJg6fOXnrUzo74zCATuP2us2wD77oSXlMKIxxUofq8kRnlj6TIl5xo2TtSaOEbGEAuPIcFJc3xyaCVbMB3JlWpTWgNoH2oE3rQ/1sRo4TpG6aKy3L2xsc3yS0/65ZikpMsM/KQX0QPHUPp/+WuaMJs7XMtTj5/OGXNRjh0LQ1xUSVb1n7/kKlRQyUtPQtPr5RUbheOz4X51s8j2aQzdzJLvEsZCHIlWuMFXaIiX+nPIIOi7q14UizUDoc+K0rAOUUGLIHt4asr6sB5J2mW4sGIITZHm/CTYdDCX3Z9vJXBU0aUtKOOKccrs7TfrmwCimMyl/t9CPb5kqSlgb4Y/fUFJuLM0iChrpRqKCoWMQlPVYTU0njEuRsUitxjJYlOKLIRPGwvjb0suwLWuJhY5lBC8YQPommWPMbFqZuljs+AGu8hZ/demQgcYxSOOmUFMgcznpF3AbcNAPdlawQOd3qqYhRYk1e37XGSpKzrs4cvOKQbFECZc20FnPNR9yL9BC+y+QSpalj6H+IHRvnaSv9gJzbKDS',\n",
       " 'ResponseMetadata': {'RequestId': 'c724959a-e964-475c-b64d-ae513b66adee',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c724959a-e964-475c-b64d-ae513b66adee',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '4126',\n",
       "   'date': 'Sat, 27 Jun 2020 06:07:38 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.list_processing_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status of the Processor Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-27-06-07-25-298', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '128', '--balance-dataset', 'False']}, 'RoleArn': 'arn:aws:iam::057716757052:role/service-role/AmazonSageMaker-ExecutionRole-20191128T110038', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-27-06-07-25-298', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 6, 27, 6, 7, 25, 899000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 27, 6, 7, 25, 696000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '06010559-9d56-436d-8ca9-5d097a27f680', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '06010559-9d56-436d-8ca9-5d097a27f680', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2317', 'date': 'Sat, 27 Jun 2020 06:07:39 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Processed Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-train\n",
      "s3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-validation\n",
      "s3://sagemaker-us-east-2-057716757052/sagemaker-scikit-learn-2020-06-27-06-07-25-298/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-27 06:11:16       5584 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-06-27 06:11:23       5658 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-27 06:11:16        510 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-06-27 06:11:24        523 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-27 06:11:16        505 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-06-27 06:11:24        509 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_raw_input_data' (str)\n",
      "Stored 'max_seq_length' (int)\n",
      "Stored 'train_split_percentage' (float)\n",
      "Stored 'validation_split_percentage' (float)\n",
      "Stored 'test_split_percentage' (float)\n",
      "Stored 'processed_train_data_s3_uri' (str)\n",
      "Stored 'processed_validation_data_s3_uri' (str)\n",
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_raw_input_data\n",
    "%store max_seq_length\n",
    "%store train_split_percentage\n",
    "%store validation_split_percentage\n",
    "%store test_split_percentage\n",
    "%store processed_train_data_s3_uri\n",
    "%store processed_validation_data_s3_uri\n",
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "max_seq_length                               -> 128\n",
      "processed_test_data_s3_uri                   -> 's3://sagemaker-us-east-2-057716757052/sagemaker-s\n",
      "processed_train_data_s3_uri                  -> 's3://sagemaker-us-east-2-057716757052/sagemaker-s\n",
      "processed_validation_data_s3_uri             -> 's3://sagemaker-us-east-2-057716757052/sagemaker-s\n",
      "s3_destination_path_csv                      -> 's3://sagemaker-us-east-2-057716757052/tweet_emoti\n",
      "s3_raw_input_data                            -> 's3://sagemaker-us-east-2-057716757052/tweet_emoti\n",
      "save_split_data_dir                          -> 'data/split'\n",
      "test_split_percentage                        -> 0.05\n",
      "train_split_percentage                       -> 0.9\n",
      "validation_split_percentage                  -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
