{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Raw Text into BERT Embeddings with Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Preprocess Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess-scikit-text-to-bert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess-scikit-text-to-bert.py\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import functools\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "DATA_COLUMN = 'TWEET'\n",
    "LABEL_COLUMN = 'LABEL'\n",
    "LABEL_VALUES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"BERT feature vectors.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    \n",
    "    \n",
    "class Input(object):\n",
    "  \"\"\"A single training/test input for sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, text, label=None):\n",
    "    \"\"\"Constructs an Input.\n",
    "    Args:\n",
    "      text: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.label = label\n",
    "\n",
    "def convert_input(text_input, max_seq_length):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    #\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # \n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "    #\n",
    "    tokens = tokenizer.tokenize(text_input.text)    \n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=max_seq_length)\n",
    "\n",
    "    # Convert the text-based tokens to ids from the pre-trained BERT vocabulary\n",
    "    input_ids = encode_plus_tokens['input_ids']\n",
    "    # Specifies which tokens BERT should pay attention to (0 or 1)\n",
    "    input_mask = encode_plus_tokens['attention_mask']\n",
    "    # Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # Label for our training data (star_rating 1 through 5)\n",
    "    label_id = label_map[text_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id)\n",
    "\n",
    "    return features\n",
    "\n",
    "def convert_features_to_tfrecord(inputs,\n",
    "                                 output_file,\n",
    "                                 max_seq_length):\n",
    "    \"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, text_input) in enumerate(inputs):\n",
    "        if input_idx % 10000 == 0:\n",
    "            print(\"Writing example %d of %d\" % (input_idx, len(inputs)))\n",
    "\n",
    "            bert_features = convert_input(text_input, max_seq_length)\n",
    "        \n",
    "            tfrecord_features = collections.OrderedDict()\n",
    "            \n",
    "            tfrecord_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_ids))\n",
    "            tfrecord_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_mask))\n",
    "            tfrecord_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.segment_ids))\n",
    "            tfrecord_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[bert_features.label_id]))\n",
    "\n",
    "            tfrecord = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\n",
    "            \n",
    "            tfrecord_writer.write(tfrecord.SerializeToString())\n",
    "\n",
    "    tfrecord_writer.close()\n",
    "\n",
    "    \n",
    "    \n",
    "def list_arg(raw_value):\n",
    "    \"\"\"argparse type for a list of strings\"\"\"\n",
    "    return str(raw_value).split(',')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def _transform_tsv_to_tfrecord(file, \n",
    "                               max_seq_length, \n",
    "                               balance_dataset):\n",
    "    print('file {}'.format(file))\n",
    "    print('max_seq_length {}'.format(max_seq_length))\n",
    "    print('balance_dataset {}'.format(balance_dataset))\n",
    "\n",
    "    filename_without_extension = Path(Path(file).stem).stem\n",
    "\n",
    "    df = pd.read_csv(file, \n",
    "                     compression='gzip')\n",
    "\n",
    "    df.isna().values.any()\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of dataframe {}'.format(df.shape))\n",
    "\n",
    "        \n",
    "    print('Shape of dataframe before splitting {}'.format(df.shape))\n",
    "    \n",
    "    print('train split percentage {}'.format(args.train_split_percentage))\n",
    "    print('validation split percentage {}'.format(args.validation_split_percentage))\n",
    "    print('test split percentage {}'.format(args.test_split_percentage))    \n",
    "    \n",
    "    holdout_percentage = 1.00 - args.train_split_percentage\n",
    "    print('holdout percentage {}'.format(holdout_percentage))\n",
    "    df_train, df_holdout = train_test_split(df, \n",
    "                                            test_size=holdout_percentage, \n",
    "                                            stratify=df[LABEL_COLUMN])\n",
    "\n",
    "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
    "    print('test holdout percentage {}'.format(test_holdout_percentage))\n",
    "    df_validation, df_test = train_test_split(df_holdout, \n",
    "                                              test_size=test_holdout_percentage,\n",
    "                                              stratify=df_holdout[LABEL_COLUMN])\n",
    "    \n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_validation = df_validation.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of train dataframe {}'.format(df_train.shape))\n",
    "    print('Shape of validation dataframe {}'.format(df_validation.shape))\n",
    "    print('Shape of test dataframe {}'.format(df_test.shape))\n",
    "\n",
    "    train_inputs = df_train.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                         label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_inputs = df_validation.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                            label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    test_inputs = df_test.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    # Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "    # \n",
    "    # \n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # 4. Map our words to indexes using a vocab file that BERT provides\n",
    "    # 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "    # 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "    # \n",
    "    # We don't have to worry about these details.  The Transformers tokenizer does this for us.\n",
    "    # \n",
    "    train_data = '{}/bert/train'.format(args.output_data)\n",
    "    validation_data = '{}/bert/validation'.format(args.output_data)\n",
    "    test_data = '{}/bert/test'.format(args.output_data)\n",
    "\n",
    "    # Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\n",
    "    df_train_embeddings = convert_features_to_tfrecord(train_inputs, \n",
    "                                                       '{}/part-{}-{}.tfrecord'.format(train_data, args.current_host, filename_without_extension), \n",
    "                                                       max_seq_length)\n",
    "\n",
    "    df_validation_embeddings = convert_features_to_tfrecord(validation_inputs, '{}/part-{}-{}.tfrecord'.format(validation_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "\n",
    "    df_test_embeddings = convert_features_to_tfrecord(test_inputs, '{}/part-{}-{}.tfrecord'.format(test_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "        \n",
    "def parse_args():\n",
    "    # Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\n",
    "    resconfig = {}\n",
    "    try:\n",
    "        with open('/opt/ml/config/resourceconfig.json', 'r') as cfgfile:\n",
    "            resconfig = json.load(cfgfile)\n",
    "    except FileNotFoundError:\n",
    "        print('/opt/ml/config/resourceconfig.json not found.  current_host is unknown.')\n",
    "        pass # Ignore\n",
    "\n",
    "    # Local testing with CLI args\n",
    "    parser = argparse.ArgumentParser(description='Process')\n",
    "\n",
    "    parser.add_argument('--hosts', type=list_arg,\n",
    "        default=resconfig.get('hosts', ['unknown']),\n",
    "        help='Comma-separated list of host names running the job'\n",
    "    )\n",
    "    parser.add_argument('--current-host', type=str,\n",
    "        default=resconfig.get('current_host', 'unknown'),\n",
    "        help='Name of this host running the job'\n",
    "    )\n",
    "    parser.add_argument('--input-data', type=str,\n",
    "        default='/opt/ml/processing/input/data',\n",
    "    )\n",
    "    parser.add_argument('--output-data', type=str,\n",
    "        default='/opt/ml/processing/output',\n",
    "    )\n",
    "    parser.add_argument('--train-split-percentage', type=float,\n",
    "        default=0.90,\n",
    "    )\n",
    "    parser.add_argument('--validation-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )    \n",
    "    parser.add_argument('--test-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )\n",
    "    parser.add_argument('--balance-dataset', type=eval,\n",
    "        default=False\n",
    "    )\n",
    "    parser.add_argument('--max-seq-length', type=int,\n",
    "        default=128,\n",
    "    )  \n",
    "    \n",
    "    return parser.parse_args()\n",
    "        \n",
    "    \n",
    "def process(args):\n",
    "    print('Current host: {}'.format(args.current_host))\n",
    "    \n",
    "    train_data = None\n",
    "    validation_data = None\n",
    "    test_data = None\n",
    "\n",
    "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \n",
    "                                                 max_seq_length=args.max_seq_length,\n",
    "                                                 balance_dataset=args.balance_dataset\n",
    "\n",
    "    )\n",
    "    input_files = glob.glob('{}/*'.format(args.input_data))\n",
    "    print(\"********** input files ***************\")    \n",
    "    print(\"args.input_data: \", args.input_data)\n",
    "    print(\"input_files: \", input_files)\n",
    "\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print('num_cpus {}'.format(num_cpus))\n",
    "\n",
    "    p = multiprocessing.Pool(num_cpus)\n",
    "    p.map(transform_tsv_to_tfrecord, input_files)\n",
    "\n",
    "    print(\"********** Listing tf-record files ***************\")        \n",
    "    print('Listing contents of {}'.format(args.output_data))\n",
    "    dirs_output = os.listdir(args.output_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(train_data))\n",
    "    dirs_output = os.listdir(train_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(validation_data))\n",
    "    dirs_output = os.listdir(validation_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(test_data))\n",
    "    dirs_output = os.listdir(test_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    print('################ START #######################')    \n",
    "    print('Loaded arguments:')\n",
    "    print(args)\n",
    "    \n",
    "    print('Environment variables:')\n",
    "#     print(os.environ)\n",
    "\n",
    "    process(args)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Local Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_split_data_dir = 'data/split'\n",
    "output_data_dir = 'data/output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/output/bert/train\n",
    "! mkdir -p data/output/bert/validation\n",
    "! mkdir -p data/output/bert/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.30.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.31.1)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.1.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.12.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.19.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.24.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.18.0)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-47.3.1-py3-none-any.whl (582 kB)\n",
      "\u001b[K     |████████████████████████████████| 582 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.14.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.23)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.0)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "Successfully installed setuptools-47.3.1\n",
      "2020-06-28 05:08:30.840126: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-06-28 05:08:30.840273: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-06-28 05:08:30.840295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2.1.0\n",
      "Requirement already satisfied: transformers==2.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.1.91)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.12.39)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.0.43)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.19.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2020.6.8)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.5.2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.7)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (4.46.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (1.15.39)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (6.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (0.15.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (1.23)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2.6)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers==2.8.0) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers==2.8.0) (2.7.3)\n",
      "/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\n",
      "################ START #######################\n",
      "Loaded arguments:\n",
      "Namespace(balance_dataset=False, current_host='unknown', hosts=['unknown'], input_data='data/split', max_seq_length=128, output_data='data/output', test_split_percentage=0.05, train_split_percentage=0.9, validation_split_percentage=0.05)\n",
      "Environment variables:\n",
      "Current host: unknown\n",
      "********** input files ***************\n",
      "args.input_data:  data/split\n",
      "input_files:  ['data/split/tweet_file_01.csv.gz', 'data/split/tweet_file_02.csv.gz']\n",
      "num_cpus 4\n",
      "file data/split/tweet_file_01.csv.gz\n",
      "max_seq_length 128\n",
      "balance_dataset False\n",
      "file data/split/tweet_file_02.csv.gz\n",
      "max_seq_length 128\n",
      "balance_dataset False\n",
      "Shape of dataframe (116039, 2)\n",
      "Shape of dataframe (116038, 2)\n",
      "Shape of dataframe before splitting (116039, 2)\n",
      "Shape of dataframe before splitting (116038, 2)\n",
      "train split percentage 0.9\n",
      "train split percentage 0.9\n",
      "validation split percentage 0.05\n",
      "test split percentage 0.05\n",
      "validation split percentage 0.05\n",
      "test split percentage 0.05\n",
      "holdout percentage 0.09999999999999998\n",
      "holdout percentage 0.09999999999999998\n",
      "test holdout percentage 0.5000000000000001\n",
      "test holdout percentage 0.5000000000000001\n",
      "Shape of train dataframe (104434, 2)\n",
      "Shape of validation dataframe (5801, 2)\n",
      "Shape of test dataframe (5803, 2)\n",
      "Shape of train dataframe (104435, 2)\n",
      "Shape of validation dataframe (5801, 2)\n",
      "Shape of test dataframe (5803, 2)\n",
      "Writing example 0 of 104434\n",
      "Writing example 10000 of 104434\n",
      "Writing example 20000 of 104434\n",
      "Writing example 30000 of 104434\n",
      "Writing example 40000 of 104434\n",
      "Writing example 50000 of 104434\n",
      "Writing example 60000 of 104434\n",
      "Writing example 70000 of 104434\n",
      "Writing example 80000 of 104434\n",
      "Writing example 90000 of 104434\n",
      "Writing example 100000 of 104434\n",
      "Writing example 0 of 5801\n",
      "Writing example 0 of 5803\n",
      "Writing example 0 of 104435\n",
      "Writing example 10000 of 104435\n",
      "Writing example 20000 of 104435\n",
      "Writing example 30000 of 104435\n",
      "Writing example 40000 of 104435\n",
      "Writing example 50000 of 104435\n",
      "Writing example 60000 of 104435\n",
      "Writing example 70000 of 104435\n",
      "Writing example 80000 of 104435\n",
      "Writing example 90000 of 104435\n",
      "Writing example 100000 of 104435\n",
      "Writing example 0 of 5801\n",
      "Writing example 0 of 5803\n",
      "********** Listing tf-record files ***************\n",
      "Listing contents of data/output\n",
      "bert\n",
      "Listing contents of None\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      "__pycache__\n",
      "3.4.2Train_Local_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.1..Train_Notebook_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "tf_script_bert_tweet.py\n",
      "3.4.3Train_SM_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "data\n",
      "3.4.1.certify_train_rnn.ipynb\n",
      ".ipynb_checkpoints\n",
      "train_container\n",
      "4.1.Deploy_Local_weet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "TweetInput.py\n",
      "1.0.bert-input_tweet.ipynb\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "preprocess-scikit-text-to-bert.py\n",
      "Listing contents of None\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      "__pycache__\n",
      "3.4.2Train_Local_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.1..Train_Notebook_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "tf_script_bert_tweet.py\n",
      "3.4.3Train_SM_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "data\n",
      "3.4.1.certify_train_rnn.ipynb\n",
      ".ipynb_checkpoints\n",
      "train_container\n",
      "4.1.Deploy_Local_weet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "TweetInput.py\n",
      "1.0.bert-input_tweet.ipynb\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "preprocess-scikit-text-to-bert.py\n",
      "Listing contents of None\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      "__pycache__\n",
      "3.4.2Train_Local_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.1..Train_Notebook_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "tf_script_bert_tweet.py\n",
      "3.4.3Train_SM_Docker_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "data\n",
      "3.4.1.certify_train_rnn.ipynb\n",
      ".ipynb_checkpoints\n",
      "train_container\n",
      "4.1.Deploy_Local_weet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "TweetInput.py\n",
      "1.0.bert-input_tweet.ipynb\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "preprocess-scikit-text-to-bert.py\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "!python preprocess-scikit-text-to-bert.py \\\n",
    "    --input-data {save_split_data_dir} \\\n",
    "    --output-data {output_data_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Processing Job using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = SKLearnProcessor(framework_version = '0.20.0',\n",
    "                             role = role,\n",
    "                             instance_type = 'ml.c5.2xlarge',\n",
    "                             instance_count = 2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_input_data = s3_destination_path_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage = 0.90\n",
    "validation_split_percentage = 0.05\n",
    "test_split_percentage = 0.05\n",
    "max_seq_length = 128\n",
    "balance_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-06-28-05-08-39-660\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processor.run(code='preprocess-scikit-text-to-bert.py',\n",
    "              inputs=[ProcessingInput(source=s3_raw_input_data,\n",
    "                                      destination='/opt/ml/processing/input/data/',\n",
    "                                      s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--max-seq-length', str(max_seq_length),\n",
    "                         '--balance-dataset', str(balance_dataset)\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-06-28-05-08-39-660\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to CloudWatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2020-06-28-05-08-39-660;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, scikit_processing_job_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, scikit_processing_job_name, region)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at processor jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingJobSummaries': [{'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-28-05-08-39-660',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-28-05-08-39-660',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 28, 5, 8, 40, 83000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 28, 5, 8, 40, 83000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'InProgress'},\n",
       "  {'ProcessingJobName': 'tf-2-workflow-24-13-52-22',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/tf-2-workflow-24-13-52-22',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 24, 13, 52, 23, 362000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 24, 13, 56, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 24, 13, 56, 0, 345000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'spark-amazon-reviews-processor-2020-06-13-03-25-06-577',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/spark-amazon-reviews-processor-2020-06-13-03-25-06-577',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 13, 3, 25, 7, 60000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 13, 4, 8, 15, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 13, 4, 8, 15, 291000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Failed',\n",
       "   'FailureReason': 'AlgorithmError: See job logs for more information'},\n",
       "  {'ProcessingJobName': 'spark-amazon-reviews-processor-2020-06-13-02-39-01-813',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/spark-amazon-reviews-processor-2020-06-13-02-39-01-813',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 13, 2, 39, 2, 279000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 13, 3, 21, 32, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 13, 3, 21, 32, 254000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Failed',\n",
       "   'FailureReason': 'AlgorithmError: See job logs for more information'},\n",
       "  {'ProcessingJobName': 'spark-amazon-reviews-processor-2020-06-13-01-47-41-669',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/spark-amazon-reviews-processor-2020-06-13-01-47-41-669',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 13, 1, 47, 42, 92000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 13, 2, 30, 19, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 13, 2, 30, 19, 668000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Failed',\n",
       "   'FailureReason': 'AlgorithmError: See job logs for more information'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-13-01-36-18-209',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-13-01-36-18-209',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 13, 1, 36, 18, 698000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 13, 1, 40, 49, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 13, 1, 40, 49, 326000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-06-12-15-05-25-132',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/spark-amazon-reviews-analyzer-2020-06-12-15-05-25-132',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 12, 15, 5, 25, 552000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 12, 15, 11, 36, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 12, 15, 11, 36, 389000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pr-1-4951b9b03b2341c99030e16114264dca677785d8c3ee404981131ab51d',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/pr-1-4951b9b03b2341c99030e16114264dca677785d8c3ee404981131ab51d',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 12, 3, 13, 37, 529000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 12, 3, 17, 46, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 12, 3, 17, 46, 873000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'db-1-deecff59c2e647189ed359797bbb8e8ded2ad9fc042e4441a9f7feb2d7',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/db-1-deecff59c2e647189ed359797bbb8e8ded2ad9fc042e4441a9f7feb2d7',\n",
       "   'CreationTime': datetime.datetime(2020, 6, 12, 3, 10, 22, 805000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 6, 12, 3, 13, 30, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 6, 12, 3, 13, 30, 849000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'}],\n",
       " 'ResponseMetadata': {'RequestId': '89c7db44-f5d6-4ba7-9e16-19a647b183cb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '89c7db44-f5d6-4ba7-9e16-19a647b183cb',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3254',\n",
       "   'date': 'Sun, 28 Jun 2020 05:08:39 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.list_processing_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status of the Processor Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2020-06-28-05-08-39-660', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '128', '--balance-dataset', 'False']}, 'RoleArn': 'arn:aws:iam::057716757052:role/service-role/AmazonSageMaker-ExecutionRole-20200501T215180', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/sagemaker-scikit-learn-2020-06-28-05-08-39-660', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 6, 28, 5, 8, 40, 83000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 28, 5, 8, 40, 83000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '0460db1e-9a06-4a90-a9aa-4f5a30749dbd', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0460db1e-9a06-4a90-a9aa-4f5a30749dbd', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2317', 'date': 'Sun, 28 Jun 2020 05:08:39 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Processed Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-train\n",
      "s3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-validation\n",
      "s3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-06-28-05-08-39-660/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:12:47       5634 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-06-28 05:12:47       5603 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:12:47        507 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-06-28 05:12:47        501 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-28 05:12:47        524 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-06-28 05:12:47        517 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_raw_input_data' (str)\n",
      "Stored 'max_seq_length' (int)\n",
      "Stored 'train_split_percentage' (float)\n",
      "Stored 'validation_split_percentage' (float)\n",
      "Stored 'test_split_percentage' (float)\n",
      "Stored 'processed_train_data_s3_uri' (str)\n",
      "Stored 'processed_validation_data_s3_uri' (str)\n",
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_raw_input_data\n",
    "%store max_seq_length\n",
    "%store train_split_percentage\n",
    "%store validation_split_percentage\n",
    "%store test_split_percentage\n",
    "%store processed_train_data_s3_uri\n",
    "%store processed_validation_data_s3_uri\n",
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "max_seq_length                               -> 128\n",
      "processed_test_data_s3_uri                   -> 's3://sagemaker-us-west-2-057716757052/sagemaker-s\n",
      "processed_train_data_s3_uri                  -> 's3://sagemaker-us-west-2-057716757052/sagemaker-s\n",
      "processed_validation_data_s3_uri             -> 's3://sagemaker-us-west-2-057716757052/sagemaker-s\n",
      "s3_destination_path_csv                      -> 's3://sagemaker-us-west-2-057716757052/tweet_emoti\n",
      "s3_raw_input_data                            -> 's3://sagemaker-us-west-2-057716757052/tweet_emoti\n",
      "save_split_data_dir                          -> 'data/split'\n",
      "test_split_percentage                        -> 0.05\n",
      "train_split_percentage                       -> 0.9\n",
      "validation_split_percentage                  -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
