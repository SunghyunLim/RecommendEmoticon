{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.0] Tweet Text를 BERT Embedding Vector로 변환\n",
    "이 노트북에서는 아래와 같은 작업이 진행 됩니다. 참고로 노트북의 코드는 아래 Reference 코드를 거의 사용 함.\n",
    "- Preprocess Script 생성\n",
    "    - Input Text --> BERT Feature Vector 로 변환 --> TF Record로 변환\n",
    "- 위 스크립트를 테스트 목적으로 Local Notebook에서 실행\n",
    "- SageMaker Processor Job으로 2개의 인스턴스를 사용하여 위 스크립트를 실행\n",
    "    - Train, Validation, Test의 각각 2개의 TF Records 파일이 S3에 저장됨.\n",
    "    \n",
    "---\n",
    "이 노트북은 약 6분 정도 소요 됩니다.    \n",
    "\n",
    "---\n",
    "Reference:\n",
    "    - https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/02_Prepare_Dataset_ProcessingJob_BERT_Scikit.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Preprocess Script\n",
    "이 파일은 아래 소스에서 Label의 5개 --> 10개 변경한 버전 임.\n",
    "- 소스: \n",
    "    - https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/preprocess-scikit-text-to-bert.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess-scikit-text-to-bert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess-scikit-text-to-bert.py\n",
    "\n",
    "# 이 파일은 아래 소스에서 Label의 5개 --> 10개 변경한 버전 임.\n",
    "# https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/preprocess-scikit-text-to-bert.py\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import functools\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "DATA_COLUMN = 'TWEET'\n",
    "LABEL_COLUMN = 'LABEL'\n",
    "LABEL_VALUES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"BERT feature vectors.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    \n",
    "    \n",
    "class Input(object):\n",
    "  \"\"\"A single training/test input for sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, text, label=None):\n",
    "    \"\"\"Constructs an Input.\n",
    "    Args:\n",
    "      text: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.label = label\n",
    "\n",
    "def convert_input(text_input, max_seq_length):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    #\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # \n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "    #\n",
    "    tokens = tokenizer.tokenize(text_input.text)    \n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=max_seq_length)\n",
    "\n",
    "    # Convert the text-based tokens to ids from the pre-trained BERT vocabulary\n",
    "    input_ids = encode_plus_tokens['input_ids']\n",
    "    # Specifies which tokens BERT should pay attention to (0 or 1)\n",
    "    input_mask = encode_plus_tokens['attention_mask']\n",
    "    # Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # Label for our training data (star_rating 1 through 5)\n",
    "    label_id = label_map[text_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id)\n",
    "\n",
    "    return features\n",
    "\n",
    "def convert_features_to_tfrecord(inputs,\n",
    "                                 output_file,\n",
    "                                 max_seq_length):\n",
    "    \"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, text_input) in enumerate(inputs):\n",
    "        if input_idx % 10000 == 0:\n",
    "            print(\"Writing example %d of %d\" % (input_idx, len(inputs)))\n",
    "\n",
    "            bert_features = convert_input(text_input, max_seq_length)\n",
    "        \n",
    "            tfrecord_features = collections.OrderedDict()\n",
    "            \n",
    "            tfrecord_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_ids))\n",
    "            tfrecord_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_mask))\n",
    "            tfrecord_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.segment_ids))\n",
    "            tfrecord_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[bert_features.label_id]))\n",
    "\n",
    "            tfrecord = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\n",
    "            \n",
    "            tfrecord_writer.write(tfrecord.SerializeToString())\n",
    "\n",
    "    tfrecord_writer.close()\n",
    "\n",
    "    \n",
    "    \n",
    "def list_arg(raw_value):\n",
    "    \"\"\"argparse type for a list of strings\"\"\"\n",
    "    return str(raw_value).split(',')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def _transform_tsv_to_tfrecord(file, \n",
    "                               max_seq_length, \n",
    "                               balance_dataset):\n",
    "    print('file {}'.format(file))\n",
    "    print('max_seq_length {}'.format(max_seq_length))\n",
    "    print('balance_dataset {}'.format(balance_dataset))\n",
    "\n",
    "    filename_without_extension = Path(Path(file).stem).stem\n",
    "\n",
    "    df = pd.read_csv(file, \n",
    "                     compression='gzip')\n",
    "\n",
    "    df.isna().values.any()\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of dataframe {}'.format(df.shape))\n",
    "\n",
    "        \n",
    "    print('Shape of dataframe before splitting {}'.format(df.shape))\n",
    "    \n",
    "    print('train split percentage {}'.format(args.train_split_percentage))\n",
    "    print('validation split percentage {}'.format(args.validation_split_percentage))\n",
    "    print('test split percentage {}'.format(args.test_split_percentage))    \n",
    "    \n",
    "    holdout_percentage = 1.00 - args.train_split_percentage\n",
    "    print('holdout percentage {}'.format(holdout_percentage))\n",
    "    df_train, df_holdout = train_test_split(df, \n",
    "                                            test_size=holdout_percentage, \n",
    "                                            stratify=df[LABEL_COLUMN])\n",
    "\n",
    "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
    "    print('test holdout percentage {}'.format(test_holdout_percentage))\n",
    "    df_validation, df_test = train_test_split(df_holdout, \n",
    "                                              test_size=test_holdout_percentage,\n",
    "                                              stratify=df_holdout[LABEL_COLUMN])\n",
    "    \n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_validation = df_validation.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of train dataframe {}'.format(df_train.shape))\n",
    "    print('Shape of validation dataframe {}'.format(df_validation.shape))\n",
    "    print('Shape of test dataframe {}'.format(df_test.shape))\n",
    "\n",
    "    train_inputs = df_train.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                         label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_inputs = df_validation.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                            label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    test_inputs = df_test.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    # Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "    # \n",
    "    # \n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # 4. Map our words to indexes using a vocab file that BERT provides\n",
    "    # 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "    # 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "    # \n",
    "    # We don't have to worry about these details.  The Transformers tokenizer does this for us.\n",
    "    # \n",
    "    train_data = '{}/bert/train'.format(args.output_data)\n",
    "    validation_data = '{}/bert/validation'.format(args.output_data)\n",
    "    test_data = '{}/bert/test'.format(args.output_data)\n",
    "\n",
    "    # Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\n",
    "    df_train_embeddings = convert_features_to_tfrecord(train_inputs, \n",
    "                                                       '{}/part-{}-{}.tfrecord'.format(train_data, args.current_host, filename_without_extension), \n",
    "                                                       max_seq_length)\n",
    "\n",
    "    df_validation_embeddings = convert_features_to_tfrecord(validation_inputs, '{}/part-{}-{}.tfrecord'.format(validation_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "\n",
    "    df_test_embeddings = convert_features_to_tfrecord(test_inputs, '{}/part-{}-{}.tfrecord'.format(test_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "        \n",
    "def parse_args():\n",
    "    # Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\n",
    "    resconfig = {}\n",
    "    try:\n",
    "        with open('/opt/ml/config/resourceconfig.json', 'r') as cfgfile:\n",
    "            resconfig = json.load(cfgfile)\n",
    "    except FileNotFoundError:\n",
    "        print('/opt/ml/config/resourceconfig.json not found.  current_host is unknown.')\n",
    "        pass # Ignore\n",
    "\n",
    "    # Local testing with CLI args\n",
    "    parser = argparse.ArgumentParser(description='Process')\n",
    "\n",
    "    parser.add_argument('--hosts', type=list_arg,\n",
    "        default=resconfig.get('hosts', ['unknown']),\n",
    "        help='Comma-separated list of host names running the job'\n",
    "    )\n",
    "    parser.add_argument('--current-host', type=str,\n",
    "        default=resconfig.get('current_host', 'unknown'),\n",
    "        help='Name of this host running the job'\n",
    "    )\n",
    "    parser.add_argument('--input-data', type=str,\n",
    "        default='/opt/ml/processing/input/data',\n",
    "    )\n",
    "    parser.add_argument('--output-data', type=str,\n",
    "        default='/opt/ml/processing/output',\n",
    "    )\n",
    "    parser.add_argument('--train-split-percentage', type=float,\n",
    "        default=0.90,\n",
    "    )\n",
    "    parser.add_argument('--validation-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )    \n",
    "    parser.add_argument('--test-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )\n",
    "    parser.add_argument('--balance-dataset', type=eval,\n",
    "        default=False\n",
    "    )\n",
    "    parser.add_argument('--max-seq-length', type=int,\n",
    "        default=128,\n",
    "    )  \n",
    "    \n",
    "    return parser.parse_args()\n",
    "        \n",
    "    \n",
    "def process(args):\n",
    "    print('Current host: {}'.format(args.current_host))\n",
    "    \n",
    "    train_data = None\n",
    "    validation_data = None\n",
    "    test_data = None\n",
    "\n",
    "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \n",
    "                                                 max_seq_length=args.max_seq_length,\n",
    "                                                 balance_dataset=args.balance_dataset\n",
    "\n",
    "    )\n",
    "    input_files = glob.glob('{}/*'.format(args.input_data))\n",
    "    print(\"********** input files ***************\")    \n",
    "    print(\"args.input_data: \", args.input_data)\n",
    "    print(\"input_files: \", input_files)\n",
    "\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print('num_cpus {}'.format(num_cpus))\n",
    "\n",
    "    p = multiprocessing.Pool(num_cpus)\n",
    "    p.map(transform_tsv_to_tfrecord, input_files)\n",
    "\n",
    "    print(\"********** Listing tf-record files ***************\")        \n",
    "    print('Listing contents of {}'.format(args.output_data))\n",
    "    dirs_output = os.listdir(args.output_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(train_data))\n",
    "    dirs_output = os.listdir(train_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(validation_data))\n",
    "    dirs_output = os.listdir(validation_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(test_data))\n",
    "    dirs_output = os.listdir(test_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    print('################ START #######################')    \n",
    "    print('Loaded arguments:')\n",
    "    print(args)\n",
    "    \n",
    "    print('Environment variables:')\n",
    "#     print(os.environ)\n",
    "\n",
    "    process(args)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Local Notebook\n",
    "아래는 로컬 노트북에서 ! python 으로 테스트를 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 폴더 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_split_data_dir = 'data/split'\n",
    "output_data_dir = 'data/output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/output/bert/train\n",
    "! mkdir -p data/output/bert/validation\n",
    "! mkdir -p data/output/bert/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.18.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.1.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.12.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.30.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.19.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (46.1.3.post20200330)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.5.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Traceback (most recent call last):\n",
      "  File \"preprocess-scikit-text-to-bert.py\", line 17, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 101, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "ModuleNotFoundError: No module named 'tensorflow.python.tools'\n"
     ]
    }
   ],
   "source": [
    "!python preprocess-scikit-text-to-bert.py \\\n",
    "    --input-data {save_split_data_dir} \\\n",
    "    --output-data {output_data_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Processing Job using Amazon SageMaker\n",
    "멀티 인스턴스(여기서는 2개)로 Preprocessing Job을 실행 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = SKLearnProcessor(framework_version = '0.20.0',\n",
    "                             role = role,\n",
    "                             instance_type = 'ml.c5.2xlarge',\n",
    "                             instance_count = 2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_input_data = s3_destination_path_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage = 0.90\n",
    "validation_split_percentage = 0.05\n",
    "test_split_percentage = 0.05\n",
    "max_seq_length = 128\n",
    "balance_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-07-22-01-40-29-885\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processor.run(code='preprocess-scikit-text-to-bert.py',\n",
    "              inputs=[ProcessingInput(source=s3_raw_input_data,\n",
    "                                      destination='/opt/ml/processing/input/data/',\n",
    "                                      s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--max-seq-length', str(max_seq_length),\n",
    "                         '--balance-dataset', str(balance_dataset)\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-07-22-01-40-29-885\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to CloudWatch \n",
    "CloudWatch에서 생성된 로그를 확인 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2020-07-22-01-40-29-885;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, scikit_processing_job_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, scikit_processing_job_name, region)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status of the Processor Job\n",
    "Processor Job을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2020-07-22-01-40-29-885', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '128', '--balance-dataset', 'False']}, 'RoleArn': 'arn:aws:iam::057716757052:role/service-role/AmazonSageMaker-ExecutionRole-20191128T110038', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:057716757052:processing-job/sagemaker-scikit-learn-2020-07-22-01-40-29-885', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 7, 22, 1, 40, 30, 335000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 7, 22, 1, 40, 30, 335000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '5235d18d-eacb-4826-800c-f2c758ca08cd', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '5235d18d-eacb-4826-800c-f2c758ca08cd', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2317', 'date': 'Wed, 22 Jul 2020 01:40:29 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Processed Output Data\n",
    "실제 전처리된 데이타를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-train\n",
      "s3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-validation\n",
      "s3://sagemaker-us-west-2-057716757052/sagemaker-scikit-learn-2020-07-22-01-40-29-885/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-22 01:45:14       5637 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-07-22 01:45:09       5629 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-22 01:45:15        498 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-07-22 01:45:10        504 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-22 01:45:15        512 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-07-22 01:45:10        506 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_raw_input_data' (str)\n",
      "Stored 'max_seq_length' (int)\n",
      "Stored 'train_split_percentage' (float)\n",
      "Stored 'validation_split_percentage' (float)\n",
      "Stored 'test_split_percentage' (float)\n",
      "Stored 'processed_train_data_s3_uri' (str)\n",
      "Stored 'processed_validation_data_s3_uri' (str)\n",
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_raw_input_data\n",
    "%store max_seq_length\n",
    "%store train_split_percentage\n",
    "%store validation_split_percentage\n",
    "%store test_split_percentage\n",
    "%store processed_train_data_s3_uri\n",
    "%store processed_validation_data_s3_uri\n",
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "max_seq_length                               -> 128\n",
      "processed_test_data_s3_uri                   -> 's3://sagemaker-us-west-2-057716757052/sagemaker-s\n",
      "processed_train_data_s3_uri                  -> 's3://sagemaker-us-west-2-057716757052/sagemaker-s\n",
      "processed_validation_data_s3_uri             -> 's3://sagemaker-us-west-2-057716757052/sagemaker-s\n",
      "s3_destination_path_csv                      -> 's3://sagemaker-us-west-2-057716757052/tweet_emoti\n",
      "s3_raw_input_data                            -> 's3://sagemaker-us-west-2-057716757052/tweet_emoti\n",
      "save_split_data_dir                          -> 'data/split'\n",
      "test_split_percentage                        -> 0.05\n",
      "train_split_percentage                       -> 0.9\n",
      "validation_split_percentage                  -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
