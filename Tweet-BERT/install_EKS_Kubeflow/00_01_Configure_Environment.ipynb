{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EKS 환경 설정\n",
    "\n",
    "아래 노트북은 원본 노트북에 설명을 추가 하였습니다.\n",
    "- 실제 실행시에 여기의 셀의 \"결과 값\" 과 일치하는지를 확인하면서 실행 하십시오.\n",
    "- 만일 에러등이 발생하면 확인하시고 다시 실행 해야 합니다.\n",
    "    - https://github.com/data-science-on-aws/workshop/blob/master/12_kubeflow/00_01_Configure_Environment.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associate IAM Policies with EKS Worker Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export INSTANCE_ROLE_NAME=eksctl-cluster-nodegroup-cpu-node-NodeInstanceRole-4DG6CRI38MUD\n",
      "export INSTANCE_PROFILE_ARN=arn:aws:iam::343441690612:role/eksctl-cluster-nodegroup-cpu-node-NodeInstanceRole-4DG6CRI38MUD\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "### Source the environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "export INSTANCE_ROLE_NAME=$(aws iam list-roles \\\n",
    "    | jq -r \".Roles[] \\\n",
    "    | select(.RoleName \\\n",
    "    | startswith(\\\"eksctl-$AWS_CLUSTER_NAME\\\") and contains(\\\"NodeInstanceRole\\\")) \\\n",
    "    .RoleName\")\n",
    "echo \"export INSTANCE_ROLE_NAME=${INSTANCE_ROLE_NAME}\" | tee -a ~/.bash_profile\n",
    "\n",
    "export INSTANCE_PROFILE_ARN=$(aws iam list-roles \\\n",
    "    | jq -r \".Roles[] \\\n",
    "    | select(.RoleName \\\n",
    "    | startswith(\\\"eksctl-$AWS_CLUSTER_NAME\\\") and contains(\\\"NodeInstanceRole\\\")) \\\n",
    "    .Arn\")\n",
    "echo \"export INSTANCE_PROFILE_ARN=${INSTANCE_PROFILE_ARN}\" | tee -a ~/.bash_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "### Source the environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "#### Allow Access from/to the Elastic Container Registry (ECR)\n",
    "# This allows our cluster worker nodes to load custom Docker images (ie. models) from ECR.  We will load these custom Docker images in a later section.\n",
    "aws iam attach-role-policy --role-name $INSTANCE_ROLE_NAME --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\n",
    "\n",
    "echo \"Completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associated IAM and OIDC\n",
    "To use IAM Roles for Service Accounts in your cluster, you must create an OIDC identity provider in the IAM console.  \n",
    "\n",
    "See https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html for more info.\n",
    "\n",
    "# _This may take a few minutes.  Please be patient._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ℹ]  eksctl version 0.25.0\n",
      "[ℹ]  using region ap-northeast-2\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 46.193815ms\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 107.984398ms\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 161.901324ms\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 264.566232ms\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 840.0924ms\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 1.358504704s\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 3.048183744s\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 4.748050944s\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 11.948485632s\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 30.635499008s\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 57.274186752s\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 1m42.745389056s\n",
      "[!]  retryable error (EC2MetadataError: failed to make EC2Metadata request\n",
      "\tstatus code: 405, request id: \n",
      "caused by: ) from ec2metadata/GetToken - will retry after delay of 2m40.636588032s\n",
      "request expired, resigning\n",
      "[ℹ]  IAM Open ID Connect provider is already associated with cluster \"cluster\" in \"ap-northeast-2\"\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "### Source the environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "eksctl utils associate-iam-oidc-provider --cluster ${AWS_CLUSTER_NAME} --approve\n",
    "\n",
    "echo \"Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oidc.eks.ap-northeast-2.amazonaws.com/id/89C372954BDCA6AC1A2F60DCFEF39EAD\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "### Source the environment\n",
    "source ~/.bash_profile\n",
    "\n",
    "aws eks describe-cluster --name ${AWS_CLUSTER_NAME} --region ${AWS_REGION} --query \"cluster.identity.oidc.issuer\" --output text\n",
    "\n",
    "echo \"Completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update `~/.kube/config` with our new EKS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated context arn:aws:eks:ap-northeast-2:343441690612:cluster/cluster in /home/ec2-user/.kube/config\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "source ~/.bash_profile\n",
    "\n",
    "aws eks --region ${AWS_REGION} update-kubeconfig --name ${AWS_CLUSTER_NAME} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Your EKS Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              STATUS   AGE\n",
      "anonymous         Active   3h59m\n",
      "cert-manager      Active   4h13m\n",
      "default           Active   4h32m\n",
      "istio-system      Active   4h13m\n",
      "knative-serving   Active   4h12m\n",
      "kube-node-lease   Active   4h32m\n",
      "kube-public       Active   4h32m\n",
      "kube-system       Active   4h32m\n",
      "kubeflow          Active   4h13m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl get ns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                STATUS   ROLES    AGE     VERSION\n",
      "ip-192-168-39-223.ap-northeast-2.compute.internal   Ready    <none>   4h27m   v1.17.9-eks-4c6976\n",
      "ip-192-168-74-46.ap-northeast-2.compute.internal    Ready    <none>   4h27m   v1.17.9-eks-4c6976\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl get nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
