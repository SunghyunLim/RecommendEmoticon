{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.0] Tweet Text를 BERT Embedding Vector로 변환\n",
    "이 노트북에서는 아래와 같은 작업이 진행 됩니다. 참고로 노트북의 코드는 아래 Reference 코드를 거의 사용 함.\n",
    "- Preprocess Script 생성\n",
    "    - Input Text --> BERT Feature Vector 로 변환 --> TF Record로 변환\n",
    "- 위 스크립트를 테스트 목적으로 Local Notebook에서 실행\n",
    "- SageMaker Processor Job으로 2개의 인스턴스를 사용하여 위 스크립트를 실행\n",
    "    - Train, Validation, Test의 각각 2개의 TF Records 파일이 S3에 저장됨.\n",
    "    \n",
    "---\n",
    "이 노트북은 약 6분 정도 소요 됩니다.    \n",
    "\n",
    "---\n",
    "Reference:\n",
    "    - https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/02_Prepare_Dataset_ProcessingJob_BERT_Scikit.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Preprocess Script\n",
    "이 파일은 아래 소스에서 Label의 5개 --> 10개 변경한 버전 임.\n",
    "- 소스: \n",
    "    - https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/preprocess-scikit-text-to-bert.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess-scikit-text-to-bert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess-scikit-text-to-bert.py\n",
    "\n",
    "# 이 파일은 아래 소스에서 Label의 5개 --> 10개 변경한 버전 임.\n",
    "# https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/preprocess-scikit-text-to-bert.py\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import functools\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "DATA_COLUMN = 'TWEET'\n",
    "LABEL_COLUMN = 'LABEL'\n",
    "LABEL_VALUES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"BERT feature vectors.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    \n",
    "    \n",
    "class Input(object):\n",
    "  \"\"\"A single training/test input for sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, text, label=None):\n",
    "    \"\"\"Constructs an Input.\n",
    "    Args:\n",
    "      text: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.label = label\n",
    "\n",
    "def convert_input(text_input, max_seq_length):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    #\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # \n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "    #\n",
    "    tokens = tokenizer.tokenize(text_input.text)    \n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=max_seq_length)\n",
    "\n",
    "    # Convert the text-based tokens to ids from the pre-trained BERT vocabulary\n",
    "    input_ids = encode_plus_tokens['input_ids']\n",
    "    # Specifies which tokens BERT should pay attention to (0 or 1)\n",
    "    input_mask = encode_plus_tokens['attention_mask']\n",
    "    # Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # Label for our training data (star_rating 1 through 5)\n",
    "    label_id = label_map[text_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id)\n",
    "\n",
    "    return features\n",
    "\n",
    "def convert_features_to_tfrecord(inputs,\n",
    "                                 output_file,\n",
    "                                 max_seq_length):\n",
    "    \"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, text_input) in enumerate(inputs):\n",
    "        if input_idx % 100 == 0:\n",
    "            print(\"Writing example %d of %d\" % (input_idx, len(inputs)))\n",
    "\n",
    "            bert_features = convert_input(text_input, max_seq_length)\n",
    "        \n",
    "            tfrecord_features = collections.OrderedDict()\n",
    "            \n",
    "            tfrecord_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_ids))\n",
    "            tfrecord_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_mask))\n",
    "            tfrecord_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.segment_ids))\n",
    "            tfrecord_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[bert_features.label_id]))\n",
    "\n",
    "            tfrecord = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\n",
    "            \n",
    "            tfrecord_writer.write(tfrecord.SerializeToString())\n",
    "\n",
    "    tfrecord_writer.close()\n",
    "\n",
    "    \n",
    "    \n",
    "def list_arg(raw_value):\n",
    "    \"\"\"argparse type for a list of strings\"\"\"\n",
    "    return str(raw_value).split(',')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def _transform_tsv_to_tfrecord(file, \n",
    "                               max_seq_length, \n",
    "                               balance_dataset):\n",
    "    print('file {}'.format(file))\n",
    "    print('max_seq_length {}'.format(max_seq_length))\n",
    "    print('balance_dataset {}'.format(balance_dataset))\n",
    "\n",
    "    filename_without_extension = Path(Path(file).stem).stem\n",
    "\n",
    "    df = pd.read_csv(file, \n",
    "                     compression='gzip')\n",
    "\n",
    "    df.isna().values.any()\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of dataframe {}'.format(df.shape))\n",
    "\n",
    "        \n",
    "    print('Shape of dataframe before splitting {}'.format(df.shape))\n",
    "    \n",
    "    print('train split percentage {}'.format(args.train_split_percentage))\n",
    "    print('validation split percentage {}'.format(args.validation_split_percentage))\n",
    "    print('test split percentage {}'.format(args.test_split_percentage))    \n",
    "    \n",
    "    holdout_percentage = 1.00 - args.train_split_percentage\n",
    "    print('holdout percentage {}'.format(holdout_percentage))\n",
    "    df_train, df_holdout = train_test_split(df, \n",
    "                                            test_size=holdout_percentage, \n",
    "                                            stratify=df[LABEL_COLUMN])\n",
    "\n",
    "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
    "    print('test holdout percentage {}'.format(test_holdout_percentage))\n",
    "    df_validation, df_test = train_test_split(df_holdout, \n",
    "                                              test_size=test_holdout_percentage,\n",
    "                                              stratify=df_holdout[LABEL_COLUMN])\n",
    "    \n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_validation = df_validation.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of train dataframe {}'.format(df_train.shape))\n",
    "    print('Shape of validation dataframe {}'.format(df_validation.shape))\n",
    "    print('Shape of test dataframe {}'.format(df_test.shape))\n",
    "\n",
    "    train_inputs = df_train.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                         label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_inputs = df_validation.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                            label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    test_inputs = df_test.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    # Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "    # \n",
    "    # \n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # 4. Map our words to indexes using a vocab file that BERT provides\n",
    "    # 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "    # 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "    # \n",
    "    # We don't have to worry about these details.  The Transformers tokenizer does this for us.\n",
    "    # \n",
    "    train_data = '{}/bert/train'.format(args.output_data)\n",
    "    validation_data = '{}/bert/validation'.format(args.output_data)\n",
    "    test_data = '{}/bert/test'.format(args.output_data)\n",
    "\n",
    "    # Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\n",
    "    df_train_embeddings = convert_features_to_tfrecord(train_inputs, \n",
    "                                                       '{}/part-{}-{}.tfrecord'.format(train_data, args.current_host, filename_without_extension), \n",
    "                                                       max_seq_length)\n",
    "\n",
    "    df_validation_embeddings = convert_features_to_tfrecord(validation_inputs, '{}/part-{}-{}.tfrecord'.format(validation_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "\n",
    "    df_test_embeddings = convert_features_to_tfrecord(test_inputs, '{}/part-{}-{}.tfrecord'.format(test_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "        \n",
    "def parse_args():\n",
    "    # Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\n",
    "    resconfig = {}\n",
    "    try:\n",
    "        with open('/opt/ml/config/resourceconfig.json', 'r') as cfgfile:\n",
    "            resconfig = json.load(cfgfile)\n",
    "    except FileNotFoundError:\n",
    "        print('/opt/ml/config/resourceconfig.json not found.  current_host is unknown.')\n",
    "        pass # Ignore\n",
    "\n",
    "    # Local testing with CLI args\n",
    "    parser = argparse.ArgumentParser(description='Process')\n",
    "\n",
    "    parser.add_argument('--hosts', type=list_arg,\n",
    "        default=resconfig.get('hosts', ['unknown']),\n",
    "        help='Comma-separated list of host names running the job'\n",
    "    )\n",
    "    parser.add_argument('--current-host', type=str,\n",
    "        default=resconfig.get('current_host', 'unknown'),\n",
    "        help='Name of this host running the job'\n",
    "    )\n",
    "    parser.add_argument('--input-data', type=str,\n",
    "        default='/opt/ml/processing/input/data',\n",
    "    )\n",
    "    parser.add_argument('--output-data', type=str,\n",
    "        default='/opt/ml/processing/output',\n",
    "    )\n",
    "    parser.add_argument('--train-split-percentage', type=float,\n",
    "        default=0.90,\n",
    "    )\n",
    "    parser.add_argument('--validation-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )    \n",
    "    parser.add_argument('--test-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )\n",
    "    parser.add_argument('--balance-dataset', type=eval,\n",
    "        default=False\n",
    "    )\n",
    "    parser.add_argument('--max-seq-length', type=int,\n",
    "        default=32,\n",
    "    )  \n",
    "    \n",
    "    return parser.parse_args()\n",
    "        \n",
    "    \n",
    "def process(args):\n",
    "    print('Current host: {}'.format(args.current_host))\n",
    "    \n",
    "    train_data = None\n",
    "    validation_data = None\n",
    "    test_data = None\n",
    "\n",
    "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \n",
    "                                                 max_seq_length=args.max_seq_length,\n",
    "                                                 balance_dataset=args.balance_dataset\n",
    "\n",
    "    )\n",
    "    input_files = glob.glob('{}/*'.format(args.input_data))\n",
    "    print(\"********** input files ***************\")    \n",
    "    print(\"args.input_data: \", args.input_data)\n",
    "    print(\"input_files: \", input_files)\n",
    "\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print('num_cpus {}'.format(num_cpus))\n",
    "\n",
    "    p = multiprocessing.Pool(num_cpus)\n",
    "    p.map(transform_tsv_to_tfrecord, input_files)\n",
    "\n",
    "    print(\"********** Listing tf-record files ***************\")        \n",
    "    print('Listing contents of {}'.format(args.output_data))\n",
    "    dirs_output = os.listdir(args.output_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(train_data))\n",
    "    dirs_output = os.listdir(train_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(validation_data))\n",
    "    dirs_output = os.listdir(validation_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(test_data))\n",
    "    dirs_output = os.listdir(test_data)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    print('################ START #######################')    \n",
    "    print('Loaded arguments:')\n",
    "    print(args)\n",
    "    \n",
    "    print('Environment variables:')\n",
    "#     print(os.environ)\n",
    "\n",
    "    process(args)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Local Notebook\n",
    "아래는 로컬 노트북에서 ! python 으로 테스트를 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 폴더 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_split_data_dir = 'data/split'\n",
    "output_data_dir = 'data/output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/output/bert/train\n",
    "! mkdir -p data/output/bert/validation\n",
    "! mkdir -p data/output/bert/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.18.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.1.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.30.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (46.1.3.post20200330)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.20.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.2)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.5)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "2020-08-16 00:20:39.064014: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-08-16 00:20:39.064106: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-08-16 00:20:39.064119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2.1.0\n",
      "Requirement already satisfied: transformers==2.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.8.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.0.43)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.7)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.14.27)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2020.7.14)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (4.44.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.1.91)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (3.0.12)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2.23.0)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.5.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.18.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (7.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (1.17.27)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (1.24.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.27->boto3->transformers==2.8.0) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.27->boto3->transformers==2.8.0) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\n",
      "################ START #######################\n",
      "Loaded arguments:\n",
      "Namespace(balance_dataset=False, current_host='unknown', hosts=['unknown'], input_data='data/split', max_seq_length=32, output_data='data/output', test_split_percentage=0.05, train_split_percentage=0.9, validation_split_percentage=0.05)\n",
      "Environment variables:\n",
      "Current host: unknown\n",
      "********** input files ***************\n",
      "args.input_data:  data/split\n",
      "input_files:  ['data/split/tweet_file_01.csv.gz', 'data/split/tweet_file_02.csv.gz']\n",
      "num_cpus 8\n",
      "file data/split/tweet_file_01.csv.gz\n",
      "max_seq_length 32\n",
      "balance_dataset False\n",
      "file data/split/tweet_file_02.csv.gz\n",
      "max_seq_length 32\n",
      "balance_dataset False\n",
      "Shape of dataframe (42385, 2)\n",
      "Shape of dataframe before splitting (42385, 2)\n",
      "Shape of dataframe (42386, 2)\n",
      "train split percentage 0.9\n",
      "validation split percentage 0.05\n",
      "Shape of dataframe before splitting (42386, 2)\n",
      "test split percentage 0.05\n",
      "train split percentage 0.9\n",
      "holdout percentage 0.09999999999999998\n",
      "validation split percentage 0.05\n",
      "test split percentage 0.05\n",
      "holdout percentage 0.09999999999999998\n",
      "test holdout percentage 0.5000000000000001\n",
      "test holdout percentage 0.5000000000000001\n",
      "Shape of train dataframe (38147, 2)\n",
      "Shape of validation dataframe (2119, 2)\n",
      "Shape of test dataframe (2120, 2)\n",
      "Shape of train dataframe (38146, 2)\n",
      "Shape of validation dataframe (2119, 2)\n",
      "Shape of test dataframe (2120, 2)\n",
      "Writing example 0 of 38147\n",
      "Writing example 100 of 38147\n",
      "Writing example 200 of 38147\n",
      "Writing example 300 of 38147\n",
      "Writing example 400 of 38147\n",
      "Writing example 500 of 38147\n",
      "Writing example 600 of 38147\n",
      "Writing example 700 of 38147\n",
      "Writing example 800 of 38147\n",
      "Writing example 900 of 38147\n",
      "Writing example 1000 of 38147\n",
      "Writing example 1100 of 38147\n",
      "Writing example 1200 of 38147\n",
      "Writing example 1300 of 38147\n",
      "Writing example 1400 of 38147\n",
      "Writing example 1500 of 38147\n",
      "Writing example 1600 of 38147\n",
      "Writing example 1700 of 38147\n",
      "Writing example 0 of 38146\n",
      "Writing example 1800 of 38147\n",
      "Writing example 1900 of 38147\n",
      "Writing example 2000 of 38147\n",
      "Writing example 100 of 38146\n",
      "Writing example 2100 of 38147\n",
      "Writing example 200 of 38146\n",
      "Writing example 300 of 38146\n",
      "Writing example 2200 of 38147\n",
      "Writing example 2300 of 38147\n",
      "Writing example 2400 of 38147\n",
      "Writing example 2500 of 38147\n",
      "Writing example 400 of 38146\n",
      "Writing example 2600 of 38147\n",
      "Writing example 500 of 38146\n",
      "Writing example 2700 of 38147\n",
      "Writing example 600 of 38146\n",
      "Writing example 2800 of 38147\n",
      "Writing example 2900 of 38147\n",
      "Writing example 700 of 38146\n",
      "Writing example 3000 of 38147\n",
      "Writing example 800 of 38146\n",
      "Writing example 3100 of 38147\n",
      "Writing example 900 of 38146\n",
      "Writing example 3200 of 38147\n",
      "Writing example 1000 of 38146\n",
      "Writing example 3300 of 38147\n",
      "Writing example 3400 of 38147\n",
      "Writing example 1100 of 38146\n",
      "Writing example 3500 of 38147\n",
      "Writing example 1200 of 38146\n",
      "Writing example 1300 of 38146\n",
      "Writing example 3600 of 38147\n",
      "Writing example 1400 of 38146\n",
      "Writing example 3700 of 38147\n",
      "Writing example 3800 of 38147\n",
      "Writing example 3900 of 38147\n",
      "Writing example 1500 of 38146\n",
      "Writing example 4000 of 38147\n",
      "Writing example 1600 of 38146\n",
      "Writing example 4100 of 38147\n",
      "Writing example 1700 of 38146\n",
      "Writing example 4200 of 38147\n",
      "Writing example 4300 of 38147\n",
      "Writing example 1800 of 38146\n",
      "Writing example 1900 of 38146\n",
      "Writing example 2000 of 38146\n",
      "Writing example 4400 of 38147\n",
      "Writing example 4500 of 38147\n",
      "Writing example 2100 of 38146\n",
      "Writing example 4600 of 38147\n",
      "Writing example 2200 of 38146\n",
      "Writing example 2300 of 38146\n",
      "Writing example 4700 of 38147\n",
      "Writing example 4800 of 38147\n",
      "Writing example 2400 of 38146\n",
      "Writing example 4900 of 38147\n",
      "Writing example 2500 of 38146\n",
      "Writing example 5000 of 38147\n",
      "Writing example 2600 of 38146\n",
      "Writing example 5100 of 38147\n",
      "Writing example 2700 of 38146\n",
      "Writing example 5200 of 38147\n",
      "Writing example 2800 of 38146\n",
      "Writing example 2900 of 38146\n",
      "Writing example 5300 of 38147\n",
      "Writing example 3000 of 38146\n",
      "Writing example 5400 of 38147\n",
      "Writing example 3100 of 38146\n",
      "Writing example 3200 of 38146\n",
      "Writing example 5500 of 38147\n",
      "Writing example 3300 of 38146\n",
      "Writing example 3400 of 38146\n",
      "Writing example 5600 of 38147\n",
      "Writing example 3500 of 38146\n",
      "Writing example 5700 of 38147\n",
      "Writing example 3600 of 38146\n",
      "Writing example 5800 of 38147\n",
      "Writing example 3700 of 38146\n",
      "Writing example 5900 of 38147\n",
      "Writing example 3800 of 38146\n",
      "Writing example 6000 of 38147\n",
      "Writing example 3900 of 38146\n",
      "Writing example 4000 of 38146\n",
      "Writing example 6100 of 38147\n",
      "Writing example 6200 of 38147\n",
      "Writing example 4100 of 38146\n",
      "Writing example 4200 of 38146\n",
      "Writing example 6300 of 38147\n",
      "Writing example 4300 of 38146\n",
      "Writing example 6400 of 38147\n",
      "Writing example 4400 of 38146\n",
      "Writing example 6500 of 38147\n",
      "Writing example 4500 of 38146\n",
      "Writing example 6600 of 38147\n",
      "Writing example 4600 of 38146\n",
      "Writing example 6700 of 38147\n",
      "Writing example 4700 of 38146\n",
      "Writing example 6800 of 38147\n",
      "Writing example 6900 of 38147\n",
      "Writing example 4800 of 38146\n",
      "Writing example 7000 of 38147\n",
      "Writing example 4900 of 38146\n",
      "Writing example 7100 of 38147\n",
      "Writing example 5000 of 38146\n",
      "Writing example 7200 of 38147\n",
      "Writing example 5100 of 38146\n",
      "Writing example 5200 of 38146\n",
      "Writing example 7300 of 38147\n",
      "Writing example 7400 of 38147\n",
      "Writing example 5300 of 38146\n",
      "Writing example 5400 of 38146\n",
      "Writing example 7500 of 38147\n",
      "Writing example 5500 of 38146\n",
      "Writing example 5600 of 38146\n",
      "Writing example 7600 of 38147\n",
      "Writing example 5700 of 38146\n",
      "Writing example 7700 of 38147\n",
      "Writing example 5800 of 38146\n",
      "Writing example 7800 of 38147\n",
      "Writing example 7900 of 38147\n",
      "Writing example 5900 of 38146\n",
      "Writing example 6000 of 38146\n",
      "Writing example 8000 of 38147\n",
      "Writing example 6100 of 38146\n",
      "Writing example 6200 of 38146\n",
      "Writing example 8100 of 38147\n",
      "Writing example 6300 of 38146\n",
      "Writing example 6400 of 38146\n",
      "Writing example 8200 of 38147\n",
      "Writing example 6500 of 38146\n",
      "Writing example 8300 of 38147\n",
      "Writing example 6600 of 38146\n",
      "Writing example 8400 of 38147\n",
      "Writing example 8500 of 38147\n",
      "Writing example 6700 of 38146\n",
      "Writing example 8600 of 38147\n",
      "Writing example 6800 of 38146\n",
      "Writing example 8700 of 38147\n",
      "Writing example 8800 of 38147\n",
      "Writing example 6900 of 38146\n",
      "Writing example 8900 of 38147\n",
      "Writing example 9000 of 38147\n",
      "Writing example 7000 of 38146\n",
      "Writing example 9100 of 38147\n",
      "Writing example 7100 of 38146\n",
      "Writing example 9200 of 38147\n",
      "Writing example 9300 of 38147\n",
      "Writing example 7200 of 38146\n",
      "Writing example 9400 of 38147\n",
      "Writing example 9500 of 38147\n",
      "Writing example 7300 of 38146\n",
      "Writing example 7400 of 38146\n",
      "Writing example 9600 of 38147\n",
      "Writing example 7500 of 38146\n",
      "Writing example 9700 of 38147\n",
      "Writing example 7600 of 38146\n",
      "Writing example 9800 of 38147\n",
      "Writing example 7700 of 38146\n",
      "Writing example 9900 of 38147\n",
      "Writing example 10000 of 38147\n",
      "Writing example 7800 of 38146\n",
      "Writing example 7900 of 38146\n",
      "Writing example 10100 of 38147\n",
      "Writing example 8000 of 38146\n",
      "Writing example 10200 of 38147\n",
      "Writing example 8100 of 38146\n",
      "Writing example 8200 of 38146\n",
      "Writing example 10300 of 38147\n",
      "Writing example 10400 of 38147\n",
      "Writing example 8300 of 38146\n",
      "Writing example 8400 of 38146\n",
      "Writing example 10500 of 38147\n",
      "Writing example 10600 of 38147\n",
      "Writing example 8500 of 38146\n",
      "Writing example 10700 of 38147\n",
      "Writing example 8600 of 38146\n",
      "Writing example 8700 of 38146\n",
      "Writing example 10800 of 38147\n",
      "Writing example 8800 of 38146\n",
      "Writing example 10900 of 38147\n",
      "Writing example 8900 of 38146\n",
      "Writing example 11000 of 38147\n",
      "Writing example 11100 of 38147\n",
      "Writing example 9000 of 38146\n",
      "Writing example 11200 of 38147\n",
      "Writing example 9100 of 38146\n",
      "Writing example 11300 of 38147\n",
      "Writing example 11400 of 38147\n",
      "Writing example 11500 of 38147\n",
      "Writing example 9200 of 38146\n",
      "Writing example 11600 of 38147\n",
      "Writing example 9300 of 38146\n",
      "Writing example 9400 of 38146\n",
      "Writing example 11700 of 38147\n",
      "Writing example 9500 of 38146\n",
      "Writing example 11800 of 38147\n",
      "Writing example 9600 of 38146\n",
      "Writing example 11900 of 38147\n",
      "Writing example 9700 of 38146\n",
      "Writing example 9800 of 38146\n",
      "Writing example 12000 of 38147\n",
      "Writing example 9900 of 38146\n",
      "Writing example 12100 of 38147\n",
      "Writing example 12200 of 38147\n",
      "Writing example 10000 of 38146\n",
      "Writing example 12300 of 38147\n",
      "Writing example 10100 of 38146\n",
      "Writing example 10200 of 38146\n",
      "Writing example 12400 of 38147\n",
      "Writing example 12500 of 38147\n",
      "Writing example 10300 of 38146\n",
      "Writing example 10400 of 38146\n",
      "Writing example 12600 of 38147\n",
      "Writing example 10500 of 38146\n",
      "Writing example 12700 of 38147\n",
      "Writing example 10600 of 38146\n",
      "Writing example 10700 of 38146\n",
      "Writing example 12800 of 38147\n",
      "Writing example 10800 of 38146\n",
      "Writing example 10900 of 38146\n",
      "Writing example 12900 of 38147\n",
      "Writing example 11000 of 38146\n",
      "Writing example 13000 of 38147\n",
      "Writing example 11100 of 38146\n",
      "Writing example 13100 of 38147\n",
      "Writing example 13200 of 38147\n",
      "Writing example 11200 of 38146\n",
      "Writing example 11300 of 38146\n",
      "Writing example 13300 of 38147\n",
      "Writing example 13400 of 38147\n",
      "Writing example 11400 of 38146\n",
      "Writing example 13500 of 38147\n",
      "Writing example 11500 of 38146\n",
      "Writing example 13600 of 38147\n",
      "Writing example 11600 of 38146\n",
      "Writing example 11700 of 38146\n",
      "Writing example 13700 of 38147\n",
      "Writing example 13800 of 38147\n",
      "Writing example 11800 of 38146\n",
      "Writing example 13900 of 38147\n",
      "Writing example 11900 of 38146\n",
      "Writing example 14000 of 38147\n",
      "Writing example 12000 of 38146\n",
      "Writing example 14100 of 38147\n",
      "Writing example 12100 of 38146\n",
      "Writing example 14200 of 38147\n",
      "Writing example 12200 of 38146\n",
      "Writing example 14300 of 38147\n",
      "Writing example 12300 of 38146\n",
      "Writing example 12400 of 38146\n",
      "Writing example 14400 of 38147\n",
      "Writing example 14500 of 38147\n",
      "Writing example 12500 of 38146\n",
      "Writing example 14600 of 38147\n",
      "Writing example 12600 of 38146\n",
      "Writing example 14700 of 38147\n",
      "Writing example 14800 of 38147\n",
      "Writing example 12700 of 38146\n",
      "Writing example 14900 of 38147\n",
      "Writing example 12800 of 38146\n",
      "Writing example 15000 of 38147\n",
      "Writing example 12900 of 38146\n",
      "Writing example 15100 of 38147\n",
      "Writing example 13000 of 38146\n",
      "Writing example 13100 of 38146\n",
      "Writing example 15200 of 38147\n",
      "Writing example 13200 of 38146\n",
      "Writing example 15300 of 38147\n",
      "Writing example 13300 of 38146\n",
      "Writing example 13400 of 38146\n",
      "Writing example 13500 of 38146\n",
      "Writing example 15400 of 38147\n",
      "Writing example 13600 of 38146\n",
      "Writing example 13700 of 38146\n",
      "Writing example 15500 of 38147\n",
      "Writing example 13800 of 38146\n",
      "Writing example 15600 of 38147\n",
      "Writing example 15700 of 38147\n",
      "Writing example 13900 of 38146\n",
      "Writing example 15800 of 38147\n",
      "Writing example 14000 of 38146\n",
      "Writing example 15900 of 38147\n",
      "Writing example 14100 of 38146\n",
      "Writing example 16000 of 38147\n",
      "Writing example 16100 of 38147\n",
      "Writing example 14200 of 38146\n",
      "Writing example 16200 of 38147\n",
      "Writing example 16300 of 38147\n",
      "Writing example 14300 of 38146\n",
      "Writing example 14400 of 38146\n",
      "Writing example 16400 of 38147\n",
      "Writing example 16500 of 38147\n",
      "Writing example 14500 of 38146\n",
      "Writing example 14600 of 38146\n",
      "Writing example 16600 of 38147\n",
      "Writing example 14700 of 38146\n",
      "Writing example 16700 of 38147\n",
      "Writing example 14800 of 38146\n",
      "Writing example 16800 of 38147\n",
      "Writing example 16900 of 38147\n",
      "Writing example 14900 of 38146\n",
      "Writing example 17000 of 38147\n",
      "Writing example 17100 of 38147\n",
      "Writing example 15000 of 38146\n",
      "Writing example 17200 of 38147\n",
      "Writing example 15100 of 38146\n",
      "Writing example 17300 of 38147\n",
      "Writing example 15200 of 38146\n",
      "Writing example 17400 of 38147\n",
      "Writing example 17500 of 38147\n",
      "Writing example 15300 of 38146\n",
      "Writing example 17600 of 38147\n",
      "Writing example 15400 of 38146\n",
      "Writing example 15500 of 38146\n",
      "Writing example 17700 of 38147\n",
      "Writing example 17800 of 38147\n",
      "Writing example 15600 of 38146\n",
      "Writing example 17900 of 38147\n",
      "Writing example 15700 of 38146\n",
      "Writing example 15800 of 38146\n",
      "Writing example 18000 of 38147\n",
      "Writing example 15900 of 38146\n",
      "Writing example 16000 of 38146\n",
      "Writing example 16100 of 38146\n",
      "Writing example 18100 of 38147\n",
      "Writing example 16200 of 38146\n",
      "Writing example 18200 of 38147\n",
      "Writing example 16300 of 38146\n",
      "Writing example 18300 of 38147\n",
      "Writing example 18400 of 38147\n",
      "Writing example 16400 of 38146\n",
      "Writing example 18500 of 38147\n",
      "Writing example 16500 of 38146\n",
      "Writing example 18600 of 38147\n",
      "Writing example 16600 of 38146\n",
      "Writing example 16700 of 38146\n",
      "Writing example 18700 of 38147\n",
      "Writing example 16800 of 38146\n",
      "Writing example 18800 of 38147\n",
      "Writing example 18900 of 38147\n",
      "Writing example 16900 of 38146\n",
      "Writing example 19000 of 38147\n",
      "Writing example 17000 of 38146\n",
      "Writing example 19100 of 38147\n",
      "Writing example 17100 of 38146\n",
      "Writing example 17200 of 38146\n",
      "Writing example 19200 of 38147\n",
      "Writing example 17300 of 38146\n",
      "Writing example 19300 of 38147\n",
      "Writing example 19400 of 38147\n",
      "Writing example 19500 of 38147\n",
      "Writing example 17400 of 38146\n",
      "Writing example 19600 of 38147\n",
      "Writing example 17500 of 38146\n",
      "Writing example 17600 of 38146\n",
      "Writing example 17700 of 38146\n",
      "Writing example 19700 of 38147\n",
      "Writing example 19800 of 38147\n",
      "Writing example 17800 of 38146\n",
      "Writing example 17900 of 38146\n",
      "Writing example 19900 of 38147\n",
      "Writing example 20000 of 38147\n",
      "Writing example 18000 of 38146\n",
      "Writing example 20100 of 38147\n",
      "Writing example 18100 of 38146\n",
      "Writing example 20200 of 38147\n",
      "Writing example 18200 of 38146\n",
      "Writing example 20300 of 38147\n",
      "Writing example 18300 of 38146\n",
      "Writing example 18400 of 38146\n",
      "Writing example 20400 of 38147\n",
      "Writing example 18500 of 38146\n",
      "Writing example 20500 of 38147\n",
      "Writing example 18600 of 38146\n",
      "Writing example 20600 of 38147\n",
      "Writing example 18700 of 38146\n",
      "Writing example 20700 of 38147\n",
      "Writing example 20800 of 38147\n",
      "Writing example 18800 of 38146\n",
      "Writing example 20900 of 38147\n",
      "Writing example 18900 of 38146\n",
      "Writing example 21000 of 38147\n",
      "Writing example 19000 of 38146\n",
      "Writing example 21100 of 38147\n",
      "Writing example 21200 of 38147\n",
      "Writing example 19100 of 38146\n",
      "Writing example 19200 of 38146\n",
      "Writing example 21300 of 38147\n",
      "Writing example 21400 of 38147\n",
      "Writing example 19300 of 38146\n",
      "Writing example 19400 of 38146\n",
      "Writing example 21500 of 38147\n",
      "Writing example 19500 of 38146\n",
      "Writing example 21600 of 38147\n",
      "Writing example 19600 of 38146\n",
      "Writing example 21700 of 38147\n",
      "Writing example 21800 of 38147\n",
      "Writing example 19700 of 38146\n",
      "Writing example 21900 of 38147\n",
      "Writing example 19800 of 38146\n",
      "Writing example 19900 of 38146\n",
      "Writing example 22000 of 38147\n",
      "Writing example 22100 of 38147\n",
      "Writing example 20000 of 38146\n",
      "Writing example 22200 of 38147\n",
      "Writing example 20100 of 38146\n",
      "Writing example 22300 of 38147\n",
      "Writing example 20200 of 38146\n",
      "Writing example 20300 of 38146\n",
      "Writing example 22400 of 38147\n",
      "Writing example 22500 of 38147\n",
      "Writing example 20400 of 38146\n",
      "Writing example 22600 of 38147\n",
      "Writing example 20500 of 38146\n",
      "Writing example 22700 of 38147\n",
      "Writing example 20600 of 38146\n",
      "Writing example 22800 of 38147\n",
      "Writing example 20700 of 38146\n",
      "Writing example 20800 of 38146\n",
      "Writing example 20900 of 38146\n",
      "Writing example 22900 of 38147\n",
      "Writing example 21000 of 38146\n",
      "Writing example 23000 of 38147\n",
      "Writing example 23100 of 38147\n",
      "Writing example 23200 of 38147\n",
      "Writing example 21100 of 38146\n",
      "Writing example 23300 of 38147\n",
      "Writing example 23400 of 38147\n",
      "Writing example 21200 of 38146\n",
      "Writing example 23500 of 38147\n",
      "Writing example 21300 of 38146\n",
      "Writing example 23600 of 38147\n",
      "Writing example 23700 of 38147\n",
      "Writing example 21400 of 38146\n",
      "Writing example 21500 of 38146\n",
      "Writing example 23800 of 38147\n",
      "Writing example 23900 of 38147\n",
      "Writing example 24000 of 38147\n",
      "Writing example 21600 of 38146\n",
      "Writing example 24100 of 38147\n",
      "Writing example 21700 of 38146\n",
      "Writing example 24200 of 38147\n",
      "Writing example 21800 of 38146\n",
      "Writing example 24300 of 38147\n",
      "Writing example 21900 of 38146\n",
      "Writing example 22000 of 38146\n",
      "Writing example 24400 of 38147\n",
      "Writing example 22100 of 38146\n",
      "Writing example 24500 of 38147\n",
      "Writing example 22200 of 38146\n",
      "Writing example 24600 of 38147\n",
      "Writing example 22300 of 38146\n",
      "Writing example 24700 of 38147\n",
      "Writing example 24800 of 38147\n",
      "Writing example 24900 of 38147\n",
      "Writing example 22400 of 38146\n",
      "Writing example 25000 of 38147\n",
      "Writing example 25100 of 38147\n",
      "Writing example 25200 of 38147\n",
      "Writing example 22500 of 38146\n",
      "Writing example 25300 of 38147\n",
      "Writing example 25400 of 38147\n",
      "Writing example 22600 of 38146\n",
      "Writing example 25500 of 38147\n",
      "Writing example 25600 of 38147\n",
      "Writing example 22700 of 38146\n",
      "Writing example 25700 of 38147\n",
      "Writing example 25800 of 38147\n",
      "Writing example 22800 of 38146\n",
      "Writing example 25900 of 38147\n",
      "Writing example 22900 of 38146\n",
      "Writing example 26000 of 38147\n",
      "Writing example 26100 of 38147\n",
      "Writing example 23000 of 38146\n",
      "Writing example 26200 of 38147\n",
      "Writing example 23100 of 38146\n",
      "Writing example 26300 of 38147\n",
      "Writing example 26400 of 38147\n",
      "Writing example 23200 of 38146\n",
      "Writing example 26500 of 38147\n",
      "Writing example 26600 of 38147\n",
      "Writing example 23300 of 38146\n",
      "Writing example 26700 of 38147\n",
      "Writing example 23400 of 38146\n",
      "Writing example 26800 of 38147\n",
      "Writing example 23500 of 38146\n",
      "Writing example 26900 of 38147\n",
      "Writing example 27000 of 38147\n",
      "Writing example 23600 of 38146\n",
      "Writing example 27100 of 38147\n",
      "Writing example 23700 of 38146\n",
      "Writing example 27200 of 38147\n",
      "Writing example 23800 of 38146\n",
      "Writing example 27300 of 38147\n",
      "Writing example 27400 of 38147\n",
      "Writing example 23900 of 38146\n",
      "Writing example 24000 of 38146\n",
      "Writing example 27500 of 38147\n",
      "Writing example 24100 of 38146\n",
      "Writing example 27600 of 38147\n",
      "Writing example 24200 of 38146\n",
      "Writing example 27700 of 38147\n",
      "Writing example 24300 of 38146\n",
      "Writing example 27800 of 38147\n",
      "Writing example 24400 of 38146\n",
      "Writing example 27900 of 38147\n",
      "Writing example 24500 of 38146\n",
      "Writing example 28000 of 38147\n",
      "Writing example 24600 of 38146\n",
      "Writing example 28100 of 38147\n",
      "Writing example 24700 of 38146\n",
      "Writing example 28200 of 38147\n",
      "Writing example 24800 of 38146\n",
      "Writing example 28300 of 38147\n",
      "Writing example 28400 of 38147\n",
      "Writing example 24900 of 38146\n",
      "Writing example 25000 of 38146\n",
      "Writing example 28500 of 38147\n",
      "Writing example 25100 of 38146\n",
      "Writing example 28600 of 38147\n",
      "Writing example 25200 of 38146\n",
      "Writing example 28700 of 38147\n",
      "Writing example 25300 of 38146\n",
      "Writing example 28800 of 38147\n",
      "Writing example 25400 of 38146\n",
      "Writing example 28900 of 38147\n",
      "Writing example 29000 of 38147\n",
      "Writing example 25500 of 38146\n",
      "Writing example 29100 of 38147\n",
      "Writing example 25600 of 38146\n",
      "Writing example 29200 of 38147\n",
      "Writing example 25700 of 38146\n",
      "Writing example 29300 of 38147\n",
      "Writing example 29400 of 38147\n",
      "Writing example 25800 of 38146\n",
      "Writing example 29500 of 38147\n",
      "Writing example 25900 of 38146\n",
      "Writing example 29600 of 38147\n",
      "Writing example 26000 of 38146\n",
      "Writing example 29700 of 38147\n",
      "Writing example 29800 of 38147\n",
      "Writing example 26100 of 38146\n",
      "Writing example 26200 of 38146\n",
      "Writing example 29900 of 38147\n",
      "Writing example 30000 of 38147\n",
      "Writing example 26300 of 38146\n",
      "Writing example 30100 of 38147\n",
      "Writing example 30200 of 38147\n",
      "Writing example 26400 of 38146\n",
      "Writing example 30300 of 38147\n",
      "Writing example 30400 of 38147\n",
      "Writing example 26500 of 38146\n",
      "Writing example 26600 of 38146\n",
      "Writing example 30500 of 38147\n",
      "Writing example 26700 of 38146\n",
      "Writing example 30600 of 38147\n",
      "Writing example 26800 of 38146\n",
      "Writing example 30700 of 38147\n",
      "Writing example 30800 of 38147\n",
      "Writing example 30900 of 38147\n",
      "Writing example 26900 of 38146\n",
      "Writing example 31000 of 38147\n",
      "Writing example 31100 of 38147\n",
      "Writing example 27000 of 38146\n",
      "Writing example 31200 of 38147\n",
      "Writing example 27100 of 38146\n",
      "Writing example 27200 of 38146\n",
      "Writing example 31300 of 38147\n",
      "Writing example 31400 of 38147\n",
      "Writing example 27300 of 38146\n",
      "Writing example 31500 of 38147\n",
      "Writing example 31600 of 38147\n",
      "Writing example 27400 of 38146\n",
      "Writing example 27500 of 38146\n",
      "Writing example 27600 of 38146\n",
      "Writing example 31700 of 38147\n",
      "Writing example 27700 of 38146\n",
      "Writing example 31800 of 38147\n",
      "Writing example 31900 of 38147\n",
      "Writing example 27800 of 38146\n",
      "Writing example 27900 of 38146\n",
      "Writing example 32000 of 38147\n",
      "Writing example 28000 of 38146\n",
      "Writing example 32100 of 38147\n",
      "Writing example 28100 of 38146\n",
      "Writing example 32200 of 38147\n",
      "Writing example 28200 of 38146\n",
      "Writing example 28300 of 38146\n",
      "Writing example 32300 of 38147\n",
      "Writing example 28400 of 38146\n",
      "Writing example 32400 of 38147\n",
      "Writing example 28500 of 38146\n",
      "Writing example 32500 of 38147\n",
      "Writing example 32600 of 38147\n",
      "Writing example 28600 of 38146\n",
      "Writing example 28700 of 38146\n",
      "Writing example 32700 of 38147\n",
      "Writing example 28800 of 38146\n",
      "Writing example 28900 of 38146\n",
      "Writing example 32800 of 38147\n",
      "Writing example 29000 of 38146\n",
      "Writing example 29100 of 38146\n",
      "Writing example 32900 of 38147\n",
      "Writing example 29200 of 38146\n",
      "Writing example 33000 of 38147\n",
      "Writing example 29300 of 38146\n",
      "Writing example 33100 of 38147\n",
      "Writing example 29400 of 38146\n",
      "Writing example 29500 of 38146\n",
      "Writing example 33200 of 38147\n",
      "Writing example 29600 of 38146\n",
      "Writing example 33300 of 38147\n",
      "Writing example 29700 of 38146\n",
      "Writing example 33400 of 38147\n",
      "Writing example 29800 of 38146\n",
      "Writing example 33500 of 38147\n",
      "Writing example 29900 of 38146\n",
      "Writing example 33600 of 38147\n",
      "Writing example 30000 of 38146\n",
      "Writing example 33700 of 38147\n",
      "Writing example 33800 of 38147\n",
      "Writing example 30100 of 38146\n",
      "Writing example 33900 of 38147\n",
      "Writing example 30200 of 38146\n",
      "Writing example 34000 of 38147\n",
      "Writing example 30300 of 38146\n",
      "Writing example 30400 of 38146\n",
      "Writing example 34100 of 38147\n",
      "Writing example 30500 of 38146\n",
      "Writing example 34200 of 38147\n",
      "Writing example 30600 of 38146\n",
      "Writing example 30700 of 38146\n",
      "Writing example 34300 of 38147\n",
      "Writing example 30800 of 38146\n",
      "Writing example 30900 of 38146\n",
      "Writing example 34400 of 38147\n",
      "Writing example 31000 of 38146\n",
      "Writing example 34500 of 38147\n",
      "Writing example 31100 of 38146\n",
      "Writing example 34600 of 38147\n",
      "Writing example 34700 of 38147\n",
      "Writing example 31200 of 38146\n",
      "Writing example 34800 of 38147\n",
      "Writing example 31300 of 38146\n",
      "Writing example 34900 of 38147\n",
      "Writing example 31400 of 38146\n",
      "Writing example 31500 of 38146\n",
      "Writing example 35000 of 38147\n",
      "Writing example 31600 of 38146\n",
      "Writing example 35100 of 38147\n",
      "Writing example 35200 of 38147\n",
      "Writing example 31700 of 38146\n",
      "Writing example 35300 of 38147\n",
      "Writing example 31800 of 38146\n",
      "Writing example 35400 of 38147\n",
      "Writing example 31900 of 38146\n",
      "Writing example 35500 of 38147\n",
      "Writing example 32000 of 38146\n",
      "Writing example 35600 of 38147\n",
      "Writing example 32100 of 38146\n",
      "Writing example 35700 of 38147\n",
      "Writing example 35800 of 38147\n",
      "Writing example 32200 of 38146\n",
      "Writing example 32300 of 38146\n",
      "Writing example 32400 of 38146\n",
      "Writing example 35900 of 38147\n",
      "Writing example 36000 of 38147\n",
      "Writing example 32500 of 38146\n",
      "Writing example 36100 of 38147\n",
      "Writing example 32600 of 38146\n",
      "Writing example 32700 of 38146\n",
      "Writing example 36200 of 38147\n",
      "Writing example 32800 of 38146\n",
      "Writing example 36300 of 38147\n",
      "Writing example 32900 of 38146\n",
      "Writing example 36400 of 38147\n",
      "Writing example 36500 of 38147\n",
      "Writing example 36600 of 38147\n",
      "Writing example 33000 of 38146\n",
      "Writing example 36700 of 38147\n",
      "Writing example 33100 of 38146\n",
      "Writing example 36800 of 38147\n",
      "Writing example 36900 of 38147\n",
      "Writing example 33200 of 38146\n",
      "Writing example 37000 of 38147\n",
      "Writing example 33300 of 38146\n",
      "Writing example 33400 of 38146\n",
      "Writing example 37100 of 38147\n",
      "Writing example 33500 of 38146\n",
      "Writing example 37200 of 38147\n",
      "Writing example 37300 of 38147\n",
      "Writing example 33600 of 38146\n",
      "Writing example 37400 of 38147\n",
      "Writing example 37500 of 38147\n",
      "Writing example 33700 of 38146\n",
      "Writing example 37600 of 38147\n",
      "Writing example 33800 of 38146\n",
      "Writing example 37700 of 38147\n",
      "Writing example 37800 of 38147\n",
      "Writing example 33900 of 38146\n",
      "Writing example 34000 of 38146\n",
      "Writing example 37900 of 38147\n",
      "Writing example 34100 of 38146\n",
      "Writing example 34200 of 38146\n",
      "Writing example 38000 of 38147\n",
      "Writing example 38100 of 38147\n",
      "Writing example 34300 of 38146\n",
      "Writing example 0 of 2119\n",
      "Writing example 34400 of 38146\n",
      "Writing example 100 of 2119\n",
      "Writing example 34500 of 38146\n",
      "Writing example 200 of 2119\n",
      "Writing example 34600 of 38146\n",
      "Writing example 34700 of 38146\n",
      "Writing example 300 of 2119\n",
      "Writing example 34800 of 38146\n",
      "Writing example 34900 of 38146\n",
      "Writing example 400 of 2119\n",
      "Writing example 500 of 2119\n",
      "Writing example 35000 of 38146\n",
      "Writing example 600 of 2119\n",
      "Writing example 35100 of 38146\n",
      "Writing example 700 of 2119\n",
      "Writing example 35200 of 38146\n",
      "Writing example 35300 of 38146\n",
      "Writing example 800 of 2119\n",
      "Writing example 35400 of 38146\n",
      "Writing example 900 of 2119\n",
      "Writing example 35500 of 38146\n",
      "Writing example 1000 of 2119\n",
      "Writing example 1100 of 2119\n",
      "Writing example 35600 of 38146\n",
      "Writing example 1200 of 2119\n",
      "Writing example 1300 of 2119\n",
      "Writing example 35700 of 38146\n",
      "Writing example 35800 of 38146\n",
      "Writing example 1400 of 2119\n",
      "Writing example 1500 of 2119\n",
      "Writing example 35900 of 38146\n",
      "Writing example 36000 of 38146\n",
      "Writing example 1600 of 2119\n",
      "Writing example 1700 of 2119\n",
      "Writing example 36100 of 38146\n",
      "Writing example 1800 of 2119\n",
      "Writing example 36200 of 38146\n",
      "Writing example 1900 of 2119\n",
      "Writing example 2000 of 2119\n",
      "Writing example 36300 of 38146\n",
      "Writing example 2100 of 2119\n",
      "Writing example 36400 of 38146\n",
      "Writing example 36500 of 38146\n",
      "Writing example 36600 of 38146\n",
      "Writing example 0 of 2120\n",
      "Writing example 36700 of 38146\n",
      "Writing example 100 of 2120\n",
      "Writing example 36800 of 38146\n",
      "Writing example 200 of 2120\n",
      "Writing example 300 of 2120\n",
      "Writing example 36900 of 38146\n",
      "Writing example 37000 of 38146\n",
      "Writing example 400 of 2120\n",
      "Writing example 500 of 2120\n",
      "Writing example 37100 of 38146\n",
      "Writing example 37200 of 38146\n",
      "Writing example 600 of 2120\n",
      "Writing example 37300 of 38146\n",
      "Writing example 37400 of 38146\n",
      "Writing example 700 of 2120\n",
      "Writing example 800 of 2120\n",
      "Writing example 37500 of 38146\n",
      "Writing example 37600 of 38146\n",
      "Writing example 900 of 2120\n",
      "Writing example 37700 of 38146\n",
      "Writing example 1000 of 2120\n",
      "Writing example 37800 of 38146\n",
      "Writing example 1100 of 2120\n",
      "Writing example 37900 of 38146\n",
      "Writing example 1200 of 2120\n",
      "Writing example 38000 of 38146\n",
      "Writing example 1300 of 2120\n",
      "Writing example 38100 of 38146\n",
      "Writing example 1400 of 2120\n",
      "Writing example 0 of 2119\n",
      "Writing example 1500 of 2120\n",
      "Writing example 100 of 2119\n",
      "Writing example 1600 of 2120\n",
      "Writing example 200 of 2119\n",
      "Writing example 1700 of 2120\n",
      "Writing example 300 of 2119\n",
      "Writing example 1800 of 2120\n",
      "Writing example 400 of 2119\n",
      "Writing example 1900 of 2120\n",
      "Writing example 500 of 2119\n",
      "Writing example 600 of 2119\n",
      "Writing example 2000 of 2120\n",
      "Writing example 2100 of 2120\n",
      "Writing example 700 of 2119\n",
      "Writing example 800 of 2119\n",
      "Writing example 900 of 2119\n",
      "Writing example 1000 of 2119\n",
      "Writing example 1100 of 2119\n",
      "Writing example 1200 of 2119\n",
      "Writing example 1300 of 2119\n",
      "Writing example 1400 of 2119\n",
      "Writing example 1500 of 2119\n",
      "Writing example 1600 of 2119\n",
      "Writing example 1700 of 2119\n",
      "Writing example 1800 of 2119\n",
      "Writing example 1900 of 2119\n",
      "Writing example 2000 of 2119\n",
      "Writing example 2100 of 2119\n",
      "Writing example 0 of 2120\n",
      "Writing example 100 of 2120\n",
      "Writing example 200 of 2120\n",
      "Writing example 300 of 2120\n",
      "Writing example 400 of 2120\n",
      "Writing example 500 of 2120\n",
      "Writing example 600 of 2120\n",
      "Writing example 700 of 2120\n",
      "Writing example 800 of 2120\n",
      "Writing example 900 of 2120\n",
      "Writing example 1000 of 2120\n",
      "Writing example 1100 of 2120\n",
      "Writing example 1200 of 2120\n",
      "Writing example 1300 of 2120\n",
      "Writing example 1400 of 2120\n",
      "Writing example 1500 of 2120\n",
      "Writing example 1600 of 2120\n",
      "Writing example 1700 of 2120\n",
      "Writing example 1800 of 2120\n",
      "Writing example 1900 of 2120\n",
      "Writing example 2000 of 2120\n",
      "Writing example 2100 of 2120\n",
      "********** Listing tf-record files ***************\n",
      "Listing contents of data/output\n",
      "bert\n",
      "Listing contents of None\n",
      "img\n",
      "tensorboard\n",
      "preprocess-scikit-text-to-bert.py\n",
      "3.1.Create-Train_Script.ipynb\n",
      "transformers\n",
      "inference.py\n",
      "4.1.2.Deploy-Model-Kubeflow.ipynb\n",
      "data\n",
      "TweetData.py\n",
      "model\n",
      "tf_script_bert_tweet.py\n",
      "EKS_Kubeflow\n",
      "TweetUtil.py\n",
      ".ipynb_checkpoints\n",
      "4.1.0-Make_Custom_Inference_Image.ipynb\n",
      "4.2.API_GATEWAY.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      "1.0.Prepare-Tweet-Data.ipynb\n",
      "train_container\n",
      "__pycache__\n",
      "pipeline.yaml\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.4.2.Train_Docker_On_SageMaker.ipynb\n",
      "Working\n",
      "sagemaker-tensorflow-serving-container\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "4.1.1.Deploy-Custom-TFS-Image.ipynb\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "serving.tar\n",
      "Z_TEST_CODE.ipynb\n",
      "3.4.1.Make_Train_Container_ECR.ipynb\n",
      "6.1.Tweet_BERT_Pilpeline_SageMaker.ipynb\n",
      "4.0.Deploy-BuiltIn-TFS-Image.ipynb\n",
      "tensorflow\n",
      "tweet_BERT.zip\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "Listing contents of None\n",
      "img\n",
      "tensorboard\n",
      "preprocess-scikit-text-to-bert.py\n",
      "3.1.Create-Train_Script.ipynb\n",
      "transformers\n",
      "inference.py\n",
      "4.1.2.Deploy-Model-Kubeflow.ipynb\n",
      "data\n",
      "TweetData.py\n",
      "model\n",
      "tf_script_bert_tweet.py\n",
      "EKS_Kubeflow\n",
      "TweetUtil.py\n",
      ".ipynb_checkpoints\n",
      "4.1.0-Make_Custom_Inference_Image.ipynb\n",
      "4.2.API_GATEWAY.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      "1.0.Prepare-Tweet-Data.ipynb\n",
      "train_container\n",
      "__pycache__\n",
      "pipeline.yaml\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.4.2.Train_Docker_On_SageMaker.ipynb\n",
      "Working\n",
      "sagemaker-tensorflow-serving-container\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "4.1.1.Deploy-Custom-TFS-Image.ipynb\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "serving.tar\n",
      "Z_TEST_CODE.ipynb\n",
      "3.4.1.Make_Train_Container_ECR.ipynb\n",
      "6.1.Tweet_BERT_Pilpeline_SageMaker.ipynb\n",
      "4.0.Deploy-BuiltIn-TFS-Image.ipynb\n",
      "tensorflow\n",
      "tweet_BERT.zip\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "Listing contents of None\n",
      "img\n",
      "tensorboard\n",
      "preprocess-scikit-text-to-bert.py\n",
      "3.1.Create-Train_Script.ipynb\n",
      "transformers\n",
      "inference.py\n",
      "4.1.2.Deploy-Model-Kubeflow.ipynb\n",
      "data\n",
      "TweetData.py\n",
      "model\n",
      "tf_script_bert_tweet.py\n",
      "EKS_Kubeflow\n",
      "TweetUtil.py\n",
      ".ipynb_checkpoints\n",
      "4.1.0-Make_Custom_Inference_Image.ipynb\n",
      "4.2.API_GATEWAY.ipynb\n",
      "2.0.process-bert-input_tweet.ipynb\n",
      "1.0.Prepare-Tweet-Data.ipynb\n",
      "train_container\n",
      "__pycache__\n",
      "pipeline.yaml\n",
      "3.3.Train_ScriptMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "3.4.2.Train_Docker_On_SageMaker.ipynb\n",
      "Working\n",
      "sagemaker-tensorflow-serving-container\n",
      "1.5..process_bert_input_scratch.ipynb\n",
      "4.1.1.Deploy-Custom-TFS-Image.ipynb\n",
      "3.2.Train_LocalMode_Tweet_BERT_Transformers_TensorFlow.ipynb\n",
      "serving.tar\n",
      "Z_TEST_CODE.ipynb\n",
      "3.4.1.Make_Train_Container_ECR.ipynb\n",
      "6.1.Tweet_BERT_Pilpeline_SageMaker.ipynb\n",
      "4.0.Deploy-BuiltIn-TFS-Image.ipynb\n",
      "tensorflow\n",
      "tweet_BERT.zip\n",
      "3.0.Train_Tweet_BERT_Transformers_TensorFlow_AdHoc.ipynb\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "!python preprocess-scikit-text-to-bert.py \\\n",
    "    --input-data {save_split_data_dir} \\\n",
    "    --output-data {output_data_dir} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Processing Job using Amazon SageMaker\n",
    "멀티 인스턴스(여기서는 2개)로 Preprocessing Job을 실행 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = SKLearnProcessor(framework_version = '0.20.0',\n",
    "                             role = role,\n",
    "                             instance_type = 'ml.c5.2xlarge',\n",
    "                             instance_count = 2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_input_data = s3_destination_path_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage = 0.90\n",
    "validation_split_percentage = 0.05\n",
    "test_split_percentage = 0.05\n",
    "max_seq_length = 32\n",
    "balance_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-08-16-00-20-44-413\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processor.run(code='preprocess-scikit-text-to-bert.py',\n",
    "              inputs=[ProcessingInput(source=s3_raw_input_data,\n",
    "                                      destination='/opt/ml/processing/input/data/',\n",
    "                                      s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--max-seq-length', str(max_seq_length),\n",
    "                         '--balance-dataset', str(balance_dataset)\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-08-16-00-20-44-413\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to CloudWatch \n",
    "CloudWatch에서 생성된 로그를 확인 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=ap-northeast-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2020-08-16-00-20-44-413;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, scikit_processing_job_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/?region=ap-northeast-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, scikit_processing_job_name, region)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status of the Processor Job\n",
    "Processor Job을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2020-08-16-00-20-44-413', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '32', '--balance-dataset', 'False']}, 'RoleArn': 'arn:aws:iam::343441690612:role/service-role/AmazonSageMaker-ExecutionRole-20200801T163342', 'ProcessingJobArn': 'arn:aws:sagemaker:ap-northeast-2:343441690612:processing-job/sagemaker-scikit-learn-2020-08-16-00-20-44-413', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 8, 16, 0, 20, 44, 764000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 8, 16, 0, 20, 44, 764000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'd6326d94-99dc-4bf0-89af-f900f27622fe', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd6326d94-99dc-4bf0-89af-f900f27622fe', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2351', 'date': 'Sun, 16 Aug 2020 00:20:44 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Processed Output Data\n",
    "실제 전처리된 데이타를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-train\n",
      "s3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-validation\n",
      "s3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-16-00-20-44-413/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-16 00:25:28      78905 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-08-16 00:25:33      78853 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-16 00:25:28       4487 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-08-16 00:25:33       4490 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-16 00:25:28       4618 part-algo-1-tweet_file_01.tfrecord\n",
      "2020-08-16 00:25:33       4480 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_raw_input_data' (str)\n",
      "Stored 'max_seq_length' (int)\n",
      "Stored 'train_split_percentage' (float)\n",
      "Stored 'validation_split_percentage' (float)\n",
      "Stored 'test_split_percentage' (float)\n",
      "Stored 'processed_train_data_s3_uri' (str)\n",
      "Stored 'processed_validation_data_s3_uri' (str)\n",
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_raw_input_data\n",
    "%store max_seq_length\n",
    "%store train_split_percentage\n",
    "%store validation_split_percentage\n",
    "%store test_split_percentage\n",
    "%store processed_train_data_s3_uri\n",
    "%store processed_validation_data_s3_uri\n",
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "ecr_infer_custom_image_tf_serving_20_cpu             -> '343441690612.dkr.ecr.ap-northeast-2.amazonaws.com\n",
      "max_seq_length                                       -> 32\n",
      "processed_test_data_s3_uri                           -> 's3://sagemaker-ap-northeast-2-343441690612/sagema\n",
      "processed_train_data_s3_uri                          -> 's3://sagemaker-ap-northeast-2-343441690612/sagema\n",
      "processed_validation_data_s3_uri                     -> 's3://sagemaker-ap-northeast-2-343441690612/sagema\n",
      "s3_destination_path_csv                              -> 's3://sagemaker-ap-northeast-2-343441690612/tweet_\n",
      "s3_raw_input_data                                    -> 's3://sagemaker-ap-northeast-2-343441690612/tweet_\n",
      "save_split_data_dir                                  -> 'data/split'\n",
      "test_split_percentage                                -> 0.05\n",
      "train_split_percentage                               -> 0.9\n",
      "training_job_name                                    -> 'bert2tweet-2020-08-02-12-25-01-260'\n",
      "validation_split_percentage                          -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
