{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 4.1] Deploy from Scratch\n",
    "\n",
    "\n",
    "여기서는 다음과 같은 작업을 합니다.\n",
    "\n",
    "- 모델 아티펙트 (model.tar.gz) 파일을 S3에서 로컬에 다운로드\n",
    "- TF Saved_Model 의 정의를 확인\n",
    "- SageMaker Model 생성\n",
    "- Endpoint 생성\n",
    "- Inference의 Request Serializer and Deserializer 생성\n",
    "- 프리딕터 생성\n",
    "- 셈플 데이타로 추론\n",
    "\n",
    "---\n",
    "이 노트북은 약 10분 정도 소요 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 프로그램 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: astroid 2.3.3 has requirement wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q wrapt --upgrade --ignore-installed\n",
    "!pip install -q tensorflow==2.1.0\n",
    "!pip install -q transformers==2.8.0\n",
    "!pip install -q sagemaker==1.56.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Model to the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert2tweet-2020-07-22-11-41-39-445\n"
     ]
    }
   ],
   "source": [
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_download = 'model'\n",
    "os.makedirs(model_download, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-057716757052/bert2tweet-2020-07-22-11-41-39-445/output/model.tar.gz to model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz {model_download}/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow/\n",
      "tensorflow/saved_model/\n",
      "tensorflow/saved_model/0/\n",
      "tensorflow/saved_model/0/saved_model.pb\n",
      "tensorflow/saved_model/0/variables/\n",
      "tensorflow/saved_model/0/variables/variables.data-00000-of-00002\n",
      "tensorflow/saved_model/0/variables/variables.data-00001-of-00002\n",
      "tensorflow/saved_model/0/variables/variables.index\n",
      "tensorflow/saved_model/0/assets/\n",
      "transformers/\n",
      "transformers/fine-tuned/\n",
      "transformers/fine-tuned/config.json\n",
      "transformers/fine-tuned/tf_model.h5\n",
      "tensorflow/\n",
      "tensorflow/saved_model/\n",
      "tensorflow/saved_model/0/\n",
      "tensorflow/saved_model/0/saved_model.pb\n",
      "tensorflow/saved_model/0/variables/\n",
      "tensorflow/saved_model/0/variables/variables.index\n",
      "tensorflow/saved_model/0/variables/variables.data-00000-of-00002\n",
      "tensorflow/saved_model/0/variables/variables.data-00001-of-00002\n",
      "tensorflow/saved_model/0/assets/\n",
      "transformers/\n",
      "transformers/fine-tuned/\n",
      "transformers/fine-tuned/config.json\n",
      "transformers/fine-tuned/tf_model.h5\n",
      "2020-07-22 12:13:20.446099: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-07-22 12:13:20.446175: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-07-22 12:13:20.446187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 128)\n",
      "        name: serving_default_input_ids:0\n",
      "    inputs['input_mask'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 128)\n",
      "        name: serving_default_input_mask:0\n",
      "    inputs['segment_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 128)\n",
      "        name: serving_default_segment_ids:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf   {model_download}/model.tar.gz\n",
    "!saved_model_cli show --all --dir ./tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "model = Model(model_data='s3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name),\n",
    "              role=role,\n",
    "              framework_version='2.0.0') # Elastic Inference does not yet support TF 2.1.0 as of sagemaker==1.56.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoint 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!Endpoint name:  tensorflow-inference-2020-07-22-12-06-28-165\n"
     ]
    }
   ],
   "source": [
    "instance_type='ml.m4.xlarge'\n",
    "\n",
    "deployed_model = model.deploy(initial_instance_count = 1,\n",
    "                             instance_type = instance_type,\n",
    "                             wait=True)\n",
    "\n",
    "endpoint_name = deployed_model.endpoint\n",
    "print('Endpoint name:  {}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Request Serializer and Deserializer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestHandler(object):\n",
    "    import json\n",
    "    \n",
    "    def __init__(self, tokenizer, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        transformed_instances = []\n",
    "\n",
    "        for instance in instances:\n",
    "            encode_plus_tokens = tokenizer.encode_plus(instance,\n",
    "                                                       pad_to_max_length=True,\n",
    "                                                       max_length=self.max_seq_length)\n",
    "\n",
    "            input_ids = encode_plus_tokens['input_ids']\n",
    "            input_mask = encode_plus_tokens['attention_mask']\n",
    "            segment_ids = [0] * self.max_seq_length\n",
    "\n",
    "            transformed_instance = {\"input_ids\": input_ids, \n",
    "                                    \"input_mask\": input_mask, \n",
    "                                    \"segment_ids\": segment_ids}\n",
    "\n",
    "            transformed_instances.append(transformed_instance)\n",
    "\n",
    "        transformed_data = {\"instances\": transformed_instances}\n",
    "\n",
    "        return json.dumps(transformed_data)\n",
    "    \n",
    "class ResponseHandler(object):\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "    \n",
    "    def __call__(self, response, accept_header):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        response_body = response.read().decode('utf-8')\n",
    "\n",
    "        response_json = json.loads(response_body)\n",
    "\n",
    "        log_probabilities = response_json[\"predictions\"]\n",
    "\n",
    "        predicted_classes = []\n",
    "\n",
    "        # Convert log_probabilities => softmax (all probabilities add up to 1) => argmax (final prediction)\n",
    "        for log_probability in log_probabilities:\n",
    "            softmax = tf.nn.softmax(log_probability)    \n",
    "            predicted_class_idx = tf.argmax(softmax, axis=-1, output_type=tf.int32)\n",
    "            predicted_class = self.classes[predicted_class_idx]\n",
    "            predicted_classes.append(predicted_class)\n",
    "\n",
    "        return predicted_classes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'tensorflow-inference-2020-07-22-12-06-28-165'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.tensorflow.serving import Predictor\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "request_handler = RequestHandler(tokenizer=tokenizer,\n",
    "                                 max_seq_length=128)\n",
    "\n",
    "response_handler = ResponseHandler(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name,\n",
    "                      sagemaker_session=sess,\n",
    "                      serializer=request_handler,\n",
    "                      deserializer=response_handler,\n",
    "                      content_type='application/json',\n",
    "                      model_name='saved_model',\n",
    "                      model_version=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = 'data/test/tweet_file_test.csv'\n",
    "test_df = pd.read_csv(test_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@theweeknd meadows fest in the day and snl at night ny gonna be',\n",
       "       \"@aye_mickey sadiebabyy_ oh gosh i just hope y'all don't act up too much\",\n",
       "       'me af', ...,\n",
       "       '@gingerjamie222 baby pull me closer in the backseat of your corsa. rt and like tag someone it applies to full cover on my faceboo',\n",
       "       '@tanyawarren handmade to order pretty gift for a new baby flockbn kprs eshopsuk mummybloggers https://',\n",
       "       'real life'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = 'data/test/tweet_file_test.csv'\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "test_df.TWEET.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TWEET', 'LABEL'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = test_df.sample(10)\n",
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji_to_idx is loaded\n",
      "😂\n"
     ]
    }
   ],
   "source": [
    "from TweetUtil import TweetUtil\n",
    "\n",
    "tweet_util = TweetUtil()\n",
    "tweet_util.load_emoji_data('emoji_to_idx.pickle')\n",
    "emoji = tweet_util.get_emo_class_label(3)\n",
    "print(emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[9]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[9]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "sample_df = test_df.sample(10)\n",
    "columns = ['TWEET', 'LABEL']\n",
    "for tweet, label in zip(sample_df.TWEET.values, sample_df.LABEL.values):\n",
    "    # print(\"label: {}, tweet: {}\".format(label, tweet))\n",
    "    \n",
    "    reviews = [tweet]\n",
    "\n",
    "    predicted_classes = predictor.predict(reviews)\n",
    "    print(predicted_classes)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction, Ground_truth- 3:😂, 5:😍 \n",
      " tweet: @baddiessnation holy shit she's beautiful\n",
      "Prediction, Ground_truth- 3:😂, 6:😩 \n",
      " tweet: my baby gettn caught up in sum he aint do power\n",
      "Prediction, Ground_truth- 9:🤔, 3:😂 \n",
      " tweet: @theywantj you smoking dick if you think ima be all for you you fw a whole other person\n",
      "Prediction, Ground_truth- 3:😂, 4:😊 \n",
      " tweet: @averyanderson3_ thank you\n",
      "Prediction, Ground_truth- 3:😂, 2:🔥 \n",
      " tweet: @daiiyrap the migos performing hannah montana with a live symphony orchestra\n",
      "Prediction, Ground_truth- 3:😂, 8:🙄 \n",
      " tweet: lost my homework from work because mfs wanna touch it and then put it somewhere it wasn't at\n",
      "Prediction, Ground_truth- 3:😂, 3:😂 \n",
      " tweet: he plays it better on what level though imagine him trying to discredit what rakitic has achieved\n",
      "Prediction, Ground_truth- 3:😂, 5:😍 \n",
      " tweet: @myyiisshhaa syyyy9_ wait ho your avi\n",
      "Prediction, Ground_truth- 3:😂, 3:😂 \n",
      " tweet: @fred_alaniz oscxrmartinez98 knowing your dad he's gonna tune it either way\n",
      "Prediction, Ground_truth- 3:😂, 5:😍 \n",
      " tweet: unexpected bes\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "sample_df = test_df.sample(10)\n",
    "columns = ['TWEET', 'LABEL']\n",
    "for tweet, label in zip(sample_df.TWEET.values, sample_df.LABEL.values):\n",
    "    # print(\"label: {}, tweet: {}\".format(label, tweet))\n",
    "    \n",
    "    reviews = [tweet]\n",
    "\n",
    "    predicted_classes = predictor.predict(reviews)[0]\n",
    "    print('Prediction, Ground_truth- {}:{}, {}:{} \\n tweet: {}'.format(\n",
    "        predicted_classes, \n",
    "        tweet_util.get_emo_class_label(predicted_classes),\n",
    "        label, \n",
    "        tweet_util.get_emo_class_label(label),        \n",
    "        tweet))    \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
